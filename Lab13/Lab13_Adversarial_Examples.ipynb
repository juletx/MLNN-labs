{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 13.\n",
    "\n",
    "In this Lab we will focus on Adversarial Examples.\n",
    "\n",
    "\n",
    "The code in this notebook has been borrowed from the \"Craft Image Adversarial Samples with Tensorflow\" repository,  available in https://github.com/gongzhitaao/tensorflow-adversarial\n",
    "\n",
    "We will also use material from the book \"Hands-On Machine Learning with Scikit-Learn and TensorFlow. Concepts, Tools, and Techniques to Build Intelligent Systems\" by Aurélien Géron, that it is recommended as Bibliography of the course. http://shop.oreilly.com/product/0636920052289.do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the python libraries required to solve the problems\n",
    "\n",
    "import os\n",
    "# supress tensorflow logging other than errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import ModeKeys, Estimator\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import neat\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# from attacks.fgsm import fgsm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function sets the seeds of the stochastic functions\n",
    "# to make the output of this notebook stable across runs\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # This line forces tensorflow to use the CPU instead of the GPU\n",
    "    # I did this because had problems with my CPU. You can set 'GPU': 1\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As a baseline dataset we use the the MNIST dataset that is loaded here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "img_chas = 1\n",
    "input_shape = (img_rows, img_cols, img_chas)\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "# We import the MNIST dataset\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "print('\\nLoading mnist')\n",
    "\n",
    "# The train and test sets are defined \n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")\n",
    "\n",
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "X_train = X_train.reshape(-1, img_rows, img_cols, img_chas)\n",
    "X_test = X_test.reshape(-1, img_rows, img_cols, img_chas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial examples (https://arxiv.org/pdf/1412.6572.pdf) are inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence.\n",
    "\n",
    "In this lab we will learn:\n",
    "\n",
    "1. What functions we can use to create adversarial examples.\n",
    "2. How to use the functions that create the adversarial examples.\n",
    "3. What is the effect of the adversarial example in the acccuracy of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to create adversarial examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the implementation of the  Fast Gradient Sign Method (FGSM). This method receives an observation (e.g., an MNIST image) and a model that outputs a correct classification of the image (e.g., a DNN classifier). FGSM outputs an image that is a pertubation of the original image, and makes the model to produce an incorrect classification. \n",
    "\n",
    "Several algorithms for creating adversarial examples exist, each of which operate in different ways to achieve their goal. \n",
    "FSGM finds adversarial perturbations which increase the value of the loss function (the one that we usually try to minimize during training).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(model, x, eps=0.01, epochs=1, clip_min=0., clip_max=1.):\n",
    "    \"\"\"\n",
    "    Fast gradient sign method.\n",
    "\n",
    "    See https://arxiv.org/abs/1412.6572 and https://arxiv.org/abs/1607.02533\n",
    "    for details. This function implements the revised version, since the original FGSM\n",
    "    has label leaking problem (https://arxiv.org/abs/1611.01236).\n",
    "\n",
    "    :param model: A wrapper that returns the output as well as logits.\n",
    "    :param x: The input placeholder.\n",
    "    :param eps: The scale factor for noise.\n",
    "    :param epochs: The number of epoch to run.\n",
    "    :param clip_min: The minimum value in output.\n",
    "    :param clip_max: The maximum value in output.\n",
    "\n",
    "    :return: A tensor, contains adversarial samples for each input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # x_adv will contain the adversarial example\n",
    "    x_adv = tf.identity(x)\n",
    "\n",
    "    # This is the classification given by the model to the original observation    \n",
    "    ybar = model(x_adv)\n",
    "    \n",
    "    # Number of possible values for the target class variable\n",
    "    ydim = ybar.get_shape().as_list()[1]\n",
    "    \n",
    "    # Classes for which the prediction is maximized \n",
    "    indices = tf.argmax(ybar, axis=1)\n",
    "    \n",
    "    \n",
    "    # If the dimension of the target variable is 1, then \n",
    "    # target is 0 or 1 according to whether the prob. is <0.5 or > 0.5\n",
    "    # If the dimension of the target variable is > 1, then \n",
    "    # then the target is the class that maximizes the probability\n",
    "    \n",
    "    target = tf.cond(\n",
    "        tf.equal(ydim, 1),\n",
    "        lambda: tf.nn.relu(tf.sign(ybar - 0.5)),\n",
    "        lambda: tf.one_hot(indices, ydim, on_value=1.0, off_value=0.0))\n",
    "    \n",
    "\n",
    "    # The loss_function is sigmoid or softmax according to the number\n",
    "    # of classes\n",
    "    if 1 == ydim:\n",
    "        loss_fn = tf.nn.sigmoid_cross_entropy_with_logits\n",
    "    else:\n",
    "        loss_fn = tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "\n",
    "    # The scale factor is taken as positive    \n",
    "    eps = tf.abs(eps)\n",
    "\n",
    "    # Conditional check for the while (maximum number of epochs)\n",
    "    def _cond(x_adv, i):\n",
    "        return tf.less(i, epochs)\n",
    "\n",
    "    # This is the essential part of the FSGM procedure    \n",
    "    def _body(x_adv, i):\n",
    "        \n",
    "        # We get the prediction (and the logits) of the model\n",
    "        ybar, logits = model(x_adv, logits=True)\n",
    "        \n",
    "        # We also compute the probability for the target\n",
    "        loss = loss_fn(labels=target, logits=logits)\n",
    "        \n",
    "        # The gradient gives us the direction of maximum improvement\n",
    "        # of the loss function for each component (pixel) of the observation\n",
    "        dy_dx, = tf.gradients(loss, x_adv)\n",
    "        \n",
    "        # We use the gradient to \"move\" the adversarial example in the direction\n",
    "        # of the gradient (instead of going in the opposite direction to minimize loss)\n",
    "        x_adv = tf.stop_gradient(x_adv + eps*tf.sign(dy_dx))\n",
    "        \n",
    "        # Finally, the adversarial example is clipped \n",
    "        x_adv = tf.clip_by_value(x_adv, clip_min, clip_max)\n",
    "        return x_adv, i+1\n",
    "\n",
    "    x_adv, _ = tf.while_loop(_cond, _body, (x_adv, 0), back_prop=False,\n",
    "                             name='fgsm')\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test FGSM, we need a model. In the following cell we define a convolutional network (similar to the ones we have practiced with in the lab before). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, logits=False, training=False):\n",
    "    \n",
    "    \n",
    "    conv0 = tf.layers.conv2d(x, filters=36, kernel_size=[3, 3],\n",
    "                             padding='same', name='conv0',\n",
    "                             activation=tf.nn.relu)\n",
    "    \n",
    "    pool0 = tf.layers.max_pooling2d(conv0, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool0')\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(pool0, filters=36,\n",
    "                             kernel_size=[3, 3], padding='same',\n",
    "                             name='conv1', activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool1')\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(pool1, filters=36,\n",
    "                             kernel_size=[3, 3], padding='same',\n",
    "                             name='conv1', activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(conv2, pool_size=[2, 2],\n",
    "                                    strides=2, name='pool1')\n",
    "    \n",
    "    \n",
    "    flat = tf.layers.Flatten()(pool2)\n",
    "    \n",
    "    dense = tf.layers.dense(flat, units=576, activation=tf.nn.relu,\n",
    "                            name='dense')\n",
    "    \n",
    "    dropout = tf.layers.dropout(dense, rate=0.2, training=training,\n",
    "                                name='dropout')\n",
    "    \n",
    "    logits_ = tf.layers.dense(dropout, units=10, name='logits')\n",
    "    y = tf.nn.softmax(logits_, name='ybar')\n",
    "    if logits:\n",
    "        return y, logits_\n",
    "    return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network has been defined. We specify the pipeline to train it, including the loss function that we want to optimize (in this example, softmax_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tensorflow tensors into one \"enviroment\" to avoid\n",
    "# accidental overwriting.\n",
    "class Dummy:\n",
    "    pass\n",
    "env = Dummy()\n",
    "\n",
    "# We need a scope since the inference graph will be reused later\n",
    "with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n",
    "    env.x = tf.placeholder(tf.float32, (None, img_rows, img_cols,\n",
    "                                        img_chas), name='x')\n",
    "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
    "    env.training = tf.placeholder(bool, (), name='mode')\n",
    "\n",
    "    env.ybar, logits = model(env.x, logits=True,\n",
    "                             training=env.training)\n",
    "\n",
    "    z = tf.argmax(env.y, axis=1)\n",
    "    zbar = tf.argmax(env.ybar, axis=1)\n",
    "    count = tf.cast(tf.equal(z, zbar), tf.float32)\n",
    "    env.acc = tf.reduce_mean(count, name='acc')\n",
    "\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits_v2(labels=env.y,\n",
    "                                                   logits=logits)\n",
    "    env.loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "    env.optim = tf.train.AdamOptimizer().minimize(env.loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the model to find the adversarial samples, using the FGSM method, as defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the reuse=True flag\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    env.x_adv = fgsm(model, env.x, epochs=12, eps=0.02)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the train set into a validation set and a  \"reduced\" train set. Later we will know what do we need the validation set for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nShuffling training data')\n",
    "ind = np.random.permutation(X_train.shape[0])\n",
    "X_train, y_train = X_train[ind], y_train[ind]\n",
    "\n",
    "# split training/validation dataset\n",
    "validation_split = 0.1\n",
    "n_train = int(X_train.shape[0]*(1-validation_split))\n",
    "X_valid = X_train[n_train:]\n",
    "X_train = X_train[:n_train]\n",
    "y_valid = y_train[n_train:]\n",
    "y_train = y_train[:n_train]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorflow graph session is defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions _evaluate and _predict implement the training of the network and the predictio, respectively. They are a modular way to organize the learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _evaluate(X_data, y_data, env):\n",
    "    print('\\nEvaluating')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    loss, acc = 0, 0\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_loss, batch_acc = sess.run(\n",
    "            [env.loss, env.acc],\n",
    "            feed_dict={env.x: X_data[start:end],\n",
    "                       env.y: y_data[start:end],\n",
    "                       env.training: False})\n",
    "        loss += batch_loss*batch_size\n",
    "        acc += batch_acc*batch_size\n",
    "    loss /= n_sample\n",
    "    acc /= n_sample\n",
    "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def _predict(X_data, env):\n",
    "    print('\\nPredicting')\n",
    "    n_sample = X_data.shape[0]\n",
    "    batch_size = 128\n",
    "    n_batch = int(np.ceil(n_sample/batch_size))\n",
    "    yval = np.empty((X_data.shape[0], n_classes))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        batch_y = sess.run(env.ybar, feed_dict={\n",
    "            env.x: X_data[start:end], env.training: False})\n",
    "        yval[start:end] = batch_y\n",
    "    print()\n",
    "    return yval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell the model is learned using the reduced train dataset and creating minibatches as we have done many times before. The validation set, that is NOT used for training, is used to estimate the quality of the model.\n",
    "\n",
    "The algorithm prints the accuracy and the loss function for this train data and also, at the end, for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nTraining')\n",
    "n_sample = X_train.shape[0]\n",
    "batch_size = 128\n",
    "n_batch = int(np.ceil(n_sample/batch_size))\n",
    "\n",
    "n_epoch = 2 # More epochs might be needed\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        sess.run(env.optim, feed_dict={env.x: X_train[start:end],\n",
    "                                       env.y: y_train[start:end],\n",
    "                                       env.training: True})\n",
    "    _evaluate(X_valid, y_valid, env)\n",
    "    \n",
    "\n",
    "print('\\nTesting against clean data')\n",
    "_evaluate(X_test, y_test, env)\n",
    "\n",
    "save_path = saver.save(sess, \"/tmp/mnist_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, in the results of the learning process, that the accuracy of the model was high, above 0.94 in my case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning adversarial examples using FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell the adversarial examples are created from the test data. Notice that tmp stores the output of the FGSM procedure as implemented within the environment env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('\\nCrafting adversarial')\n",
    "n_sample = X_test.shape[0]\n",
    "batch_size = 128\n",
    "n_batch = int(np.ceil(n_sample/batch_size))\n",
    "n_epoch = 20\n",
    "X_adv = np.empty_like(X_test)\n",
    "for ind in range(n_batch):\n",
    "    print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "    start = ind*batch_size\n",
    "    end = min(n_sample, start+batch_size)\n",
    "    tmp = sess.run(env.x_adv, feed_dict={env.x: X_test[start:end],\n",
    "                                         env.y: y_test[start:end],\n",
    "                                         env.training: False})\n",
    "    X_adv[start:end] = tmp\n",
    "print('\\nSaving adversarial')\n",
    "os.makedirs('data', exist_ok=True)\n",
    "np.save('data/ex_00.npy', X_adv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the accuracy of the model for the created adversarial examples (expect it to be low and compare with the accuracy values that we have previously obtained with the same model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTesting against adversarial data')\n",
    "_evaluate(X_adv, y_test, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell visualizes one adversarial example for each of the ten digits. Run the script and visualize the file img/ex_00.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y1 = _predict(X_test, env)\n",
    "y2 = _predict(X_adv, env)\n",
    "\n",
    "z0 = np.argmax(y_test, axis=1)\n",
    "z1 = np.argmax(y1, axis=1)\n",
    "z2 = np.argmax(y2, axis=1)\n",
    "\n",
    "X_tmp = np.empty((10, 28, 28))\n",
    "y_tmp = np.empty((10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Target {0}'.format(i))\n",
    "    ind, = np.where(np.all([z0==i, z1==i, z2!=i], axis=0))\n",
    "    cur = np.random.choice(ind)\n",
    "\n",
    "    X_tmp[i] = np.squeeze(X_adv[cur])\n",
    "    y_tmp[i] = y2[cur]\n",
    "\n",
    "print('\\nPlotting results')\n",
    "fig = plt.figure(figsize=(10, 1.8))\n",
    "gs = gridspec.GridSpec(1, 10, wspace=0.1, hspace=0.1)\n",
    "\n",
    "label = np.argmax(y_tmp, axis=1)\n",
    "proba = np.max(y_tmp, axis=1)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.imshow(X_tmp[i], cmap='gray', interpolation='none')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('{0} ({1:.2f})'.format(label[i], proba[i]),\n",
    "                  fontsize=12)\n",
    "\n",
    "print('\\nSaving figure')\n",
    "gs.tight_layout(fig)\n",
    "os.makedirs('img', exist_ok=True)\n",
    "plt.savefig('img/ex_00.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "\n",
    "Modify the value of epsilon and observe the effect on the generated adversarial examples.\n",
    "Change the directory in which the images are saved to see the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the reuse=True flag\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    env.x_adv = fgsm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nTraining')\n",
    "n_sample = X_train.shape[0]\n",
    "batch_size = 128\n",
    "n_batch = int(np.ceil(n_sample/batch_size))\n",
    "\n",
    "n_epoch = 2 # More epochs might be needed\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        sess.run(env.optim, feed_dict={env.x: X_train[start:end],\n",
    "                                       env.y: y_train[start:end],\n",
    "                                       env.training: True})\n",
    "    _evaluate(X_valid, y_valid, env)\n",
    "    \n",
    "\n",
    "print('\\nTesting against clean data')\n",
    "_evaluate(X_test, y_test, env)\n",
    "\n",
    "save_path = saver.save(sess, \"/tmp/mnist_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nCrafting adversarial')\n",
    "n_sample = X_test.shape[0]\n",
    "batch_size = 128\n",
    "n_batch = int(np.ceil(n_sample/batch_size))\n",
    "n_epoch = 20\n",
    "X_adv = np.empty_like(X_test)\n",
    "for ind in range(n_batch):\n",
    "    print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "    start = ind*batch_size\n",
    "    end = min(n_sample, start+batch_size)\n",
    "    tmp = sess.run(env.x_adv, feed_dict={env.x: X_test[start:end],\n",
    "                                         env.y: y_test[start:end],\n",
    "                                         env.training: False})\n",
    "    X_adv[start:end] = tmp\n",
    "print('\\nSaving adversarial')\n",
    "os.makedirs('data', exist_ok=True)\n",
    "np.save('data/ex_00.npy', X_adv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTesting against adversarial data')\n",
    "_evaluate(X_adv, y_test, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y1 = _predict(X_test, env)\n",
    "y2 = _predict(X_adv, env)\n",
    "\n",
    "z0 = np.argmax(y_test, axis=1)\n",
    "z1 = np.argmax(y1, axis=1)\n",
    "z2 = np.argmax(y2, axis=1)\n",
    "\n",
    "X_tmp = np.empty((10, 28, 28))\n",
    "y_tmp = np.empty((10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Target {0}'.format(i))\n",
    "    ind, = np.where(np.all([z0==i, z1==i, z2!=i], axis=0))\n",
    "    cur = np.random.choice(ind)\n",
    "\n",
    "    X_tmp[i] = np.squeeze(X_adv[cur])\n",
    "    y_tmp[i] = y2[cur]\n",
    "\n",
    "print('\\nPlotting results')\n",
    "fig = plt.figure(figsize=(10, 1.8))\n",
    "gs = gridspec.GridSpec(1, 10, wspace=0.1, hspace=0.1)\n",
    "\n",
    "label = np.argmax(y_tmp, axis=1)\n",
    "proba = np.max(y_tmp, axis=1)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.imshow(X_tmp[i], cmap='gray', interpolation='none')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('{0} ({1:.2f})'.format(label[i], proba[i]),\n",
    "                  fontsize=12)\n",
    "\n",
    "print('\\nSaving figure')\n",
    "gs.tight_layout(fig)\n",
    "os.makedirs(___, exist_ok=True)\n",
    "plt.savefig(___ + '/ex_00.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 \n",
    "\n",
    " 2.1) Reuse the CNN and the code in the following cell to generate adversarial examples for the fashion dataset.\n",
    " \n",
    " 2.2) The effectiveness of adversarial learning is totally dependent on the quality of the model used for classification. Therefore, if the accuracy of the trained model is not very high (this can happen, as the Fashion MNIST problem is harder than the MINST), modify the model (possibly by adding the capacity of the model) and/or increase the number of training epochs.\n",
    " \n",
    " 2.3) Copy and modify the code that was previously used to visualize and save the adversarial examples of the fashion dataset (save them with a different name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist, fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_And_Normalize_Image_Dataset(db_name='mnist', max_train_samples = 10000,max_test_samples = 10000):\n",
    "\n",
    "    if db_name=='mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()               \n",
    "    elif db_name=='fashion':\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()        \n",
    "        \n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    \n",
    "    one_hot_y_train = np.zeros((y_train.size, y_train.max()+1))\n",
    "    one_hot_y_train[np.arange(y_train.size),y_train] = 1\n",
    "    \n",
    "    one_hot_y_test = np.zeros((y_test.size, y_test.max()+1))\n",
    "    one_hot_y_test[np.arange(y_test.size),y_test] = 1\n",
    "\n",
    "\n",
    "    return (x_train, one_hot_y_train), (x_test, one_hot_y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_fashion, y_train_fashion), (X_test_fashion, y_test_fashion) = Read_And_Normalize_Image_Dataset(db_name='fashion', max_train_samples = 10000, max_test_samples = 10000)\n",
    "#fashion_train = np.reshape(fashion_train, (-1, 784))\n",
    "#fashion_test = np.reshape(fashion_test, (-1, 784))\n",
    "# The train and test sets are defined \n",
    "\n",
    "X_train_fashion = X_train_fashion.astype('float32') / 255.\n",
    "X_test_fashion  = X_test_fashion.astype('float32') / 255.\n",
    "\n",
    "X_train_fashion = X_train_fashion.reshape(-1, img_rows, img_cols, img_chas)\n",
    "X_test_fashion  = X_test_fashion.reshape(-1, img_rows, img_cols, img_chas)\n",
    "\n",
    "y_train_fashion =  y_train_fashion.astype(\"int\")\n",
    "y_test_fashion =  y_test_fashion.astype(\"int\")\n",
    "\n",
    "print('Shuffling training data')\n",
    "ind = np.random.permutation(X_train.shape[0])\n",
    "X_train_fashion, y_train_fashion = X_train_fashion[ind], y_train_fashion[ind]\n",
    "\n",
    "# split training/validation dataset\n",
    "validation_split = 0.1\n",
    "n_train = int(X_train_fashion.shape[0]*(1-validation_split))\n",
    "X_valid_fashion = X_train_fashion[n_train:]\n",
    "X_train_fashion = X_train_fashion[:n_train]\n",
    "y_valid_fashion = y_train_fashion[n_train:]\n",
    "y_train_fashion = y_train_fashion[:n_train]\n",
    "\n",
    "print(X_valid_fashion.shape, X_train_fashion.shape, X_test_fashion.shape, y_valid_fashion.shape, y_train_fashion.shape, y_test_fashion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fashion(x, logits=False, training=False):\n",
    "\n",
    "    if logits:\n",
    "        return y, logits_\n",
    "    return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the reuse=True flag\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    env.x_adv = fgsm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nTraining')\n",
    "n_sample = X_train_fashion.shape[0]\n",
    "batch_size = 128\n",
    "n_batch = int(np.ceil(n_sample/batch_size))\n",
    "\n",
    "n_epoch = 10 # More epochs might be needed\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        sess.run(env.optim, feed_dict={env.x: ___,\n",
    "                                       env.y: ___,\n",
    "                                       env.training: True})\n",
    "    _evaluate(___, ___, env)\n",
    "    \n",
    "\n",
    "print('\\nTesting against clean data')\n",
    "_evaluate(___, ___, env)\n",
    "\n",
    "save_path = saver.save(sess, \"/tmp/fahsion_mnist_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nCrafting adversarial')\n",
    "n_sample = X_test_fashion.shape[0]\n",
    "batch_size = 128\n",
    "n_batch = int(np.ceil(n_sample/batch_size))\n",
    "n_epoch = 20\n",
    "X_adv_fashion = np.empty_like(X_test)\n",
    "for ind in range(n_batch):\n",
    "    print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "    start = ind*batch_size\n",
    "    end = min(n_sample, start+batch_size)\n",
    "    tmp = sess.run(env.x_adv, feed_dict={env.x: ___,\n",
    "                                         env.y: ___,\n",
    "                                         env.training: False})\n",
    "    X_adv_fashion[start:end] = tmp\n",
    "print('\\nSaving adversarial')\n",
    "os.makedirs('data', exist_ok=True)\n",
    "np.save('data/ex_00.npy', X_adv_fashion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTesting against adversarial data')\n",
    "_evaluate(___, ___, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y1 = _predict(X_test, env)\n",
    "y2 = _predict(X_adv, env)\n",
    "\n",
    "z0 = np.argmax(y_test, axis=1)\n",
    "z1 = np.argmax(y1, axis=1)\n",
    "z2 = np.argmax(y2, axis=1)\n",
    "\n",
    "X_tmp = np.empty((10, 28, 28))\n",
    "y_tmp = np.empty((10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Target {0}'.format(i))\n",
    "    ind, = np.where(np.all([z0==i, z1==i, z2!=i], axis=0))\n",
    "    if ind.shape[0] > 0:\n",
    "        cur = np.random.choice(ind)\n",
    "\n",
    "        X_tmp[i] = np.squeeze(X_adv[cur])\n",
    "        y_tmp[i] = y2[cur]\n",
    "    else:\n",
    "        print(\"No examples classified in this class\")\n",
    "\n",
    "print('\\nPlotting results')\n",
    "fig = plt.figure(figsize=(10, 1.8))\n",
    "gs = gridspec.GridSpec(1, 10, wspace=0.1, hspace=0.1)\n",
    "\n",
    "label = np.argmax(y_tmp, axis=1)\n",
    "proba = np.max(y_tmp, axis=1)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.imshow(X_tmp[i], cmap='gray', interpolation='none')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('{0} ({1:.2f})'.format(label[i], proba[i]),\n",
    "                  fontsize=12)\n",
    "\n",
    "print('\\nSaving figure')\n",
    "gs.tight_layout(fig)\n",
    "os.makedirs('img_fashion', exist_ok=True)\n",
    "plt.savefig('img_fashion/ex_00.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the classes in Fahsion MNIST have the following meanings:\n",
    "\n",
    "| Label | Description |\n",
    "| :---: |    :---:    |\n",
    "|   0   | T-shirt/top |\n",
    "|   1   |   Trouser   |\n",
    "|   2   |   Pullover  |\n",
    "|   3   |    Dress    |\n",
    "|   4   |    Coat     |\n",
    "|   5   |    Sandal   |\n",
    "|   6   |    Shirt    |\n",
    "|   7   |   Sneaker   |\n",
    "|   8   |     Bag     |\n",
    "|   9   |  Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "\n",
    "Download the github directory https://github.com/gongzhitaao/tensorflow-adversarial. In the folder \"attacks\" you will find other attacks in addition to fgsm\n",
    "\n",
    "3.1) Use the \"Jacobian-based Saliency Map Approach\" (jsma, available in saliency_map.py) to create adversarial examples using the MNIST dataset and the model previously trained. These examples should get \"1\" as prediction from the model. \n",
    "\n",
    "3.2) Complete the rest of the cells as necessary to plot the adversarial examples.\n",
    "\n",
    "3.3) Inspect the file img/adv_examples_ones.png.\n",
    "\n",
    "\n",
    "Suggestion: Copy the content of saliency_map.py to a new cell and modify the \"env\" environment so you could learn the adversarial examples in a similar way we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma(model, x, y=None, epochs=1, eps=1.0, k=1, clip_min=0.0, clip_max=1.0,\n",
    "         score_fn=lambda t, o: t * tf.abs(o)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tensorflow tensors into one \"enviroment\" to avoid\n",
    "# accidental overwriting.\n",
    "class Dummy:\n",
    "    pass\n",
    "env = Dummy()\n",
    "\n",
    "# We need a scope since the inference graph will be reused later\n",
    "with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the reuse=True flag\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    env.x_adv =     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nTraining')\n",
    "n_sample = X_train.shape[0]\n",
    "batch_size = 128\n",
    "n_batch = int(np.ceil(n_sample/batch_size))\n",
    "\n",
    "n_epoch = 2 # More epochs might be needed\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch {0}/{1}'.format(epoch+1, n_epoch))\n",
    "    for ind in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "        start = ind*batch_size\n",
    "        end = min(n_sample, start+batch_size)\n",
    "        sess.run(env.optim, feed_dict={env.x: X_train[start:end],\n",
    "                                       env.y: y_train[start:end],\n",
    "                                       env.training: True})\n",
    "    _evaluate(X_valid, y_valid, env)\n",
    "    \n",
    "\n",
    "print('\\nTesting against clean data')\n",
    "_evaluate(X_test, y_test, env)\n",
    "\n",
    "save_path = saver.save(sess, \"/tmp/mnist_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nCrafting adversarial')\n",
    "n_sample = X_test.shape[0]\n",
    "batch_size = 128\n",
    "n_batch = int(np.ceil(n_sample/batch_size))\n",
    "n_epoch = 20\n",
    "X_adv = np.empty_like(X_test)\n",
    "for ind in range(n_batch):\n",
    "    print(' batch {0}/{1}'.format(ind+1, n_batch), end='\\r')\n",
    "    start = ind*batch_size\n",
    "    end = min(n_sample, start+batch_size)\n",
    "    tmp = sess.run(env.x_adv, feed_dict={env.x: X_test[start:end],\n",
    "                                         env.y: y_test[start:end],\n",
    "                                         env.training: False})\n",
    "    X_adv[start:end] = tmp\n",
    "print('\\nSaving adversarial')\n",
    "os.makedirs('data', exist_ok=True)\n",
    "np.save('data/ex_00.npy', X_adv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTesting against adversarial data')\n",
    "_evaluate(X_adv_fashion, y_test_fashion, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = _predict(X_test, env)\n",
    "y2 = _predict(X_adv, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = np.argmax(y_test, axis=1)\n",
    "z1 = np.argmax(y1, axis=1)\n",
    "z2 = np.argmax(y2, axis=1)\n",
    "\n",
    "X_tmp = np.empty((10, 28, 28))\n",
    "y_tmp = np.empty((10, 10))\n",
    "fig = plt.figure(figsize=(10, 1.8))\n",
    "for i in range(10):\n",
    "    ind, = np.where(np.all([z0==i, z1==i, z2==1], axis=0))\n",
    "    cur = np.random.choice(ind)\n",
    "    X_tmp[i] = np.squeeze(X_adv[cur])\n",
    "    y_tmp[i] = y2[cur]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(np.argmax(y1, axis=1), return_counts=True))\n",
    "print(np.unique(np.argmax(y2, axis=1), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPlotting results')\n",
    "fig = plt.figure(figsize=(10, 1.8))\n",
    "gs = gridspec.GridSpec(1, 10, wspace=0.1, hspace=0.1)\n",
    "\n",
    "label = np.argmax(y_tmp, axis=1)\n",
    "proba = np.max(y_tmp, axis=1)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.imshow(X_tmp[i], cmap='gray', interpolation='none', vmin=0, vmax=0.005)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('{0} ({1:.2f})'.format(label[i], proba[i]),\n",
    "                  fontsize=12)\n",
    "\n",
    "print('\\nSaving figure')\n",
    "gs.tight_layout(fig)\n",
    "os.makedirs('img28', exist_ok=True)\n",
    "plt.savefig('img28/adv_examples_ones.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

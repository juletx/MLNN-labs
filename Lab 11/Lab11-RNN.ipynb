{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 9. Deep Neural Networks\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "In this Lab we learn how to solve sequential problems using RNNs. We will describe how can RNNs be implemented with tensorflow and evaluate their results in real problems. \n",
    "\n",
    "\n",
    "We will use material from the book \"Hands-On Machine Learning with Scikit-Learn and TensorFlow. Concepts, Tools, and Techniques to Build Intelligent Systems\" by Aurélien Géron. http://shop.oreilly.com/product/0636920052289.do that it is recommended as Bibliography of the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the python libraries required to solve the problems\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import itertools \n",
    "import os\n",
    "import sys\n",
    "import scipy.misc\n",
    "\n",
    "# We used some utilities from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Tensorflow library used for implementation of the RNNs\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb, mnist, fashion_mnist\n",
    "# Importing keras libraries\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import SimpleRNN, LSTM, Conv1D, MaxPooling1D\n",
    "from keras.layers import recurrent\n",
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding \n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "import csv\n",
    "from sklearn.externals.six.moves.urllib.request import urlopen\n",
    "\n",
    "# Used for plotting and display of figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series prediction with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first problem we consider is time series prediction with an RNN. Given a sequence of k contiguous points of the time series, $(t_{r},t_{r+1},\\dots,t_{r+k})$ that are used as inputs, we will try to predict the sequence $(t_{r+1},t_{r+2},\\dots,t_{r+k+1})$. Therefore, for each single point at time $t$, the target variable is the time series value at time $t+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function set the seeds of the tensorflow function\n",
    "# to make this notebook's output stable across runs\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function belows generates a particular time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series(t):\n",
    "    return t * np.sin(t) / 3 + 2 * np.sin(t*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To train the network we need sequences of size $k$ extracted from the time series. They are randomly selected using the function \"next_batch\" that outputs a set of <batch_size> training points, each point corresponding to a sequence of size k (in the function this size k is called \"n_steps\"). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min, t_max = 0, 30\n",
    "resolution = 0.1\n",
    "\n",
    "# Receives the number of samples (batch_size) of size (n_steps) to extract\n",
    "# from the time series, and outputs such a sample\n",
    "def next_batch(batch_size, n_steps):\n",
    "    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)\n",
    "    Ts = t0 + np.arange(0., n_steps + 1) * resolution\n",
    "    ys = time_series(Ts)\n",
    "    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we visualize the time series, and an example of training pairs $(x,y)$, where $x$ is a sequence of $k=20$ points and $y$ is the target sequence of $20$ points, the ones obtained by shifting $x$ only one position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The time series and the example of (x,y) are shown \n",
    "\n",
    "t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))\n",
    "\n",
    "n_steps = 20\n",
    "t_instance = np.linspace(12.2, 12.2 + resolution * (n_steps + 1), n_steps + 1)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"A time series (generated)\", fontsize=14)\n",
    "plt.plot(t, time_series(t), label=r\"$t . \\sin(t) / 3 + 2 . \\sin(5t)$\")\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"b-\", linewidth=3, label=\"A training instance\")\n",
    "plt.legend(loc=\"lower left\", fontsize=14)\n",
    "plt.axis([0, 30, -17, 13])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"A training instance\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"y*\", markersize=10, label=\"target\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "\n",
    "#save_fig(\"time_series_plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the parameters of the RNN are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The computation graph is initialized\n",
    "reset_graph()\n",
    "\n",
    "# n_steps is the size of the contiguous sequence that is taken from the TS\n",
    "# it coincides with the number of time steps for the RNN\n",
    "n_steps = 20\n",
    "\n",
    "# An RNN can take as inputs multiple input sequences (e.g, a matrix of n_seq x n_steps)\n",
    "# Since we use only a single TS, n_inputs=1\n",
    "n_inputs = 1\n",
    "\n",
    "# There is one layer of 100 recurrent neurons together\n",
    "n_neurons = 100\n",
    "\n",
    "# The ouput will be a single sequence\n",
    "n_outputs = 1\n",
    "\n",
    "# Notice the way the input and output are defined according to n_steps\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the following notebook cell we specify the type of RNN layer telling tensorflow which RNN cells it comprises. Remember that there are other, more efficient, memory cells, such as LSTM, GRU, etc.  \n",
    "\n",
    "In this example, a Basic RNN cell is defined, the layer has n_neurons and uses as activation function RELU. \n",
    "\n",
    "Finally, the OutputProjectionWrappers adds another layer that transforms the output of the n_neuron neurons into a single value, this function \"wraps\" the output of the recurrent neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),\n",
    "    #tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons),\n",
    "    output_size=n_outputs)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any other NN, we need to specify the loss function, and optimization algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate used by the gradient optimizer\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The loss function is the Minimum Square Error between the predicted\n",
    "# TS and the target TS\n",
    "loss = tf.reduce_mean(tf.square(outputs - y)) # MSE\n",
    "\n",
    "# Optimizer is ADAM\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# We will minimize the MSE\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Tensorflow variables are initialized\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is trained running \"n_iterations\" and training batch_size groups of training data. We will save the session at the end so we could use the network to predict new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2500\n",
    "batch_size = 50\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "    \n",
    "    saver.save(sess, \"./my_time_series_model\") # not shown in the book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training has worked well, then the MSE should have decreased with the iteractions. In the next cell the RNN is used to predict data from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  tf.Session() as sess:\n",
    "    # The session that we saved is restored\n",
    "    saver.restore(sess, \"./my_time_series_model\")  \n",
    "    # A segment of points is taken from the TS\n",
    "    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "    # Predictions are made\n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_new})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, the true TS values and the predictions are shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Testing the model\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"y*\", markersize=10, label=\"target\")\n",
    "plt.plot(t_instance[1:], y_pred[0,:,0], \"r.\", markersize=10, label=\"prediction\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "#save_fig(\"time_series_pred_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "The first exercise is similar to the previous example, it consists of predicting  the value of a time series (ISE.1 time series in this case) using the same time series until moment $t$. The difference is that we will also use another $7$ time series as input.\n",
    "\n",
    "We will use the \"ISTANBUL STOCK EXCHANGE Data Set\" from the UCI website (https://archive.ics.uci.edu/ml/datasets/ISTANBUL+STOCK+EXCHANGE#). It has been used to forecast the ISE100 index and to determine its daily directional movements. For more information about the meaning of the rate of return of a stock exchange you can see (https://en.wikipedia.org/wiki/Rate_of_return) but this is not relevant for the completion of the exercise. \n",
    "\n",
    "To solve the exercise you should: \n",
    "\n",
    "Modify the following cells to create an RNN that solves the problem taking into account the following characteristics:\n",
    "\n",
    "1. Sequence size for prediction: 30\n",
    "2. Number of neurons: 40\n",
    "3. You should compare the results of LSTM, GRU, and basic RNN cells (just write down the final MSE values you get with the three types of cells to determine which one is the best).\n",
    "4. Modify the network to predict the values of the time series taking into consideration the values of the other 7 timeseries, excluding the values of the target time series.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('data_akbilgic.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TS that will be predicted is plotted. Notice that it is not as clean as the toy problem shown in the example before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['ISE.1'])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"ISE.1 returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a correct estimation of the prediction quality we split the data between train and test in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size = df.shape[0]\n",
    "df_train = df.iloc[:int(df_size/2),:]\n",
    "df_test = df.iloc[int(df_size/2):,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is used to generate batches with which the RNN is trained. \n",
    "\n",
    "NOTICE: You will need to modify this function to answer the last part (point 4) of the exercise. But you do not need to change it to answer to complete the other parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives the number of samples (batch_size) of size (n_steps) to extract\n",
    "# from the time series, and outputs such a sample\n",
    "def next_stock_batch(batch_size, n_steps, df_base):\n",
    "    t_min = 0\n",
    "    t_max = df_base.shape[0]\n",
    "  \n",
    "    # The inputs will be formed by 8 sequences taken from\n",
    "    # 8 time series [ISE.1,SP,DAX,FTSE,NIKKEI,BOVESPA,EU,EM]\n",
    "    x = np.zeros((batch_size,n_steps,8))\n",
    "    \n",
    "    # We want to predict the returns of the Istambul stock\n",
    "    # taken into consideration the previous n_steps days\n",
    "    y = np.zeros((batch_size,n_steps,1))\n",
    "\n",
    "    # We chose batch_size random points from time series x-axis\n",
    "\n",
    "    starting_points = np.random.randint(0,t_max-n_steps-1,size=batch_size)    \n",
    "    #print(starting_points)\n",
    "\n",
    "    # We create the batches for x using all time series (8) between t and t+n_steps    \n",
    "    # We create the batches for y using only one time series between t+1 and t+n_steps+1\n",
    "    \n",
    "    for k in np.arange(batch_size):\n",
    "        lmat = []\n",
    "        for j in np.arange(n_steps+1):\n",
    "            lmat.append(df_base.iloc[starting_points[k]+j,2:].as_matrix())  \n",
    "            mat = np.array(lmat)\n",
    "        # The x values include all columns (mat[:n_steps,:]), these are ([ISE.1,SP,DAX,FTSE,NIKKEI,BOVESPA,EU,EM])\n",
    "        # and TS values in mat between 0 and n_steps\n",
    "        x[k,:,:] = mat[:n_steps]\n",
    "        \n",
    "        # The y values include only column 0 (mat[1:n_steps+1,0]), this is ([ISE.1]) \n",
    "        # and TS values in mat between 1 and n_steps+1\n",
    "        y[k,:,0] = mat[1:n_steps+1,0]\n",
    "   \n",
    "\n",
    "    return x,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the cell that comprises the definition of the RNN and its input parameters. You should modify this part to answer the questions of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The computation graph is initialized\n",
    "reset_graph()\n",
    "\n",
    "# n_steps is the size of the contiguous sequence that is taken from the TS\n",
    "# it coincides with the number of time steps for the RNN\n",
    "n_steps = 10\n",
    "\n",
    "# An RNN can take as inputs multiple input sequences (e.g, a matrix of n_seq x n_steps)\n",
    "n_inputs = 8\n",
    "\n",
    "# There is one layer of 60 recurrent neurons together\n",
    "n_neurons = 60\n",
    "\n",
    "# The ouput will be a single sequence\n",
    "n_outputs = 1\n",
    "\n",
    "# Notice the way the input and output are defined according to n_steps\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "\n",
    "\n",
    "basic = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),\n",
    "    output_size=n_outputs)\n",
    "\n",
    "\n",
    "basic_outputs, basic_states = tf.nn.dynamic_rnn(basic, X, dtype=tf.float32)\n",
    "\n",
    "# Learning rate used by the gradient optimizer (ADAM)\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The loss function is the Minimum Square Error between the predicted\n",
    "# TS and the target TS\n",
    "loss_basic = tf.reduce_mean(tf.square(basic_outputs - y)) # MSE\n",
    "\n",
    "# Optimizer is ADAM\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# We will minimize the MSE\n",
    "training_basic = optimizer.minimize(loss_basic)\n",
    "\n",
    "\n",
    "# Tensorflow variables are initialized\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "n_iterations = 500\n",
    "batch_size = 50\n",
    "printing_gap = 50\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_stock_batch(batch_size, n_steps, df_train)\n",
    "        \n",
    "        sess.run(training_basic, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        if iteration % printing_gap == 0:\n",
    "            mse = loss_basic.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tBasic MSE:\", mse)\n",
    "\n",
    "    \n",
    "    saver.save(sess, \"./istambul_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction for the train dataset (df_train)\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    # The session that we saved is restored\n",
    "    saver.restore(sess, \"./istambul_model\")  \n",
    "    # A segment of points is taken from the TS\n",
    "    X_new, y_new = next_stock_batch(1, n_steps, df_train)\n",
    "   \n",
    "    # Predictions are made\n",
    "    y_pred = sess.run(basic_outputs, feed_dict={X: X_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape,y_new.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(y_pred.reshape(n_steps),y_new.reshape(n_steps),'.')\n",
    "plt.xlabel(\"Target value in train\")\n",
    "plt.ylabel(\"Predicted value for train\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(y_pred.reshape(n_steps), label=\"prediction for train\")\n",
    "plt.plot(y_new.reshape(n_steps), label=\"target in train\")\n",
    "plt.xlabel(\" Time \")\n",
    "plt.ylabel(\"Stock return\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the predictions for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  tf.Session() as sess:\n",
    "    # The session that we saved is restored\n",
    "    saver.restore(sess, \"./istambul_model\")  \n",
    "    # A segment of points is taken from the TS\n",
    "    X_new, y_new = next_stock_batch(1, n_steps, df_test)\n",
    "   \n",
    "    # Predictions are made\n",
    "    y_pred = sess.run(basic_outputs, feed_dict={X: X_new})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape,y_new.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(y_pred.reshape(n_steps),y_new.reshape(n_steps),'.')\n",
    "plt.xlabel(\"Target value in train\")\n",
    "plt.ylabel(\"Predicted value for train\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(y_pred.reshape(n_steps), label=\"prediction for train\")\n",
    "plt.plot(y_new.reshape(n_steps), label=\"target in train\")\n",
    "plt.xlabel(\" Time \")\n",
    "plt.ylabel(\"Stock return\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the MNIST classification problem using an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The next two examples show how to solve the MNIST problem using an RNN and a \"deep\" RNN of only 3 layers. For the classification problem the idea here is to treat each image as a sequence of 28 rows of 28 pixels each. Therefore the number of steps for the RNN is 28. In each step it uses the current row of the image and the output of the RNN in the previous step to make its prediction. However, in terms of the solution quality, we are only interested in the final prediction of the network (what digit is represented in the image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_And_Normalize_Image_Dataset(db_name='mnist', max_train_samples = 10000,max_test_samples = 10000):\n",
    "\n",
    "    if db_name=='mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()               \n",
    "    elif db_name=='fashion':\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()        \n",
    "        \n",
    "    #print(x_train.shape,y_train.shape)\n",
    "    #print(x_test.shape,y_test.shape)\n",
    "\n",
    "        \n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    \n",
    "    #x_train = np.expand_dims(x_train, axis=3)\n",
    "    #x_test = np.expand_dims(x_test, axis=3)\n",
    "    #x_train = x_train.reshape((len(x_train), 28*28))\n",
    "    #x_test = x_test.reshape((len(x_test), 28*28))\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions serve to display RGB images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, vmin=None, vmax=None, greyscale=True):\n",
    "    plt.imshow(image, cmap=\"gray\" if greyscale else None, interpolation=\"nearest\", vmin=vmin, vmax=vmax)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = Read_And_Normalize_Image_Dataset(\"mnist\")\n",
    "\n",
    "\n",
    "# This line is to test you have downloaded the right dataset\n",
    "plot_image(X_train[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-layer RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters and the architecture of the one-layer are defined. Pay attention to the size of the inputs and the type of loss function used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "print(outputs, states)\n",
    "\n",
    "# The last layer will be a dense layer\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "\n",
    "# softmax cross-entropy is the loss function to optimize\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# A classification is correct if the most probable class corresponds\n",
    "# with the true class\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The network is trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "batch_size = 150\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size):\n",
    "            random_batch = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "            X_batch, y_batch = X_train[random_batch, :], y_train[random_batch]\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three-layer RNN \n",
    "\n",
    "See how the architecture of the RNN is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons_1 = 50\n",
    "n_neurons = 30\n",
    "\n",
    "n_outputs = 10\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# We define RNN cells of two different classes\n",
    "rnn_cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons_1)\n",
    "\n",
    "# The cells are stacked within the RNN\n",
    "stack_rnn = [rnn_cell1]\n",
    "for i in range(1, 3):\n",
    "    stack_rnn.append(tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons))\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(stack_rnn, state_is_tuple = True)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "\n",
    "# We need to transpose the outputs of the network because we are only interested\n",
    "# in the output of the last row. After transpose  the time axis is first\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "# tf.gather() is used for selecting the last frame.\n",
    "last = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "\n",
    "# The last layer will be a dense layer connecting all the last outputs\n",
    "# with a layer of the ten possible outputs we are interested in\n",
    "logits = tf.layers.dense(last, n_outputs)\n",
    "\n",
    "# softmax cross-entropy is the loss function to optimize\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# A classification is correct if the most probable class corresponds\n",
    "# with the true class\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size):\n",
    "            random_batch = np.random.randint(0, X_train.shape[0], size=batch_size)\n",
    "            X_batch, y_batch = X_train[random_batch, :], y_train[random_batch]\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock prediction with RNNs in Keras\n",
    "\n",
    "In this example we build RNN architectures to predict the stock market in a period between 2003 and 2008. First, information about 56 stocks is downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Retrieve the data from Internet\n",
    "\n",
    "# The data is from 2003 - 2008. This is reasonably calm: (not too long ago so\n",
    "# that we get high-tech firms, and before the 2008 crash). This kind of\n",
    "# historical data can be obtained for from APIs like the quandl.com and\n",
    "# alphavantage.co ones.\n",
    "#start_date = datetime(2003, 1, 1).date()\n",
    "#end_date = datetime(2008, 1, 1).date()\n",
    "\n",
    "symbol_dict = {\n",
    "    'TOT': 'Total',\n",
    "    'XOM': 'Exxon',\n",
    "    'CVX': 'Chevron',\n",
    "    'COP': 'ConocoPhillips',\n",
    "    'VLO': 'Valero Energy',\n",
    "    'MSFT': 'Microsoft',\n",
    "    'IBM': 'IBM',\n",
    "    'TWX': 'Time Warner',\n",
    "    'CMCSA': 'Comcast',\n",
    "    'CVC': 'Cablevision',\n",
    "    'YHOO': 'Yahoo',\n",
    "    'DELL': 'Dell',\n",
    "    'HPQ': 'HP',\n",
    "    'AMZN': 'Amazon',\n",
    "    'TM': 'Toyota',\n",
    "    'CAJ': 'Canon',\n",
    "    'SNE': 'Sony',\n",
    "    'F': 'Ford',\n",
    "    'HMC': 'Honda',\n",
    "    'NAV': 'Navistar',\n",
    "    'NOC': 'Northrop Grumman',\n",
    "    'BA': 'Boeing',\n",
    "    'KO': 'Coca Cola',\n",
    "    'MMM': '3M',\n",
    "    'MCD': 'McDonald\\'s',\n",
    "    'PEP': 'Pepsi',\n",
    "    'K': 'Kellogg',\n",
    "    'UN': 'Unilever',\n",
    "    'MAR': 'Marriott',\n",
    "    'PG': 'Procter Gamble',\n",
    "    'CL': 'Colgate-Palmolive',\n",
    "    'GE': 'General Electrics',\n",
    "    'WFC': 'Wells Fargo',\n",
    "    'JPM': 'JPMorgan Chase',\n",
    "    'AIG': 'AIG',\n",
    "    'AXP': 'American express',\n",
    "    'BAC': 'Bank of America',\n",
    "    'GS': 'Goldman Sachs',\n",
    "    'AAPL': 'Apple',\n",
    "    'SAP': 'SAP',\n",
    "    'CSCO': 'Cisco',\n",
    "    'TXN': 'Texas Instruments',\n",
    "    'XRX': 'Xerox',\n",
    "    'WMT': 'Wal-Mart',\n",
    "    'HD': 'Home Depot',\n",
    "    'GSK': 'GlaxoSmithKline',\n",
    "    'PFE': 'Pfizer',\n",
    "    'SNY': 'Sanofi-Aventis',\n",
    "    'NVS': 'Novartis',\n",
    "    'KMB': 'Kimberly-Clark',\n",
    "    'R': 'Ryder',\n",
    "    'GD': 'General Dynamics',\n",
    "    'RTN': 'Raytheon',\n",
    "    'CVS': 'CVS',\n",
    "    'CAT': 'Caterpillar',\n",
    "    'DD': 'DuPont de Nemours'}\n",
    "\n",
    "\n",
    "symbols, names = np.array(sorted(symbol_dict.items())).T\n",
    "\n",
    "quotes = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    print('Fetching quote history for %r' % symbol, file=sys.stderr)\n",
    "    url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'\n",
    "           'master/financial-data/{}.csv')\n",
    "    data = list(csv.DictReader(l.decode('utf8')\n",
    "                               for l in urlopen(url.format(symbol))))\n",
    "    quotes.append({'close': [float(rec['close']) for rec in data],\n",
    "                   'open': [float(rec['open']) for rec in data]})\n",
    "\n",
    "close_prices = np.vstack([q['close'] for q in quotes])\n",
    "open_prices = np.vstack([q['open'] for q in quotes])\n",
    "\n",
    "# The daily variations of the quotes are what carry most information\n",
    "variation = close_prices - open_prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize data for two of the time series, AAPL and AIG indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = np.arange(400,650)\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Variation for AAPL index\", fontsize=14)\n",
    "plt.plot(variation[0,period],'b')\n",
    "#plt.plot(variation[1,:],'r.')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Variation for AIG index\", fontsize=14)\n",
    "plt.plot(variation[1,period],'r-')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Variation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary function to create batches for a single time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Create_Data(data,look_back,number_samples):\n",
    "    n_seq = data.shape[-1]    \n",
    "    X = np.zeros((number_samples,look_back))\n",
    "    # We select random positions to get the sequences for learning\n",
    "    pos =  look_back + np.random.randint(n_seq-look_back,size=number_samples)\n",
    "    \n",
    "    # The target variable is the stock time series at that position\n",
    "    Y = data[pos]\n",
    "    for i in np.arange(number_samples):\n",
    "        X[i,:] = data[(pos[i]-look_back):pos[i]]\n",
    "        \n",
    "        \n",
    "    return X,Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first approach we will predict one time series using a sequence of lookup observations from the same time series, i.e., input_features=look_back and output_features=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test datasets are created. Observe the way in which they are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_observations = variation.shape[-1]\n",
    "indices_for_train = np.arange(0,int(number_observations/2))\n",
    "indices_for_test = np.arange(int(number_observations/2),number_observations)\n",
    "\n",
    "look_back = 5\n",
    "nsamples = 1000\n",
    "\n",
    "trainX, trainY = Create_Data(variation[0,indices_for_train],look_back,nsamples)\n",
    "testX, testY = Create_Data(variation[0,indices_for_test],look_back,nsamples)\n",
    "\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX,   (testX.shape[0], testX.shape[1],    1))\n",
    "trainY = np.reshape(trainY, (trainY.shape[0],1))\n",
    "testY = np.reshape(testY, (testY.shape[0], 1))\n",
    "\n",
    "trainX.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Simple RNN model is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_cells = 25\n",
    "\n",
    "stock_model = Sequential ()\n",
    "stock_model.add(SimpleRNN(number_cells, input_shape =(look_back, 1) ))\n",
    "stock_model.add(Dropout(0.2))\n",
    "stock_model.add (Dense (activation = 'linear',units=1))\n",
    "stock_model.compile (loss =\"mean_squared_error\" , metrics=['mse'], optimizer = \"adam\")  \n",
    "print(stock_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 50\n",
    "batch_size = 16\n",
    "history = stock_model.fit(trainX, trainY, validation_data=(testX,testY), batch_size =batch_size, \n",
    "                epochs =nepochs, shuffle = False)\n",
    "\n",
    "\n",
    "stock_model_train_error = stock_model.evaluate(trainX, trainY, batch_size=batch_size)\n",
    "stock_model_test_error = stock_model.evaluate(testX, testY, batch_size=batch_size)\n",
    "print(stock_model_train_error,stock_model_test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Variation for AIG index\", fontsize=14)\n",
    "plt.plot(history.history[\"mse\"],'r-')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "plt.title(\"Variation for AIG index\", fontsize=14)\n",
    "plt.plot(history.history[\"val_mse\"],'r-')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"VAL_MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "In the previous example we predicted the variation for the first stock variation[0,:] using only information about that stock. In the next exercise we would like to use information from all the stocks. \n",
    "\n",
    "Function Create_Data_Multiple, creates batches for solving this problem. \n",
    "\n",
    "Define a Keras model that contains a single LSTM with 50 cells and one fully connected hidden layer with 50 neurons. Between each layer, include a dropout, with 0.2 probability. Select an adecquate loss function for this problem.\n",
    "\n",
    "Finally, show the accuracy of the model both in the training set, and in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Data_Multiple(data,look_back,number_samples,index_to_predict):\n",
    "    n_seq = data.shape[-1]   \n",
    "    n_col = data.shape[0] \n",
    "    X = np.zeros((number_samples,look_back,n_col))\n",
    "    # We select random positions to get the sequences for learning\n",
    "    pos =  look_back + np.random.randint(n_seq-look_back,size=number_samples)\n",
    "    \n",
    "    # The target variable is the stock time series at that position\n",
    "    Y = data[index_to_predict,pos]\n",
    "    for i in np.arange(number_samples):\n",
    "        X[i,:,:] = data[:,(pos[i]-look_back):pos[i]].transpose()\n",
    "        \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 10000\n",
    "look_back = 30\n",
    "index_to_predict = 0\n",
    "indices_to_take = np.arange(0,56)\n",
    "reduced_variation = variation[indices_to_take,:]\n",
    "\n",
    "trainX, trainY = Create_Data_Multiple(reduced_variation[:,indices_for_train],\n",
    "                                      look_back,nsamples,index_to_predict)\n",
    "\n",
    "testX, testY = Create_Data_Multiple(reduced_variation[:,indices_for_test],\n",
    "                                      look_back,nsamples,index_to_predict)\n",
    "trainY = np.reshape(trainY, (trainY.shape[0],1))\n",
    "testY = np.reshape(testY, (testY.shape[0], 1))\n",
    "\n",
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_cells = 50\n",
    "\n",
    "stock_model = Sequential ()\n",
    "stock_model.add()\n",
    "stock_model.add()\n",
    "stock_model.add()\n",
    "stock_model.add()\n",
    "stock_model.compile()  \n",
    "print(stock_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 40\n",
    "batch_size = 16\n",
    "history = stock_model.fit(trainX, trainY, validation_data=(testX,testY), batch_size =batch_size, \n",
    "                epochs =nepochs, shuffle = False)\n",
    "\n",
    "\n",
    "stock_model_train_error = stock_model.evaluate(trainX, trainY, batch_size=batch_size)\n",
    "stock_model_test_error = stock_model.evaluate(testX, testY, batch_size=batch_size)\n",
    "print(stock_model_train_error,stock_model_test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow implementation of a RNN\n",
    "\n",
    "In the following cell, we set common parameters of a RNN implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "num_epochs = 500\n",
    "total_series_length = 10000\n",
    "truncated_backprop_length = 20\n",
    "state_size = 4\n",
    "batch_size = 20\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to redefine the time series we are working with, so that the values are independent of the place we take the batch from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series(t):\n",
    "    return  np.sin(t) / 3 + 2 * np.sin(t*5)\n",
    "\n",
    "t_min, t_max = 0, 30\n",
    "resolution = 0.1\n",
    "\n",
    "# Receives the number of samples (batch_size) of size (n_steps) to extract\n",
    "# from the time series, and outputs such a sample\n",
    "def next_batch(btch_sz, n_steps):\n",
    "    t0 = np.random.rand(btch_sz, 1) * (t_max - t_min - n_steps * resolution)\n",
    "    ts = t0 + np.arange(0., n_steps + 1) * resolution\n",
    "    ys = time_series(ts)\n",
    "    return ys[:, :-1].reshape(-1, n_steps), ys[:, 1:].reshape(-1, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, truncated_backprop_length])\n",
    "Y = tf.placeholder(tf.int32, [None, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [None, state_size])\n",
    "\n",
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, 1),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,1)), dtype=tf.float32)\n",
    "\n",
    "# Unpack columns\n",
    "inputs_series = tf.unstack(X, axis=1)\n",
    "g_truth_series = tf.unstack(Y, axis=1)\n",
    "\n",
    "print(len(inputs_series), X)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the recurrent part of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = init_state\n",
    "states_series = []\n",
    "\n",
    "for current_input in inputs_series:\n",
    "    current_input = tf.reshape(current_input, [-1, 1])\n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state], 1)  \n",
    "\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  \n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state\n",
    "\n",
    "predictions_series = [tf.matmul(state, W2) + b2 for state in states_series] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [tf.losses.mean_squared_error(pred, tf.reshape(val, (-1,1))) for pred, val in zip(predictions_series, g_truth_series)]\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.001).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = next_batch(batch_size, total_series_length)\n",
    "\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "            #print(batchY.shape)\n",
    "\n",
    "            _total_loss, _train_step, _current_state = sess.run([total_loss, train_step, current_state],feed_dict={X:batchX, Y:batchY, init_state:_current_state})\n",
    "\n",
    "\n",
    "        print(\"Epoch\",epoch_idx, \"Loss\", _total_loss)\n",
    "\n",
    "\n",
    "    saver.save(sess, \"./myRNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, \"./myRNN\")\n",
    "\n",
    "    x, y = next_batch(batch_size, truncated_backprop_length)\n",
    "\n",
    "    p = np.array(sess.run(predictions_series, feed_dict={X: x, init_state: np.zeros((batch_size, state_size))}))\n",
    "\n",
    "    print(p.shape, x.shape)\n",
    "\n",
    "    plt.plot(p[:,0], label=\"prediction\")\n",
    "    plt.plot(x[0,:], label=\"truth\")\n",
    "    plt.plot(y[0,:], label=\"truth+1\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "### Solving the MNIST classification problem using an RNN.\n",
    "\n",
    "Below is an incomplete model of an application of an LSTM to solve the MNIST problem. Here, we want to solve the same problem that was treated before Exercise 2, but using Keras instead of tensorflow.\n",
    "\n",
    "For that, we define a model onsisting of a single LSTM with 50 neurons, and a single dense layer. Therefore, the model will only have one single hidden layer, with 50 neurons.\n",
    "\n",
    "After training the model, obtain its accuracy in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_mnist_train, y_mnist_train), (x_mnist_test, y_mnist_test) \\\n",
    "                                = Read_And_Normalize_Image_Dataset(db_name='mnist', \\\n",
    "                                max_train_samples = 10000, max_test_samples = 10000)\n",
    "\n",
    "\n",
    "print(\"Dimension of the samples in the training set: \", x_mnist_train.shape)\n",
    "print(\"Dimension of samples in the test set: \", x_mnist_test.shape)\n",
    "\n",
    "number_mnist_classes = 10\n",
    "y_mnist_train_one_hot  = to_categorical(y_mnist_train)\n",
    "y_mnist_test_one_hot  = to_categorical(y_mnist_test)\n",
    "\n",
    "print(\"Dimension of the labels training set (one_hot): \", y_mnist_train_one_hot.shape)\n",
    "print(\"Dimension of the labels test set (one_hot): \", y_mnist_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_neurons = 50\n",
    "time_steps = 1\n",
    "n_inputs = 28\n",
    "number_classes = 10\n",
    "number_epochs = 10\n",
    "batch_size = 200\n",
    "nepochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_cells = 50\n",
    "\n",
    "mnist_model = Sequential ()\n",
    "mnist_model.add()\n",
    "mnist_model.add()\n",
    "mnist_model.add()\n",
    "mnist_model.add()\n",
    "mnist_model.compile()  \n",
    "print(mnist_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mnist_model.fit(x_mnist_train, y_mnist_train_one_hot, validation_data=(x_mnist_test,y_mnist_test_one_hot), \n",
    "                          batch_size=batch_size, epochs=nepochs, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(mnist_model.predict(x_mnist_test), axis=1)\n",
    "print(\"The accuracy on test data was:\", str(np.sum(1*(y_mnist_test == preds))/preds.shape[0]*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "We have taken advantage of the RNN implementations available in both Keras and Tensorflow. Now, we are digging one step deeper into the implementation, by implementing RNN cells ourselves. Following the example, define different variants of the common RNN (e.g., LSTM or GRU). Take a look at https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767.\n",
    "\n",
    "Try to improve the results obtained by the tensorflow RNN defined for the sinusoidal example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

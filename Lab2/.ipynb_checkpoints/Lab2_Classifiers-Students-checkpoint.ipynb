{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1. Introduction to Machine learning\n",
    "\n",
    "In this Lab we will firstly see a simple example of a classification problem.\n",
    "\n",
    "Afterwards, we will focus on three different questions related to ML:\n",
    "\n",
    "\n",
    "1) Learn how to use a sklearn to solve ML tasks (classification and regression)\n",
    "\n",
    "2) Illustrate important concepts in ML (imputation, validation, standarization, etc.) \n",
    "\n",
    "3) Learn how to create sophisticated ML pipelines for real-world problems\n",
    "\n",
    "\n",
    "We will use material from the books:\n",
    "\n",
    "- \"Hands-On Machine Learning with Scikit-Learn and TensorFlow. Concepts, Tools, and Techniques to Build Intelligent Systems\" by Aurélien Géron. http://shop.oreilly.com/product/0636920052289.do \n",
    "\n",
    "- \"Deep Learning with Python\" by F. Chollet. https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438\n",
    "\n",
    "which are recommended as Bibliography of the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juletx/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Imputer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-959a3625642d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Imputation methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Methods for preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Imputer'"
     ]
    }
   ],
   "source": [
    "# We start by importing the python libraries required to solve the problems\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Enables interaction with the plots\n",
    "%matplotlib notebook\n",
    "\n",
    "# These are modules contained in sklearn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, SCORERS\n",
    "from sklearn.utils.testing import all_estimators\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "# Imputation methods\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Methods for preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "# Feature selection Methods \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "# Methods for classifier validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Datasets in sklearn\n",
    "import sklearn.datasets as data_load\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Enables interactivity with the plots\n",
    "%matplotlib notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(points_list, labels=None, line=None):\n",
    "    plt.figure()\n",
    "    if labels is None:\n",
    "        labels = [\"\"] * len(points_list)\n",
    "    for points, label in zip(points_list, labels):\n",
    "        plt.scatter(points[0], points[1], label=label)\n",
    "    plt.xlabel(r'$x$', fontsize=fsize)\n",
    "    plt.ylabel(r'$y$', fontsize=fsize)\n",
    "    if line is not None:\n",
    "        plt.plot(line[0], line[1],'m',lw=4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create two sets of points (XA,yA)  and (XB,yB) corresponding to two different classes\n",
    "\n",
    "number_points_Class_A = 50\n",
    "number_points_Class_B = 50\n",
    "\n",
    "# Font size\n",
    "fsize = 20     \n",
    "\n",
    "# Points in Class A\n",
    "xA = 20*np.random.rand(number_points_Class_A)\n",
    "shiftA = 20*np.random.rand(number_points_Class_A)\n",
    "yA = (4+xA)/2.0 - shiftA - 0.1\n",
    "\n",
    "# Points in Class B\n",
    "xB = 20*np.random.rand(number_points_Class_B)\n",
    "shiftB = 20*np.random.rand(number_points_Class_B)\n",
    "yB = (4+xB)/2.0 + shiftB + 0.1\n",
    "\n",
    "# Hyperplane dividing the two classes\n",
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = (4+x1)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The points in the two classes are visualized with different colors \n",
    "# Points in Class I in blue. Points in Class II in orange\n",
    "plot_points([[xA, yA], [xB, yB]], labels=[\"Class I\", \"Class II\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We show that there exists a hyperplane that allows to perfectly divide \n",
    "# points in the two classes.\n",
    "# In 2-d this hyperplane corresponds to a line that is represented in green.\n",
    "\n",
    "plot_points([[xA, yA], [xB, yB]], labels=[\"Class I\", \"Class II\"], line=[x1, y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a more difficult classification problem\n",
    "# where the two region classes overlap.\n",
    "\n",
    "# Font size\n",
    "fsize = 20     \n",
    "\n",
    "# Points in Class A\n",
    "xA1 = 20*np.random.rand(number_points_Class_A)\n",
    "shiftA1 = 20*np.random.rand(number_points_Class_A)\n",
    "yA1 = (4+xA1)/2.0 - shiftA1 + 5.0\n",
    "\n",
    "# Points in Class B\n",
    "xB1 = 20*np.random.rand(number_points_Class_B)\n",
    "shiftB1 = 20*np.random.rand(number_points_Class_B)\n",
    "yB1 = (4+xB1)/2.0 + shiftB1 - 5.0\n",
    "\n",
    "# Hyperplane dividing the two classes\n",
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = (4+x1)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The points corresponding to the two classes are plotted\n",
    "\n",
    "plot_points([[xA1, yA1], [xB1, yB1]], labels=[\"Class I\", \"Class II\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same hyperplane to divide points from the two classes\n",
    "# However, as it can be appreciated, the classification provided by this hyperplane\n",
    "# is not perfect\n",
    "\n",
    "plot_points([[xA1, yA1], [xB1, yB1]], labels=[\"Class I\", \"Class II\"], line=[x1, y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a more difficult classification problem\n",
    "# where the two region classes seem to overlap and where\n",
    "# it is not evident that a linear separator exists.\n",
    "\n",
    "\n",
    "# Font size\n",
    "fsize = 20     \n",
    "\n",
    "# Points in Class A\n",
    "xA2 = 20*np.random.rand(number_points_Class_A)\n",
    "shiftA2 = 20*np.random.rand(number_points_Class_A)\n",
    "yA2 = 20*np.cos(0.2*np.pi*xA2) - shiftA2 \n",
    "\n",
    "# Points in Class B\n",
    "xB2 = 20*np.random.rand(number_points_Class_B)\n",
    "shiftB2 = 20*np.random.rand(number_points_Class_B)\n",
    "yB2 = 20*np.cos(0.2*np.pi*xB2) + shiftB2 \n",
    "\n",
    "# Sinusoidal curve dividing the two classes      \n",
    "x2 = np.linspace(0, 20, 2000)\n",
    "y2 = 20*np.cos(0.2*np.pi*x2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The points corresponding to the two classes are plotted\n",
    "\n",
    "plot_points([[xA2, yA2], [xB2, yB2]], labels=[\"Class I\", \"Class II\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we plot in green the same line that was previously used to classify the previous data\n",
    "# It can be seen that it does not provide a good separation of the data\n",
    "\n",
    "plot_points([[xA2, yA2], [xB2, yB2]], labels=[\"Class I\", \"Class II\"], line=[x1, y1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we also show a curve that makes a perfect separation of the data\n",
    "\n",
    "\n",
    "plot_points([[xA2, yA2], [xB2, yB2]], labels=[\"Class I\", \"Class II\"], line=[x2, y2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A very simple binary classification problem \n",
    "For our first classification problem, we will take the initial, easy problem back. As previously, we can divide create a perfect classifier by using a hyperplane (a line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 2-d this hyperplane corresponds to a line that is represented in green.\n",
    "\n",
    "plot_points([[xA, yA], [xB, yB]], labels=[\"Class I\", \"Class II\"], line=[x1, y1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning simple classifiers using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The common syntax for learning a classifier in sklearn implies three steps: \n",
    "\n",
    "- Declaration of the classifier\n",
    "- Fitting the classifier from the data and the labels\n",
    "- Using the classifier to predict new data\n",
    "\n",
    "In the example below we learn a logistic regression classifier that separates the two classes presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create the training data that is needed since we are working with a supervised classification algorithm. By uncommenting the \"print\" line you can check the size of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the labels for our two classes A and B. Variable c will keep the \n",
    "# labels of all points.\n",
    "# The points in Class A will have label 1 assigned and the points in Class B, \n",
    "# will have label 0.\n",
    "\n",
    "c = np.hstack((np.ones((number_points_Class_A)),np.zeros((number_points_Class_B))))\n",
    "#print(c.shape)\n",
    "\n",
    "# We create the training data concatenating examples from the two classes XA and XB\n",
    "tr_data = np.hstack((np.vstack((xA,yA)),np.vstack((xB,yB)))).transpose()\n",
    "#print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the classifier (it is a logistic regression classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn the classifier. In sklearn this is done using the function fit(data,classes)\n",
    "It will update the \"lr\" model using the data from training_data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(tr_data,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the logistic regression classifier lr to predict the classes of the dataset. In the example below we predict the classes of the same dataset used for training (training_data) but this is not a realistic case. Usually we predict the classes of data that was not used for training (called the test data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of the classifier on the training data\n",
    "prediction_training = lr.predict(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualization of the predictions given by the classifier\n",
    "\n",
    "A classifier is accurate if for most of the points the class predicted coincide with the real classes. In this sense, for a binary problem, each prediction can be of four types:\n",
    "\n",
    "1) true positive: A positive point is given positive prediction\n",
    "\n",
    "2) false positive:  A negative point is given positive prediction\n",
    "\n",
    "3) true negative: A negative point is given negative prediction\n",
    "\n",
    "4) false negative:  A positive point is given  negative prediction\n",
    "\n",
    "Below, we put each classifier prediction in one of the four groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font size\n",
    "fsize = 20   \n",
    "\n",
    "# We identify the points that were correctly and wrongly classified\n",
    "tp = np.where(prediction_training[:number_points_Class_A]==1)[0]  # True positive\n",
    "fn = np.where(prediction_training[:number_points_Class_A]==0)[0]  # False negative\n",
    "tn = number_points_Class_A+np.where(prediction_training[number_points_Class_A:]==0)[0]  # True negative\n",
    "fp = number_points_Class_A+np.where(prediction_training[number_points_Class_A:]==1)[0]  # False positive\n",
    "\n",
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = -lr.coef_[0][0]/lr.coef_[0][1]*x1 + lr.intercept_\n",
    "\n",
    "\n",
    "plot_points([[tr_data[tp,0], tr_data[tp,1]], [tr_data[fn,0], tr_data[fn,1]], [tr_data[tn,0], tr_data[tn,1]], [tr_data[fp,0], tr_data[fp,1]]], \n",
    "            labels=['True positive', 'False negative', 'True negative', 'False positive'], line=[x1, y1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing our own classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to use a Logistic Regression classifier, we are going to implement a basic classifier ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "The following cells contain an incomplete implementation of a logistic regression classifier. Based on the contents learned in class, complete it, and test whether it can perform as accurately as the one from sklearn. Use the accuracy generated from employing a 5-fold cross-validation scheme.\n",
    "\n",
    "(This implementation is an adaptation of that found in the *Python machine learning* book, Raschka, S., & Mirjalili, V. (2017). Packt Publishing Ltd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression(object):\n",
    "    def __init__(self, eta=0.01, n_iter=1000, random_state=0):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def activation(self, z):\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "        \n",
    "    def fit(self, X, y):        \n",
    "        self.w = self.rgen.normal(loc=0, scale=0.01, size=X.shape[1])\n",
    "        self.b = self.rgen.normal(loc=0, scale=0.01, size=1)\n",
    "        self.cost = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors =                                        # The error is computed as the difference between the \n",
    "                                                            # prob. of the class and the prediction of the model\n",
    "            self.w += self.eta * X.T.dot(errors)\n",
    "            self.b = self.eta * errors.sum()\n",
    "            \n",
    "            cost = (-y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))))\n",
    "            self.cost.append(cost)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return  # Given the features \"predict\" outputs the classification given by the model\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return # Given the features predict_proba outputs the probability that the solution belongs to the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an observation belonging to a certain class, as computed by Logistic Regression, can be defined as:\n",
    "\n",
    "$$P(y|x)=sigm(wx+b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylr = MyLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylr.fit(tr_data,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mylr.predict(tr_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Using the plot_points function, create a plot in which the the true and false positives and negatives are shown, along with the line computed by the MyLogisticRegression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = \n",
    "\n",
    "# We identify the points that were correctly and wrongly classified\n",
    "tp = \n",
    "fn = \n",
    "tn = \n",
    "fp = \n",
    "\n",
    "plot_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real-word classification problem and more sophisticated validation schemes\n",
    "\n",
    "\n",
    "sklearn also contains a number of databases that can be used to test the algorithms. We will use some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can check which are the datasets included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available datasets:\")\n",
    "[name for name in data_load.__all__ if \"load\" in name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Inspecting the Real-World datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the breast cancer dataset, included in UCI ML Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "\n",
    "\n",
    "It has been used for the application of ML to Cancer diagnosis and prognosis: http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is loaded\n",
    "breast_cancer_data = data_load.load_breast_cancer()\n",
    "\n",
    "#Display options\n",
    "# These options determine the way floating point numbers, \n",
    "# arrays and other NumPy objects are displayed\n",
    "np.set_printoptions(suppress=True, threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to inspect the dataset before applying any ML technique, its header and also the characteristics of the data. \n",
    "\n",
    "For example, it is very important to know the number of attributes (variables), their type, and also the size of the data (number of instances). In the description that is shown below you will find this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some information about the dataset, understand what we are aiming for\n",
    "print(breast_cancer_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze more details of the database. Rows define observations (instances of our classification problem). Columns represent variables captured in each observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the rows shown above that the range of values change among the columns. Some columns seem to have values between 0 and 1 and others much higher values. This has to be taken into account for the application of the classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this a binary problem, classes are either 0: the tumor is malign or 1: the tumor is benign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classes in the database\n",
    "breast_cancer_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous analysis, notice that we have used:   data[\"data\"] to visualize the features and data[\"target\"] to see the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing data\n",
    "\n",
    "Imputation methods serve to substitute missing values in the data. In the example below we define a small dataset of three variables, six instances and three missing values (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.array([[ 'NaN',   7,     6],\n",
    "                    [  5   ,  89,    13],\n",
    "                    [ 23   ,  12,   213],\n",
    "                    [  2   ,  87, 'NaN'],\n",
    "                    [  8   , 101,    71],\n",
    "                    [ 13   ,'NaN',    20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Imputer\" class allows to impute the missing data. It implements three strategies: \"mean\", \"median\", and \"most_frequent\".\n",
    "If works like the classifiers:\n",
    "\n",
    "1) Define the imputer\n",
    "\n",
    "2) Fit the imputer to the data using the function \"fit\"\n",
    "\n",
    "3) Impute the data using the function \"transform\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the imputer\n",
    "mean_imputer = Imputer(missing_values='NaN',strategy=\"mean\",axis=0)\n",
    "\n",
    "#  Fit the imputer\n",
    "mean_imputer.fit(my_data)\n",
    "\n",
    "# Transform (impute) the data\n",
    "imputed_data = mean_imputer.transform(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data can improve the accuracy of some classifiers. This can be done in a similar way in which we define the classifiers:\n",
    "\n",
    "1) Define the scaler\n",
    "\n",
    "2) Fit the scaler to the data using the function \"fit\"\n",
    "\n",
    "3) Scale the data using the function \"transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the data to scaler\n",
    "scaler.fit(X=breast_cancer_data[\"data\"])\n",
    "\n",
    "# Scale the data \n",
    "scaled_data = scaler.transform(X=breast_cancer_data[\"data\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample process can be done in only two steps using the functions fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit and scale the data \n",
    "scaled_data = scaler.fit_transform(X=breast_cancer_data[\"data\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful pre-processing algorithm is to binarize the data. It is NOT defined as classifiers, imputer, and scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized_data = binarize(scaled_data)\n",
    "print(binarized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection \n",
    "\n",
    "Feature selection is an important step in traditional classifiers. I allows to reduce the dimensionality of the data. This can be done in a similar way in which we define the classifiers:\n",
    "\n",
    "1) Define the feature selection method\n",
    "\n",
    "2) Fit the feature-selection method to the data using the function \"fit\"\n",
    "\n",
    "3) Select  the data using the function \"transform\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below a an information metric approach is used to select the two most informative features for the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature selection method is defined\n",
    "feature_selection = SelectKBest(f_classif,k=2)\n",
    "\n",
    "# Using fit_transform we select fit the selector to the data and finally select features\n",
    "new_features = feature_selection.fit_transform(scaled_data, breast_cancer_data[\"target\"])\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "We want to normalize the data using the \"preprocessing.MinMaxScaler()\" function that sets each variable to the range (0,1) and then selecting the two principal components of the data to visualize how they are related with the two classes of the Breast Cancer problem.\n",
    "\n",
    "Below, replace the XXX in LINE 1--LINE 5. Then execute the following cell to visualize the select features.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The scaler is defined\n",
    "min_max_scaler = XXX                              #LINE1\n",
    "    \n",
    "# The scaler is fitted\n",
    "min_max_scaler.XXX(breast_cancer_data[\"data\"])    #LINE2\n",
    "    \n",
    "# The scaler is applied\n",
    "scaled_to_min_max = min_max_scaler.transform(XXX) #LINE3\n",
    "\n",
    "\n",
    "# The pca is defined\n",
    "XXX = PCA(n_components=2)                         #LINE4\n",
    "\n",
    "# pca is fitted to the data\n",
    "pca.fit(scaled_to_min_max)\n",
    "\n",
    "# the 2 main principal components are extracted\n",
    "reduced_data = pca.XXX(XXX)                       #LINE5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to visualize the two created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font size\n",
    "fsize = 10     \n",
    "\n",
    "pos_positive = np.where(breast_cancer_data[\"target\"]==1)\n",
    "pos_negative = np.where(breast_cancer_data[\"target\"]==0)\n",
    "\n",
    "plt.figure(figsize=(7.5,3.75))\n",
    "plt.plot(reduced_data[pos_positive,0], reduced_data[pos_positive,1], 'ro')\n",
    "plt.plot(reduced_data[pos_negative,0], reduced_data[pos_negative,1], 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class A')\n",
    "red_patch = mpatches.Patch(color='red', label='Class B')\n",
    "plt.legend(handles=[blue_patch,red_patch])\n",
    "\n",
    "plt.xlabel(r'$feature_1$', fontsize=fsize)\n",
    "plt.ylabel(r'$feature_2$', fontsize=fsize)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and validating classifiers \n",
    "\n",
    "One common way of evaluating the quality of a classifier is applying cross-validation.\n",
    "We remind that cross-validation is a way to estimate the accuracy of a classifier\n",
    "using the training data.\n",
    "It works as follows:\n",
    "\n",
    "1) First, the training data is divided into k folds.\n",
    "\n",
    "2) Then, we repeat k times the following step: We use (k-1) folds to train a classifier\n",
    "and predict the classes of intances in the other k-th fold using tha classifier. \n",
    "\n",
    "The procedure is done predicting every time a different fold. \n",
    "\n",
    "Below, we show how cross-validation is implemented in sk-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the classifier accuracy using k-fold cross-validation with k=5. The result of cross-validation will be the predictions for all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cross_val_predict(dt,breast_cancer_data.data, \n",
    "                               breast_cancer_data.target,cv=5)\n",
    "\n",
    "# Let us print the predictions\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the prediction  and the target (true class value) we can compute different accuracy measures for the classifier.  We do this for the accuracy metric below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_accuracy = metrics.accuracy_score(breast_cancer_data.target, prediction) \n",
    "print(\"The accuracy of the logistic regression classifier, as computed \\\n",
    "using 5-fold crossvalidation, is: \",dt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also compute the confusion matrix for the predictions made by the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_confusion_matrix = metrics.confusion_matrix(breast_cancer_data.target, prediction)\n",
    "print(\"Confusion matrix for the predictions made by the logistic regression classifier:\")\n",
    "print(dt_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    " \n",
    "  Learn a classifier for the breast cancer dataset and estimate its accuracy. The classifier should have the following characteristics:\n",
    "   \n",
    "  1) Use a logistic regression classifier\n",
    "  \n",
    "  2) Do the predictions using cross-validation (number of folds k=10) \n",
    "  on the scaled (StandardScaler) data\n",
    "  \n",
    "  3) Compute the confusion matrix \n",
    "  \n",
    " \n",
    " Suggestions: \n",
    " \n",
    " - Copy-paste the previous cells and substitute decision tree by logistic regression\n",
    " - Use scaled data instead of original data\n",
    " - Modify the parameters of the cross-validation to have the number of folds requested \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "# Define the scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit and scale the data \n",
    "scaled_data = scaler.fit_transform(X=breast_cancer_data[\"data\"])\n",
    "\n",
    "prediction = cross_val_predict(lr,scaled_data, \n",
    "                               breast_cancer_data.target,cv=10)\n",
    "\n",
    "lr_confusion_matrix = metrics.confusion_matrix(breast_cancer_data.target, prediction)\n",
    "print(\"Confusion matrix for the predictions made by the logistic regression classifier:\")\n",
    "print(lr_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning pipelines \n",
    "\n",
    "Machine learning pipeline allows to join in a single pipeline a number of processing steps. They sequentially apply a list of transformations and a final estimator (classifier, regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we define two pipelines with different components: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components of the first pipeline\n",
    "knn = KNeighborsClassifier()    \n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# The first pipeline is defined\n",
    "knn_scale_pipeline = Pipeline([(\"scaler\", scaler), (\"k-NN\", knn)])\n",
    "\n",
    "\n",
    "# The second pipeline is defined using components from the previous exercise.\n",
    "# If you did not complete the exercise you can define the components here\n",
    "\n",
    "#three_steps_pipeline = Pipeline([(\"mm_scaler\", min_max_scaler),\n",
    "#                                 (\"pca\", pca ), (\"dt_classifier\", dt)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell pipeline1 is used to predict the classes of the breast cancer problem. Then, the accuracy of the predictions is computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1_prediction = cross_val_predict(knn_scale_pipeline,breast_cancer_data.data, \n",
    "                               breast_cancer_data.target,cv=5)\n",
    "\n",
    "pipeline1_accuracy = metrics.accuracy_score(breast_cancer_data.target, \n",
    "                                                     pipeline1_prediction) \n",
    "\n",
    "print(\"The accuracy of the secondpipeline computed using crossvalidation is: \",\n",
    "      pipeline1_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, pipeline2 is used to predict the classes of the breast cancer problem. Then, the accuracy of the predictions is computed. Notice the difference in the accuracies given by the two pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2_prediction = cross_val_predict(three_steps_pipeline,breast_cancer_data.data, \n",
    "                               breast_cancer_data.target,cv=5)\n",
    "\n",
    "pipeline2_accuracy = metrics.accuracy_score(breast_cancer_data.target, \n",
    "                                                     pipeline2_prediction) \n",
    "\n",
    "print(\"The accuracy of the second pipeline computed using crossvalidation is: \",\n",
    "      pipeline2_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT: Optimizing Pipelines\n",
    "\n",
    "Finding the best combination of pre-processing and classifiers is a difficult task. Automatic Machine Learning is about of generating the optimal ML pipelines automaticly. \n",
    "\n",
    "Now, lets use TPOT, which is a bi-objective genetic programmig tool that generates pipelines automatically, by searching for the maximum accuracy, while also attempting to keep the pipelines simple.\n",
    "\n",
    "We will  use the TPOT package to search for (almost) optimal pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpot is not part sklearn, we import it\n",
    "\n",
    "from tpot import TPOTClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tpot instance is define simmilarly to the way it is done for a regular sklearn classifier. In the example below, the evolutive algorithm will evaluate 50 pipelines (5 generations of a population of size 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=10, verbosity=2, random_state=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a good pipeline is learned using the \"fit\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.fit(features=breast_cancer_data[\"data\"], target=breast_cancer_data[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can see what the result is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.fitted_pipeline_.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Create a pipeline  that uses one scaler, one feature  selection method that produces 10 features, and a support vector machine classifier.\n",
    "\n",
    "\n",
    "4.1) Compute the accuracy, precision, and recall of your pipeline. \n",
    "\n",
    "\n",
    "Suggestion: If needed, check sklearn web page help for feature extraction methods and support vector machine classifier definition. http://scikit-learn.org/0.18/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjecting our data to a TPOT execution, with the provided configurations, suggests that the previous pipeline is the best way to build a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forwawrd to a real classification problem,\n",
    "\n",
    "5.1) Fetch a real database (different from the one used in the example) from the sklearn library (with classification purposes), understand how it is structured, and get used to it.\n",
    "\n",
    "5.2) Define and fit a classifier using the data.\n",
    "\n",
    "5.3) Use cross-validation to estimate the accuracy, recall, and precision of the classifier (if binary) or weighted accuracy (if multiclass). *Hint: run SCORERS.keys()*\n",
    "\n",
    "5.4) Use a pre-processing method to transform the data before feeding it to the classifier\n",
    "\n",
    "5.5) Create a Pipeline which includes (at least) one preprocessing method, and a classifier.\n",
    "\n",
    "5.6) Apply the pipeline to the data.\n",
    "\n",
    "5.7) Use Tpot to automatically generate a pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available datasets:\n",
    "[name for name in data_load.__all__ if \"load\" in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a reference value of the quality of the model, try to tune the parameters of the algorithm so that the model quality improves. Also, add a preprocessor, or change the classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional exercises\n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "Using the implementation completed for the MyLogisticRegression, implement a LDA classifier. Test its results by comparing it to MyLogisticRegression via accuracy (with 5-fold cross-validation) and confusions matrix. Additionally, you can generate figures with the plot_points function\n",
    "\n",
    "Note that the LDA method for predicting the probability of an observation belonging to a class is:\n",
    "\n",
    "$$p(y=1|x,\\theta)=sigm(w^T(x-x_0))$$\n",
    "\n",
    "where $y$ is the class, $x$ is our data, $\\theta$ are the parameters of the model ($w$ and $x_0$), and $sigm$ is the sigmoid function.\n",
    "\n",
    "One simple method for computing the parameters of the model is:\n",
    "\n",
    "$$w=\\Sigma^{-1}(\\mu_1-\\mu_0)$$\n",
    "$$x_0=\\frac{1}{2}(\\mu_1+\\mu_0)-(\\mu_1+\\mu_0)\\frac{log(\\frac{\\pi_1}{\\pi_0})}{(\\mu_1-\\mu_0)^T\\Sigma^{-1}(\\mu_1-\\mu_0)}$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix of the variables, $\\mu_c$ is the mean of the variable values for class $c$, and $\\pi_c$ is the $c$ class probability probability.\n",
    "\n",
    "Tip: Assuming that the problem the classifier is going to deal with has the same number of observations for both classes, the formula for computing the $x_0$ parameter can be greatly simplified.\n",
    "\n",
    "Tip2: To compute the inverse of a matrix (in this case $\\Sigma^{-1}$), you can use the np.linalg.inv function. For computing the covariance matrix, np.cov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLDAClassifier(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "    def fit(self, X, y):        \n",
    "    \n",
    "    def net_input(self, X):\n",
    "    \n",
    "    def activation(self, z):\n",
    "    \n",
    "    def predict(self, X):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "Below we want to learn a k-nearest-neighbor classifier from the train_data and use it \n",
    "to predict the classes of the test data. The problem has two classes. The positive class\n",
    "is formed by vectors with 4 or more zeros. The negative class is formed by vectors of 3 or less zeros.\n",
    "\n",
    "Below, replace the XXX in LINE 1, LINE 2, and LINE 3 to get the predictions for the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our training dataset has 10 instances\n",
    "# Each instance has 5 variables\n",
    "train_data = np.array([[1,1,1,1,1],\n",
    "                         [1,1,1,0,1],\n",
    "                         [1,1,0,1,1],\n",
    "                         [1,1,1,0,1],\n",
    "                         [1,0,1,1,1],\n",
    "                         [0,0,0,0,0],\n",
    "                         [0,0,0,1,0],\n",
    "                         [0,0,0,0,1],\n",
    "                         [1,0,0,0,0],\n",
    "                         [0,1,0,0,0]])\n",
    "\n",
    "# We have the classes for the ten instances\n",
    "# The first five instances belong to class 1\n",
    "# the rest to class 2\n",
    "train_classes = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "test_data = np.random.randint(low=0,high=2,size=(10,5)) \n",
    "print(\"test_data\", XXX)              \n",
    "                        \n",
    "\n",
    "knn = KNeighborsClassifier()      # LINE 1                                      \n",
    "knn.fit(XXX,XXX)                  # LINE 2\n",
    "\n",
    "prediction_test = knn.predict(XXX) # LINE 3\n",
    "\n",
    "print(\"predictions for test data\", prediction_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

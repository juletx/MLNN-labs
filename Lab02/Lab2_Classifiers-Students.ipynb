{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuNOHyDmtO3c"
   },
   "source": [
    "# Lab 1. Introduction to Machine learning\n",
    "\n",
    "In this Lab we will firstly see a simple example of a classification problem.\n",
    "\n",
    "Afterwards, we will focus on three different questions related to ML:\n",
    "\n",
    "\n",
    "1) Learn how to use a sklearn to solve ML tasks (classification and regression)\n",
    "\n",
    "2) Illustrate important concepts in ML (imputation, validation, standarization, etc.) \n",
    "\n",
    "3) Learn how to create sophisticated ML pipelines for real-world problems\n",
    "\n",
    "\n",
    "We will use material from the books:\n",
    "\n",
    "- \"Hands-On Machine Learning with Scikit-Learn and TensorFlow. Concepts, Tools, and Techniques to Build Intelligent Systems\" by Aurélien Géron. http://shop.oreilly.com/product/0636920052289.do \n",
    "\n",
    "- \"Deep Learning with Python\" by F. Chollet. https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438\n",
    "\n",
    "which are recommended as Bibliography of the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2608,
     "status": "ok",
     "timestamp": 1602258720188,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "LcugwSBNtO3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juletx/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# We start by importing the python libraries required to solve the problems\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Enables interaction with the plots\n",
    "%matplotlib notebook\n",
    "\n",
    "# These are modules contained in sklearn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, SCORERS\n",
    "from sklearn.utils.testing import all_estimators\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "# Imputation methods\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Methods for preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "# Feature selection Methods \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Methods for classifier validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Datasets in sklearn\n",
    "import sklearn.datasets as data_load\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Enables interactivity with the plots\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5846,
     "status": "ok",
     "timestamp": 1602258723444,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "aqGsTwDWtO3o"
   },
   "outputs": [],
   "source": [
    "def plot_points(points_list, labels=None, line=None):\n",
    "    plt.figure()\n",
    "    if labels is None:\n",
    "        labels = [\"\"] * len(points_list)\n",
    "    for points, label in zip(points_list, labels):\n",
    "        plt.scatter(points[0], points[1], label=label)\n",
    "    plt.xlabel(r'$x$', fontsize=fsize)\n",
    "    plt.ylabel(r'$y$', fontsize=fsize)\n",
    "    if line is not None:\n",
    "        plt.plot(line[0], line[1],'m',lw=4)\n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5833,
     "status": "ok",
     "timestamp": 1602258723446,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "_I-L-KDztO3s"
   },
   "outputs": [],
   "source": [
    "# We create two sets of points (XA,yA)  and (XB,yB) corresponding to two different classes\n",
    "\n",
    "number_points_Class_A = 50\n",
    "number_points_Class_B = 50\n",
    "\n",
    "# Font size\n",
    "fsize = 20     \n",
    "\n",
    "# Points in Class A\n",
    "xA = 20*np.random.rand(number_points_Class_A)\n",
    "shiftA = 20*np.random.rand(number_points_Class_A)\n",
    "yA = (4+xA)/2.0 - shiftA - 0.1\n",
    "\n",
    "# Points in Class B\n",
    "xB = 20*np.random.rand(number_points_Class_B)\n",
    "shiftB = 20*np.random.rand(number_points_Class_B)\n",
    "yB = (4+xB)/2.0 + shiftB + 0.1\n",
    "\n",
    "# Hyperplane dividing the two classes\n",
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = (4+x1)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5813,
     "status": "ok",
     "timestamp": 1602258723448,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "W4lTXisKtO3x",
    "outputId": "520e527c-575d-4388-b039-49804e5b6d57"
   },
   "outputs": [],
   "source": [
    "# The points in the two classes are visualized with different colors \n",
    "# Points in Class I in blue. Points in Class II in orange\n",
    "plot_points([[xA, yA], [xB, yB]], labels=[\"Class I\", \"Class II\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5784,
     "status": "ok",
     "timestamp": 1602258723449,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "GPAnzNRgtO36",
    "outputId": "e8720d40-cd9a-49a3-c1ca-717709b9f128"
   },
   "outputs": [],
   "source": [
    "# We show that there exists a hyperplane that allows to perfectly divide \n",
    "# points in the two classes.\n",
    "# In 2-d this hyperplane corresponds to a line that is represented in green.\n",
    "\n",
    "plot_points([[xA, yA], [xB, yB]], labels=[\"Class I\", \"Class II\"], line=[x1, y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5763,
     "status": "ok",
     "timestamp": 1602258723450,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "-GNM3bR4tO4A"
   },
   "outputs": [],
   "source": [
    "# Now we create a more difficult classification problem\n",
    "# where the two region classes overlap.\n",
    "\n",
    "# Font size\n",
    "fsize = 20     \n",
    "\n",
    "# Points in Class A\n",
    "xA1 = 20*np.random.rand(number_points_Class_A)\n",
    "shiftA1 = 20*np.random.rand(number_points_Class_A)\n",
    "yA1 = (4+xA1)/2.0 - shiftA1 + 5.0\n",
    "\n",
    "# Points in Class B\n",
    "xB1 = 20*np.random.rand(number_points_Class_B)\n",
    "shiftB1 = 20*np.random.rand(number_points_Class_B)\n",
    "yB1 = (4+xB1)/2.0 + shiftB1 - 5.0\n",
    "\n",
    "# Hyperplane dividing the two classes\n",
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = (4+x1)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5738,
     "status": "ok",
     "timestamp": 1602258723452,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "560UgOattO4F",
    "outputId": "1426af7a-652e-4237-c89a-1eadaa20b9e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The points corresponding to the two classes are plotted\n",
    "\n",
    "plot_points([[xA1, yA1], [xB1, yB1]], labels=[\"Class I\", \"Class II\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5705,
     "status": "ok",
     "timestamp": 1602258723453,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "34MroET0tO4J",
    "outputId": "facf0424-809d-417a-950a-7c7454c24b6d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We use the same hyperplane to divide points from the two classes\n",
    "# However, as it can be appreciated, the classification provided by this hyperplane\n",
    "# is not perfect\n",
    "\n",
    "plot_points([[xA1, yA1], [xB1, yB1]], labels=[\"Class I\", \"Class II\"], line=[x1, y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5684,
     "status": "ok",
     "timestamp": 1602258723455,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "oVH2X2BLtO4N"
   },
   "outputs": [],
   "source": [
    "# Now we create a more difficult classification problem\n",
    "# where the two region classes seem to overlap and where\n",
    "# it is not evident that a linear separator exists.\n",
    "\n",
    "\n",
    "# Font size\n",
    "fsize = 20     \n",
    "\n",
    "# Points in Class A\n",
    "xA2 = 20*np.random.rand(number_points_Class_A)\n",
    "shiftA2 = 20*np.random.rand(number_points_Class_A)\n",
    "yA2 = 20*np.cos(0.2*np.pi*xA2) - shiftA2 \n",
    "\n",
    "# Points in Class B\n",
    "xB2 = 20*np.random.rand(number_points_Class_B)\n",
    "shiftB2 = 20*np.random.rand(number_points_Class_B)\n",
    "yB2 = 20*np.cos(0.2*np.pi*xB2) + shiftB2 \n",
    "\n",
    "# Sinusoidal curve dividing the two classes      \n",
    "x2 = np.linspace(0, 20, 2000)\n",
    "y2 = 20*np.cos(0.2*np.pi*x2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5669,
     "status": "ok",
     "timestamp": 1602258723457,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "MXrAY9mAtO4Q",
    "outputId": "9d5698a5-448e-4ea5-a36c-2fdc6a78dfff"
   },
   "outputs": [],
   "source": [
    "# The points corresponding to the two classes are plotted\n",
    "\n",
    "plot_points([[xA2, yA2], [xB2, yB2]], labels=[\"Class I\", \"Class II\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1602258723460,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "tZorM-oTtO4U",
    "outputId": "e2c815eb-ef02-4220-c6c8-01d0231e8776"
   },
   "outputs": [],
   "source": [
    "# Now we plot in green the same line that was previously used to classify the previous data\n",
    "# It can be seen that it does not provide a good separation of the data\n",
    "\n",
    "plot_points([[xA2, yA2], [xB2, yB2]], labels=[\"Class I\", \"Class II\"], line=[x1, y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5620,
     "status": "ok",
     "timestamp": 1602258723462,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "bTu89IoRtO4Y",
    "outputId": "60d67e97-597b-472e-8009-3e825664bcef"
   },
   "outputs": [],
   "source": [
    "# Finally we also show a curve that makes a perfect separation of the data\n",
    "\n",
    "\n",
    "plot_points([[xA2, yA2], [xB2, yB2]], labels=[\"Class I\", \"Class II\"], line=[x2, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2GU50_1tO4c"
   },
   "source": [
    "### A very simple binary classification problem \n",
    "For our first classification problem, we will take the initial, easy problem back. As previously, we can divide create a perfect classifier by using a hyperplane (a line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5594,
     "status": "ok",
     "timestamp": 1602258723464,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "-NgYMvO6tO4d",
    "outputId": "9272d814-48d6-4f06-ad14-eaaf279d9f4a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In 2-d this hyperplane corresponds to a line that is represented in green.\n",
    "\n",
    "plot_points([[xA, yA], [xB, yB]], labels=[\"Class I\", \"Class II\"], line=[x1, y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8E5oGEztO4g"
   },
   "source": [
    "## Learning simple classifiers using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Joz9Ch8BtO4h"
   },
   "source": [
    "### The common syntax for learning a classifier in sklearn implies three steps: \n",
    "\n",
    "- Declaration of the classifier\n",
    "- Fitting the classifier from the data and the labels\n",
    "- Using the classifier to predict new data\n",
    "\n",
    "In the example below we learn a logistic regression classifier that separates the two classes presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BM3fIcIhtO4h"
   },
   "source": [
    "First, we will create the training data that is needed since we are working with a supervised classification algorithm. By uncommenting the \"print\" line you can check the size of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5577,
     "status": "ok",
     "timestamp": 1602258723466,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "ekCfzw-6tO4i"
   },
   "outputs": [],
   "source": [
    "# We define the labels for our two classes A and B. Variable c will keep the \n",
    "# labels of all points.\n",
    "# The points in Class A will have label 1 assigned and the points in Class B, \n",
    "# will have label 0.\n",
    "\n",
    "c = np.hstack((np.ones((number_points_Class_A)),np.zeros((number_points_Class_B))))\n",
    "print(c.shape)\n",
    "\n",
    "# We create the training data concatenating examples from the two classes XA and XB\n",
    "tr_data = np.hstack((np.vstack((xA,yA)),np.vstack((xB,yB)))).transpose()\n",
    "print(tr_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJEcUNxitO4q"
   },
   "source": [
    "We declare the classifier (it is a logistic regression classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5557,
     "status": "ok",
     "timestamp": 1602258723470,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "Mnl3iG-ftO4r"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "executionInfo": {
     "elapsed": 5543,
     "status": "ok",
     "timestamp": 1602258723471,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "QzxkBRGVtO4v",
    "outputId": "d23d639a-b4d2-48ad-d7b2-6c64a7068407"
   },
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScoXoC8UtO41"
   },
   "source": [
    "We learn the classifier. In sklearn this is done using the function fit(data,classes)\n",
    "It will update the \"lr\" model using the data from training_data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "executionInfo": {
     "elapsed": 5525,
     "status": "ok",
     "timestamp": 1602258723472,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "5zNfSQxJtO42",
    "outputId": "ef3b46a5-fdc8-4c90-c244-e0510eb17e81",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.fit(tr_data,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDoXQ3g2tO46"
   },
   "source": [
    "Finally, we use the logistic regression classifier lr to predict the classes of the dataset. In the example below we predict the classes of the same dataset used for training (training_data) but this is not a realistic case. Usually we predict the classes of data that was not used for training (called the test data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5514,
     "status": "ok",
     "timestamp": 1602258723474,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "PI-jky5XtO46"
   },
   "outputs": [],
   "source": [
    "# Predictions of the classifier on the training data\n",
    "prediction_training = lr.predict(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "executionInfo": {
     "elapsed": 5497,
     "status": "ok",
     "timestamp": 1602258723475,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "0aouJW5AtO49",
    "outputId": "ea5a395a-c820-43dc-c21b-3b29a6feba77"
   },
   "outputs": [],
   "source": [
    "prediction_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMz3bIowtO5B"
   },
   "source": [
    "###  Visualization of the predictions given by the classifier\n",
    "\n",
    "A classifier is accurate if for most of the points the class predicted coincide with the real classes. In this sense, for a binary problem, each prediction can be of four types:\n",
    "\n",
    "1) true positive: A positive point is given positive prediction\n",
    "\n",
    "2) false positive:  A negative point is given positive prediction\n",
    "\n",
    "3) true negative: A negative point is given negative prediction\n",
    "\n",
    "4) false negative:  A positive point is given  negative prediction\n",
    "\n",
    "Below, we put each classifier prediction in one of the four groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 5482,
     "status": "ok",
     "timestamp": 1602258723477,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "Xp821QXrtO5B",
    "outputId": "102d8577-0cf3-4eab-fc30-281988a91d79"
   },
   "outputs": [],
   "source": [
    "# Font size\n",
    "fsize = 20   \n",
    "\n",
    "# We identify the points that were correctly and wrongly classified\n",
    "tp = np.where(prediction_training[:number_points_Class_A]==1)[0]  # True positive\n",
    "fn = np.where(prediction_training[:number_points_Class_A]==0)[0]  # False negative\n",
    "tn = number_points_Class_A+np.where(prediction_training[number_points_Class_A:]==0)[0]  # True negative\n",
    "fp = number_points_Class_A+np.where(prediction_training[number_points_Class_A:]==1)[0]  # False positive\n",
    "\n",
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = -lr.coef_[0][0]/lr.coef_[0][1]*x1 + lr.intercept_\n",
    "\n",
    "\n",
    "plot_points([[tr_data[tp,0], tr_data[tp,1]], [tr_data[fn,0], tr_data[fn,1]], [tr_data[tn,0], tr_data[tn,1]], [tr_data[fp,0], tr_data[fp,1]]], \n",
    "            labels=['True positive', 'False negative', 'True negative', 'False positive'], line=[x1, y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 5467,
     "status": "ok",
     "timestamp": 1602258723478,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "C1kKoJK2tO5H",
    "outputId": "5cfdc724-69c9-4b4c-b411-3470eef3c514"
   },
   "outputs": [],
   "source": [
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bnsGSnMtO5K"
   },
   "source": [
    "## Implementing our own classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3mQFVKntO5K"
   },
   "source": [
    "Now that we have seen how to use a Logistic Regression classifier, we are going to implement a basic classifier ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQEmw5SttO5L"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "The following cells contain an incomplete implementation of a logistic regression classifier. Based on the contents learned in class, complete it, and test whether it can perform as accurately as the one from sklearn. Use the accuracy generated from employing a 5-fold cross-validation scheme.\n",
    "\n",
    "(This implementation is an adaptation of that found in the *Python machine learning* book, Raschka, S., & Mirjalili, V. (2017). Packt Publishing Ltd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5455,
     "status": "ok",
     "timestamp": 1602258723479,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "gcmvLZCftO5M"
   },
   "outputs": [],
   "source": [
    "class MyLogisticRegression(object):\n",
    "    def __init__(self, eta=0.01, n_iter=1000, random_state=0):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def activation(self, z):\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "        \n",
    "    def fit(self, X, y):        \n",
    "        self.w = self.rgen.normal(loc=0, scale=0.01, size=X.shape[1])\n",
    "        self.b = self.rgen.normal(loc=0, scale=0.01, size=1)\n",
    "        self.cost = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            # The error is computed as the difference between the \n",
    "            # prob. of the class and the prediction of the model\n",
    "            errors = self.predict_proba(X) - self.predict(X)\n",
    "            self.w += self.eta * X.T.dot(errors)\n",
    "            self.b = self.eta * errors.sum()\n",
    "            \n",
    "            cost = (-y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))))\n",
    "            self.cost.append(cost)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    # https://medium.com/analytics-vidhya/coding-logistic-regression-in-python-from-scratch-57284dcbfbff\n",
    "    # Given the features \"predict\" outputs the classification given by the model\n",
    "    def predict(self, X):\n",
    "        m = X.shape[1]\n",
    "        Y_prediction = np.zeros((1,m))\n",
    "        H = self.predict_proba(X)\n",
    " \n",
    "        for i in range(H.shape[1]):\n",
    "        # Convert probabilities H[0,i] to actual predictions p[0,i]\n",
    "            if H[0,i] >= 0.5:\n",
    "                Y_prediction[0,i] = 1\n",
    "            else: \n",
    "                Y_prediction[0,i] = 0\n",
    "   \n",
    "        return Y_prediction\n",
    "    \n",
    "    # Sigmoid function\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "\n",
    "    # Hypothesis function\n",
    "    def hypothesis(self, w, X, b):\n",
    "        return self.sigmoid(np.dot(w.T,X)+b) \n",
    "    \n",
    "    # Given the features predict_proba outputs the probability that the solution belongs to the class\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        w = self.w.reshape(X.shape[0], 1)\n",
    "        b = self.b\n",
    "        # Compute vector \"H\" \n",
    "        H = self.hypothesis(w, X, b)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loXJqhvStO5R"
   },
   "source": [
    "The probability of an observation belonging to a certain class, as computed by Logistic Regression, can be defined as:\n",
    "\n",
    "$$P(y|x)=sigm(wx+b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5438,
     "status": "ok",
     "timestamp": 1602258723480,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "vsRstjjutO5S"
   },
   "outputs": [],
   "source": [
    "mylr = MyLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 5471,
     "status": "error",
     "timestamp": 1602258723535,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "B9yYipzOtO5X",
    "outputId": "2483a6ad-7663-4dc8-bd4f-fe7868092f4c"
   },
   "outputs": [],
   "source": [
    "mylr.fit(tr_data,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5399,
     "status": "aborted",
     "timestamp": 1602258723481,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "EmjIp-j5tO5b"
   },
   "outputs": [],
   "source": [
    "pred = mylr.predict(tr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2BolyaMtO5l"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Using the plot_points function, create a plot in which the the true and false positives and negatives are shown, along with the line computed by the MyLogisticRegression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5364,
     "status": "aborted",
     "timestamp": 1602258723484,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "x61uBTaytO5l"
   },
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = -lr.coef_[0][0]/lr.coef_[0][1]*x1 + lr.intercept_\n",
    "y1 = mylr.w[0]*x1 + mylr.b\n",
    "\n",
    "# We identify the points that were correctly and wrongly classified\n",
    "tp = np.where(pred[:number_points_Class_A]==1)[0]  # True positive\n",
    "fn = np.where(pred[:number_points_Class_A]==0)[0]  # False negative\n",
    "tn = number_points_Class_A+np.where(pred[number_points_Class_A:]==0)[0]  # True negative\n",
    "fp = number_points_Class_A+np.where(pred[number_points_Class_A:]==1)[0]  # False positive\n",
    "\n",
    "plot_points([[tr_data[tp,0], tr_data[tp,1]], [tr_data[fn,0], tr_data[fn,1]], [tr_data[tn,0], tr_data[tn,1]], [tr_data[fp,0], tr_data[fp,1]]], \n",
    "            labels=['True positive', 'False negative', 'True negative', 'False positive'], line=[x1, y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5344,
     "status": "aborted",
     "timestamp": 1602258723485,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "km5lleJ8tO5r"
   },
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmaT_3sptO5u"
   },
   "source": [
    "## A real-word classification problem and more sophisticated validation schemes\n",
    "\n",
    "\n",
    "sklearn also contains a number of databases that can be used to test the algorithms. We will use some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCB8BamstO5v"
   },
   "source": [
    "### We can check which are the datasets included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1602258889607,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "6PZ524fetO5w",
    "outputId": "336fec62-b61b-4fad-a82a-83385b7d3f86"
   },
   "outputs": [],
   "source": [
    "print(\"Available datasets:\")\n",
    "[name for name in data_load.__all__ if \"load\" in name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ZJj_NLA3tO5z"
   },
   "source": [
    "### Inspecting the Real-World datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAFCwrHJtO50"
   },
   "source": [
    "We will use the breast cancer dataset, included in UCI ML Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "\n",
    "\n",
    "It has been used for the application of ML to Cancer diagnosis and prognosis: http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "executionInfo": {
     "elapsed": 1812,
     "status": "error",
     "timestamp": 1602258890193,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "9gRC-KQGtO50",
    "outputId": "79636e39-4219-4ec1-f621-009c861e863e"
   },
   "outputs": [],
   "source": [
    "# The dataset is loaded\n",
    "breast_cancer_data = data_load.load_breast_cancer()\n",
    "\n",
    "#Display options\n",
    "# These options determine the way floating point numbers, \n",
    "# arrays and other NumPy objects are displayed\n",
    "import sys\n",
    "np.set_printoptions(suppress=True, threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvtpji3ptO53"
   },
   "source": [
    "It is a good practice to inspect the dataset before applying any ML technique, its header and also the characteristics of the data. \n",
    "\n",
    "For example, it is very important to know the number of attributes (variables), their type, and also the size of the data (number of instances). In the description that is shown below you will find this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1767,
     "status": "aborted",
     "timestamp": 1602258890159,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "-7wSDlYItO54"
   },
   "outputs": [],
   "source": [
    "#Some information about the dataset, understand what we are aiming for\n",
    "print(breast_cancer_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSDzSLYXtO59"
   },
   "source": [
    "We analyze more details of the database. Rows define observations (instances of our classification problem). Columns represent variables captured in each observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1759,
     "status": "aborted",
     "timestamp": 1602258890162,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "WyQ60rcetO5_"
   },
   "outputs": [],
   "source": [
    "breast_cancer_data[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGrAMkz7tO6C"
   },
   "source": [
    "Notice in the rows shown above that the range of values change among the columns. Some columns seem to have values between 0 and 1 and others much higher values. This has to be taken into account for the application of the classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLOY7XqptO6D"
   },
   "source": [
    "Since this a binary problem, classes are either 0: the tumor is malign or 1: the tumor is benign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1750,
     "status": "aborted",
     "timestamp": 1602258890163,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "JXwuq54ttO6E"
   },
   "outputs": [],
   "source": [
    "#Classes in the database\n",
    "breast_cancer_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXVIZ9IwtO6G"
   },
   "source": [
    "In the previous analysis, notice that we have used:   data[\"data\"] to visualize the features and data[\"target\"] to see the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rouN9ElxtO6H"
   },
   "source": [
    "## Imputing missing data\n",
    "\n",
    "Imputation methods serve to substitute missing values in the data. In the example below we define a small dataset of three variables, six instances and three missing values (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1741,
     "status": "aborted",
     "timestamp": 1602258890165,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "iRC-AapLtO6H"
   },
   "outputs": [],
   "source": [
    "my_data = np.array([[ 'NaN',   7,     6],\n",
    "                    [  5   ,  89,    13],\n",
    "                    [ 23   ,  12,   213],\n",
    "                    [  2   ,  87, 'NaN'],\n",
    "                    [  8   , 101,    71],\n",
    "                    [ 13   ,'NaN',    20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdBpfo84tO6K"
   },
   "source": [
    "The \"Imputer\" class allows to impute the missing data. It implements three strategies: \"mean\", \"median\", and \"most_frequent\".\n",
    "If works like the classifiers:\n",
    "\n",
    "1) Define the imputer\n",
    "\n",
    "2) Fit the imputer to the data using the function \"fit\"\n",
    "\n",
    "3) Impute the data using the function \"transform\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1728,
     "status": "aborted",
     "timestamp": 1602258890166,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "0TsPf3LjtO6K"
   },
   "outputs": [],
   "source": [
    "# Define the imputer\n",
    "import math\n",
    "mean_imputer = SimpleImputer(missing_values=math.nan,strategy=\"mean\")\n",
    "\n",
    "#  Fit the imputer\n",
    "mean_imputer.fit(my_data)\n",
    "\n",
    "# Transform (impute) the data\n",
    "imputed_data = mean_imputer.transform(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1714,
     "status": "aborted",
     "timestamp": 1602258890167,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "e9IXZKFvtO6O"
   },
   "outputs": [],
   "source": [
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WERMTfaQtO6T"
   },
   "source": [
    "## Preprocessing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2yH3S3ktO6U"
   },
   "source": [
    "Scaling the data can improve the accuracy of some classifiers. This can be done in a similar way in which we define the classifiers:\n",
    "\n",
    "1) Define the scaler\n",
    "\n",
    "2) Fit the scaler to the data using the function \"fit\"\n",
    "\n",
    "3) Scale the data using the function \"transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1705,
     "status": "aborted",
     "timestamp": 1602258890168,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "MmZg6kn4tO6V"
   },
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the data to scaler\n",
    "scaler.fit(X=breast_cancer_data[\"data\"])\n",
    "\n",
    "# Scale the data \n",
    "scaled_data = scaler.transform(X=breast_cancer_data[\"data\"])\n",
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LyFxiU-tO6b"
   },
   "source": [
    "The sample process can be done in only two steps using the functions fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1698,
     "status": "aborted",
     "timestamp": 1602258890169,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "gChee4-BtO6c"
   },
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit and scale the data \n",
    "scaled_data = scaler.fit_transform(X=breast_cancer_data[\"data\"])\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaZoN6PUtO6h"
   },
   "source": [
    "Another useful pre-processing algorithm is to binarize the data. It is NOT defined as classifiers, imputer, and scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1690,
     "status": "aborted",
     "timestamp": 1602258890170,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "RbAAyM5ktO6i"
   },
   "outputs": [],
   "source": [
    "binarized_data = binarize(scaled_data)\n",
    "print(binarized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxVfibl3tO6m"
   },
   "source": [
    "## Feature selection \n",
    "\n",
    "Feature selection is an important step in traditional classifiers. It allows to reduce the dimensionality of the data. This can be done in a similar way in which we define the classifiers:\n",
    "\n",
    "1) Define the feature selection method\n",
    "\n",
    "2) Fit the feature-selection method to the data using the function \"fit\"\n",
    "\n",
    "3) Select  the data using the function \"transform\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_jdErOHtO6n"
   },
   "source": [
    "In the example below a an information metric approach is used to select the two most informative features for the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1677,
     "status": "aborted",
     "timestamp": 1602258890171,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "3u2zm12TtO6n"
   },
   "outputs": [],
   "source": [
    "# The feature selection method is defined\n",
    "feature_selection = SelectKBest(f_classif,k=2)\n",
    "\n",
    "# Using fit_transform we select fit the selector to the data and finally select features\n",
    "new_features = feature_selection.fit_transform(scaled_data, breast_cancer_data[\"target\"])\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQoK4N1LtO6r"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "We want to normalize the data using the \"preprocessing.MinMaxScaler()\" function that sets each variable to the range (0,1) and then selecting the two principal components of the data to visualize how they are related with the two classes of the Breast Cancer problem.\n",
    "\n",
    "Below, replace the XXX in LINE 1--LINE 5. Then execute the following cell to visualize the select features.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1668,
     "status": "aborted",
     "timestamp": 1602258890172,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "cscxEV-wtO6t"
   },
   "outputs": [],
   "source": [
    "# The scaler is defined\n",
    "min_max_scaler = preprocessing.MinMaxScaler()     #LINE1\n",
    "    \n",
    "# The scaler is fitted\n",
    "min_max_scaler.fit(breast_cancer_data[\"data\"])    #LINE2\n",
    "    \n",
    "# The scaler is applied\n",
    "scaled_to_min_max = min_max_scaler.transform(X=breast_cancer_data[\"data\"]) #LINE3\n",
    "print(scaled_to_min_max)\n",
    "\n",
    "# The pca is defined\n",
    "pca = PCA(n_components=2)                         #LINE4\n",
    "\n",
    "# pca is fitted to the data\n",
    "pca.fit(scaled_to_min_max)\n",
    "\n",
    "# the 2 main principal components are extracted\n",
    "reduced_data = pca.transform(scaled_to_min_max)                       #LINE5\n",
    "print(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX7ljp5rtO6w"
   },
   "source": [
    "Execute the following cell to visualize the two created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1659,
     "status": "aborted",
     "timestamp": 1602258890173,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "ZH3eG6dktO6x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Font size\n",
    "fsize = 10     \n",
    "\n",
    "pos_positive = np.where(breast_cancer_data[\"target\"]==1)\n",
    "pos_negative = np.where(breast_cancer_data[\"target\"]==0)\n",
    "\n",
    "plt.figure(figsize=(7.5,3.75))\n",
    "plt.plot(reduced_data[pos_positive,0], reduced_data[pos_positive,1], 'ro')\n",
    "plt.plot(reduced_data[pos_negative,0], reduced_data[pos_negative,1], 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class A')\n",
    "red_patch = mpatches.Patch(color='red', label='Class B')\n",
    "plt.legend(handles=[blue_patch,red_patch])\n",
    "\n",
    "plt.xlabel(r'$feature_1$', fontsize=fsize)\n",
    "plt.ylabel(r'$feature_2$', fontsize=fsize)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1RPWrvBtO62"
   },
   "source": [
    "## Learning and validating classifiers \n",
    "\n",
    "One common way of evaluating the quality of a classifier is applying cross-validation.\n",
    "We remind that cross-validation is a way to estimate the accuracy of a classifier\n",
    "using the training data.\n",
    "It works as follows:\n",
    "\n",
    "1) First, the training data is divided into k folds.\n",
    "\n",
    "2) Then, we repeat k times the following step: We use (k-1) folds to train a classifier\n",
    "and predict the classes of intances in the other k-th fold using tha classifier. \n",
    "\n",
    "The procedure is done predicting every time a different fold. \n",
    "\n",
    "Below, we show how cross-validation is implemented in sk-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb6qQkLEtO62"
   },
   "source": [
    "We define a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1649,
     "status": "aborted",
     "timestamp": 1602258890175,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "wQEfFufOtO63"
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPMl9tsYtO65"
   },
   "source": [
    "We estimate the classifier accuracy using k-fold cross-validation with k=5. The result of cross-validation will be the predictions for all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1639,
     "status": "aborted",
     "timestamp": 1602258890176,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "dkMZXGX2tO65"
   },
   "outputs": [],
   "source": [
    "prediction = cross_val_predict(dt,breast_cancer_data.data, \n",
    "                               breast_cancer_data.target,cv=5)\n",
    "\n",
    "# Let us print the predictions\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJe7Iy7ItO69"
   },
   "source": [
    "With the prediction  and the target (true class value) we can compute different accuracy measures for the classifier.  We do this for the accuracy metric below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1626,
     "status": "aborted",
     "timestamp": 1602258890176,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "hU_itRnFtO7A"
   },
   "outputs": [],
   "source": [
    "dt_accuracy = metrics.accuracy_score(breast_cancer_data.target, prediction) \n",
    "print(\"The accuracy of the logistic regression classifier, as computed \\\n",
    "using 5-fold crossvalidation, is: \",dt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "A6rUqPj_tO7F"
   },
   "source": [
    "We can also compute the confusion matrix for the predictions made by the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1616,
     "status": "aborted",
     "timestamp": 1602258890177,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "tUhSUMsZtO7G"
   },
   "outputs": [],
   "source": [
    "dt_confusion_matrix = metrics.confusion_matrix(breast_cancer_data.target, prediction)\n",
    "print(\"Confusion matrix for the predictions made by the logistic regression classifier:\")\n",
    "print(dt_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwiSP-jdtO7K"
   },
   "source": [
    "### Exercise 4\n",
    " \n",
    "  Learn a classifier for the breast cancer dataset and estimate its accuracy. The classifier should have the following characteristics:\n",
    "   \n",
    "  1) Use a logistic regression classifier\n",
    "  \n",
    "  2) Do the predictions using cross-validation (number of folds k=10) \n",
    "  on the scaled (StandardScaler) data\n",
    "  \n",
    "  3) Compute the confusion matrix \n",
    "  \n",
    " \n",
    " Suggestions: \n",
    " \n",
    " - Copy-paste the previous cells and substitute decision tree by logistic regression\n",
    " - Use scaled data instead of original data\n",
    " - Modify the parameters of the cross-validation to have the number of folds requested \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1603,
     "status": "aborted",
     "timestamp": 1602258890178,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "_qROEgKwtO7L"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "# Define the scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit and scale the data \n",
    "scaled_data = scaler.fit_transform(X=breast_cancer_data[\"data\"])\n",
    "\n",
    "prediction = cross_val_predict(lr,scaled_data, \n",
    "                               breast_cancer_data.target,cv=10)\n",
    "\n",
    "lr_confusion_matrix = metrics.confusion_matrix(breast_cancer_data.target, prediction)\n",
    "print(\"Confusion matrix for the predictions made by the logistic regression classifier:\")\n",
    "print(lr_confusion_matrix)\n",
    "\n",
    "dt_accuracy = metrics.accuracy_score(breast_cancer_data.target, prediction) \n",
    "print(\"The accuracy of the logistic regression classifier, as computed \\\n",
    "using 5-fold crossvalidation, is: \",dt_accuracy)\n",
    "\n",
    "# Check if data is balanced\n",
    "print(np.sum(breast_cancer_data.target==0))\n",
    "print(np.sum(breast_cancer_data.target==1))\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "# You can print the variables computed in each instruction for understanding better what we are doing\n",
    "def balanced_accuracy(true, prediction):\n",
    "    # We compute the number of classes (n), and the number of examples per class \n",
    "    classes, count = np.unique(true, return_counts=True)\n",
    "    # We compute the weights for each class. The summation of the weights of all examples belonging to a class must be 1/n\n",
    "    class_weights = [1/len(classes)/i for i in count]\n",
    "    # We assign weights for each example, depending on their class\n",
    "    example_weights = [class_weights[i] for i in true]\n",
    "    # We compute the accuracy weighting each example according to the representation of the class it belongs to in the data\n",
    "    return accuracy_score(true, prediction, sample_weight=example_weights)\n",
    "\n",
    "lr_accuracy = balanced_accuracy(breast_cancer_data.target, prediction) \n",
    "print(\"The balanced accuracy of the logistic regression classifier, as computed \\\n",
    "using 10-fold crossvalidation, is: \",lr_accuracy)\n",
    "\n",
    "lr_accuracy = metrics.balanced_accuracy_score(breast_cancer_data.target, prediction) \n",
    "print(\"The balanced accuracy of the logistic regression classifier, as computed \\\n",
    "using 10-fold crossvalidation, is: \",lr_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNm9Qb6wtO7Q"
   },
   "source": [
    "## Machine learning pipelines \n",
    "\n",
    "Machine learning pipeline allows to join in a single pipeline a number of processing steps. They sequentially apply a list of transformations and a final estimator (classifier, regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6ZO3uQqtO7Q"
   },
   "source": [
    "In the example below we define two pipelines with different components: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1593,
     "status": "aborted",
     "timestamp": 1602258890179,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "GPAA_loxtO7R"
   },
   "outputs": [],
   "source": [
    "# Components of the first pipeline\n",
    "knn = KNeighborsClassifier()    \n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# The first pipeline is defined\n",
    "knn_scale_pipeline = Pipeline([(\"scaler\", scaler), (\"k-NN\", knn)])\n",
    "\n",
    "# The second pipeline is defined using components from the previous exercise.\n",
    "# If you did not complete the exercise you can define the components here\n",
    "\n",
    "three_steps_pipeline = Pipeline([(\"mm_scaler\", min_max_scaler),\n",
    "                                 (\"pca\", pca ), (\"dt_classifier\", dt)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpX_RtyStO7T"
   },
   "source": [
    "In the next cell pipeline1 is used to predict the classes of the breast cancer problem. Then, the accuracy of the predictions is computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1584,
     "status": "aborted",
     "timestamp": 1602258890180,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "OaYstgn4tO7T"
   },
   "outputs": [],
   "source": [
    "pipeline1_prediction = cross_val_predict(knn_scale_pipeline,breast_cancer_data.data, \n",
    "                               breast_cancer_data.target,cv=5)\n",
    "\n",
    "pipeline1_accuracy = metrics.accuracy_score(breast_cancer_data.target, \n",
    "                                                     pipeline1_prediction) \n",
    "\n",
    "print(\"The accuracy of the secondpipeline computed using crossvalidation is: \",\n",
    "      pipeline1_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q5vWn_UtO7W"
   },
   "source": [
    "In the next cell, pipeline2 is used to predict the classes of the breast cancer problem. Then, the accuracy of the predictions is computed. Notice the difference in the accuracies given by the two pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1573,
     "status": "aborted",
     "timestamp": 1602258890181,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "u3Pe2ToNtO7W"
   },
   "outputs": [],
   "source": [
    "pipeline2_prediction = cross_val_predict(three_steps_pipeline,breast_cancer_data.data, \n",
    "                               breast_cancer_data.target,cv=5)\n",
    "\n",
    "pipeline2_accuracy = metrics.accuracy_score(breast_cancer_data.target, \n",
    "                                                     pipeline2_prediction) \n",
    "\n",
    "print(\"The accuracy of the second pipeline computed using crossvalidation is: \",\n",
    "      pipeline2_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY7AyaRLtO7Y"
   },
   "source": [
    "## TPOT: Optimizing Pipelines\n",
    "\n",
    "Finding the best combination of pre-processing and classifiers is a difficult task. Automatic Machine Learning is about of generating the optimal ML pipelines automaticly. \n",
    "\n",
    "Now, lets use TPOT, which is a bi-objective genetic programmig tool that generates pipelines automatically, by searching for the maximum accuracy, while also attempting to keep the pipelines simple.\n",
    "\n",
    "We will  use the TPOT package to search for (almost) optimal pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1561,
     "status": "aborted",
     "timestamp": 1602258890182,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "6hLZtf0ltO7Z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "# tpot is not part sklearn, we import it\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X33h5yghtO7c"
   },
   "source": [
    "The Tpot instance is define simmilarly to the way it is done for a regular sklearn classifier. In the example below, the evolutive algorithm will evaluate 50 pipelines (5 generations of a population of size 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1542,
     "status": "aborted",
     "timestamp": 1602258890183,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "-WgS8PeztO7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=10, verbosity=2, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5x9CcE2tO7g"
   },
   "source": [
    "Then, a good pipeline is learned using the \"fit\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289,
     "referenced_widgets": [
      "dceee056709041f4b2c651cf70b2f705",
      "eb71357051704335918278cd6ab98e32",
      "072896e1172a455dbcf64195a75e7a8b",
      "f674abb1f6f04712b4ffe79704334da4",
      "85a4459e63f445c99cc7c27051d35a10",
      "b61ea6ad5f6a4e598d28f86e3a8458fd",
      "f2597eb9e6854cfeb205f368bece1760",
      "0a91d819bbab447c983b7f2138b818f8"
     ]
    },
    "executionInfo": {
     "elapsed": 103833,
     "status": "ok",
     "timestamp": 1602259010774,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "VXf1GxdqtO7i",
    "outputId": "191196cf-4947-4848-9f0c-b576594cd112"
   },
   "outputs": [],
   "source": [
    "tpot.fit(features=breast_cancer_data[\"data\"], target=breast_cancer_data[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "symFtYk8tO7m"
   },
   "source": [
    "Now we can see what the result is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1515,
     "status": "aborted",
     "timestamp": 1602258890185,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "euApK-ZLtO7m"
   },
   "outputs": [],
   "source": [
    "tpot.fitted_pipeline_.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CuMA1G7tO7r"
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "Create a pipeline  that uses one scaler, one feature  selection method that produces 10 features, and a support vector machine classifier.\n",
    "\n",
    "\n",
    "4.1) Compute the accuracy, precision, and recall of your pipeline. \n",
    "\n",
    "\n",
    "Suggestion: If needed, check sklearn web page help for feature extraction methods and support vector machine classifier definition. http://scikit-learn.org/0.18/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weI4BqZqtO7r"
   },
   "source": [
    "Subjecting our data to a TPOT execution, with the provided configurations, suggests that the previous pipeline is the best way to build a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1491,
     "status": "aborted",
     "timestamp": 1602258890187,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "OJCPseaFtO7s"
   },
   "outputs": [],
   "source": [
    "# The scaler is defined\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# The feature selection method is defined\n",
    "feature_selection = SelectKBest(f_classif,k=10)\n",
    "# The classifier\n",
    "svc = svm.SVC()\n",
    "\n",
    "# The first pipeline is defined\n",
    "svc_scale_pipeline = Pipeline([(\"scaler\", scaler), (\"feature_selection\", feature_selection), (\"svc\", svc)])\n",
    "\n",
    "pipeline_prediction = cross_val_predict(svc_scale_pipeline,breast_cancer_data.data, \n",
    "                               breast_cancer_data.target,cv=5)\n",
    "\n",
    "pipeline_accuracy = metrics.accuracy_score(breast_cancer_data.target, pipeline_prediction) \n",
    "print(\"The accuracy of the pipeline computed using crossvalidation is: \", pipeline_accuracy)\n",
    "\n",
    "pipeline_precision = metrics.precision_score(breast_cancer_data.target, pipeline_prediction) \n",
    "print(\"The precision of the pipeline computed using crossvalidation is: \", pipeline_precision)\n",
    "\n",
    "pipeline_recall = metrics.recall_score(breast_cancer_data.target, pipeline_prediction) \n",
    "print(\"The recall of the pipeline computed using crossvalidation is: \", pipeline_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzvAlnoitO7v"
   },
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5nOeHi3tO7v"
   },
   "source": [
    "Moving forwawrd to a real classification problem,\n",
    "\n",
    "6.1) Fetch a real database (different from the one used in the example) from the sklearn library (with classification purposes), understand how it is structured, and get used to it.\n",
    "\n",
    "6.2) Define and fit a classifier using the data.\n",
    "\n",
    "6.3) Use cross-validation to estimate the accuracy, recall, and precision of the classifier (if binary) or weighted accuracy (if multiclass). *Hint: run SCORERS.keys()*\n",
    "\n",
    "6.4) Use a pre-processing method to transform the data before feeding it to the classifier\n",
    "\n",
    "6.5) Create a Pipeline which includes (at least) one preprocessing method, and a classifier.\n",
    "\n",
    "6.6) Apply the pipeline to the data.\n",
    "\n",
    "6.7) Use Tpot to automatically generate a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1475,
     "status": "aborted",
     "timestamp": 1602258890187,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "SY7TFF3AtO7v",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['load_boston',\n",
       " 'load_diabetes',\n",
       " 'load_digits',\n",
       " 'load_files',\n",
       " 'load_iris',\n",
       " 'load_breast_cancer',\n",
       " 'load_linnerud',\n",
       " 'load_sample_image',\n",
       " 'load_sample_images',\n",
       " 'load_svmlight_file',\n",
       " 'load_svmlight_files',\n",
       " 'load_wine']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Available datasets:\n",
    "[name for name in data_load.__all__ if \"load\" in name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to 6.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1465,
     "status": "aborted",
     "timestamp": 1602258890188,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "ncFf3f1LtO7x",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n"
     ]
    }
   ],
   "source": [
    "# The dataset is loaded\n",
    "digits_data = data_load.load_digits()\n",
    "print(digits_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#digits_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to 6.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(digits_data[\"data\"], digits_data[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to 6.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cross_val_predict(rf, digits_data[\"data\"], digits_data[\"target\"], cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier, as computed using 10-fold crossvalidation, is:  0.9476905954368392\n",
      "The balanced accuracy of the classifier, as computed using 10-fold crossvalidation, is:  0.9476838210482278\n",
      "Confusion matrix for the predictions made by the classifier:\n",
      "[[176   0   0   0   2   0   0   0   0   0]\n",
      " [  0 176   0   0   0   1   2   0   1   2]\n",
      " [  0   2 165   2   0   0   0   0   6   2]\n",
      " [  0   0   1 166   0   3   0   2   8   3]\n",
      " [  0   1   0   0 176   0   0   3   0   1]\n",
      " [  0   0   0   1   1 171   2   0   2   5]\n",
      " [  1   1   0   0   1   1 177   0   0   0]\n",
      " [  0   0   0   0   2   0   0 171   0   6]\n",
      " [  0   7   2   0   1   0   0   2 162   0]\n",
      " [  0   1   0   4   0   3   0   5   4 163]]\n"
     ]
    }
   ],
   "source": [
    "rf_accuracy = metrics.accuracy_score(digits_data[\"target\"], prediction) \n",
    "print(\"The accuracy of the classifier, as computed \\\n",
    "using 10-fold crossvalidation, is: \", rf_accuracy)\n",
    "\n",
    "rf_balanced_accuracy = metrics.balanced_accuracy_score(digits_data[\"target\"], prediction) \n",
    "print(\"The balanced accuracy of the classifier, as computed \\\n",
    "using 10-fold crossvalidation, is: \", rf_balanced_accuracy)\n",
    "\n",
    "rf_confusion_matrix = metrics.confusion_matrix(digits_data[\"target\"], prediction)\n",
    "print(\"Confusion matrix for the predictions made by the classifier:\")\n",
    "print(rf_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to 6.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pca is defined\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# pca is fitted to the data\n",
    "pca.fit(digits_data[\"data\"])\n",
    "\n",
    "# the 3 principal components are extracted\n",
    "reduced_data = pca.transform(digits_data[\"data\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(reduced_data, digits_data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cross_val_predict(rf, reduced_data, digits_data[\"target\"], cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier, as computed using 10-fold crossvalidation, is:  0.7278797996661102\n"
     ]
    }
   ],
   "source": [
    "rf_accuracy = metrics.accuracy_score(digits_data[\"target\"], prediction) \n",
    "print(\"The accuracy of the classifier, as computed \\\n",
    "using 10-fold crossvalidation, is: \", rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to 6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components of the first pipeline\n",
    "knn = KNeighborsClassifier()    \n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# The first pipeline is defined\n",
    "knn_scale_pipeline = Pipeline([(\"scaler\", scaler), (\"k-NN\", knn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to 6.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is:  0.9877573734001113\n"
     ]
    }
   ],
   "source": [
    "knn_scale_pipeline.fit(digits_data[\"data\"], digits_data[\"target\"])\n",
    "prediction = knn_scale_pipeline.predict(digits_data[\"data\"])\n",
    "\n",
    "knn_accuracy = metrics.accuracy_score(digits_data[\"target\"], prediction) \n",
    "print(\"The accuracy of the classifier is: \", knn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to 6.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=10, verbosity=2, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5x9CcE2tO7g"
   },
   "source": [
    "Then, a good pipeline is learned using the \"fit\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289,
     "referenced_widgets": [
      "dceee056709041f4b2c651cf70b2f705",
      "eb71357051704335918278cd6ab98e32",
      "072896e1172a455dbcf64195a75e7a8b",
      "f674abb1f6f04712b4ffe79704334da4",
      "85a4459e63f445c99cc7c27051d35a10",
      "b61ea6ad5f6a4e598d28f86e3a8458fd",
      "f2597eb9e6854cfeb205f368bece1760",
      "0a91d819bbab447c983b7f2138b818f8"
     ]
    },
    "executionInfo": {
     "elapsed": 103833,
     "status": "ok",
     "timestamp": 1602259010774,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "VXf1GxdqtO7i",
    "outputId": "191196cf-4947-4848-9f0c-b576594cd112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Optimization Progress'), FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.934905601980811\n",
      "Generation 2 - Current best internal CV score: 0.934905601980811\n",
      "Generation 3 - Current best internal CV score: 0.9382404828226555\n",
      "Generation 4 - Current best internal CV score: 0.9382404828226555\n",
      "Generation 5 - Current best internal CV score: 0.9382404828226555\n",
      "Best pipeline: MLPClassifier(MinMaxScaler(input_matrix), alpha=0.0001, learning_rate_init=0.001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(generations=5,\n",
       "               log_file=<ipykernel.iostream.OutStream object at 0x7fef3cf7a6d8>,\n",
       "               population_size=10, random_state=16, verbosity=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.fit(digits_data[\"data\"], digits_data[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "symFtYk8tO7m"
   },
   "source": [
    "Now we can see what the result is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1515,
     "status": "aborted",
     "timestamp": 1602258890185,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "euApK-ZLtO7m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('minmaxscaler', MinMaxScaler()),\n",
       " ('mlpclassifier', MLPClassifier(random_state=16))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.fitted_pipeline_.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IjmuvzMtO71"
   },
   "source": [
    "Once you have a reference value of the quality of the model, try to tune the parameters of the algorithm so that the model quality improves. Also, add a preprocessor, or change the classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1457,
     "status": "aborted",
     "timestamp": 1602258890189,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "Bmx2bxJLtO72"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1448,
     "status": "aborted",
     "timestamp": 1602258890190,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "35vQ680itO75"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRRTPa1HtO76"
   },
   "source": [
    "## Optional exercises\n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "Using the implementation completed for the MyLogisticRegression, implement a LDA classifier. Test its results by comparing it to MyLogisticRegression via accuracy (with 5-fold cross-validation) and confusions matrix. Additionally, you can generate figures with the plot_points function\n",
    "\n",
    "Note that the LDA method for predicting the probability of an observation belonging to a class is:\n",
    "\n",
    "$$p(y=1|x,\\theta)=sigm(w^T(x-x_0))$$\n",
    "\n",
    "where $y$ is the class, $x$ is our data, $\\theta$ are the parameters of the model ($w$ and $x_0$), and $sigm$ is the sigmoid function.\n",
    "\n",
    "One simple method for computing the parameters of the model is:\n",
    "\n",
    "$$w=\\Sigma^{-1}(\\mu_1-\\mu_0)$$\n",
    "$$x_0=\\frac{1}{2}(\\mu_1+\\mu_0)-(\\mu_1+\\mu_0)\\frac{log(\\frac{\\pi_1}{\\pi_0})}{(\\mu_1-\\mu_0)^T\\Sigma^{-1}(\\mu_1-\\mu_0)}$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix of the variables, $\\mu_c$ is the mean of the variable values for class $c$, and $\\pi_c$ is the $c$ class probability probability.\n",
    "\n",
    "Tip: Assuming that the problem the classifier is going to deal with has the same number of observations for both classes, the formula for computing the $x_0$ parameter can be greatly simplified.\n",
    "\n",
    "Tip2: To compute the inverse of a matrix (in this case $\\Sigma^{-1}$), you can use the np.linalg.inv function. For computing the covariance matrix, np.cov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1435,
     "status": "aborted",
     "timestamp": 1602258890190,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "7mnrCB46tO7-"
   },
   "outputs": [],
   "source": [
    "class MyLDAClassifier(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "    def fit(self, X, y):        \n",
    "    \n",
    "    def net_input(self, X):\n",
    "    \n",
    "    def activation(self, z):\n",
    "    \n",
    "    def predict(self, X):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1423,
     "status": "aborted",
     "timestamp": 1602258890191,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "X-a7Qr_VtO8B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9DWvALQtO8C"
   },
   "source": [
    "### Exercise 8\n",
    "\n",
    "Below we want to learn a k-nearest-neighbor classifier from the train_data and use it \n",
    "to predict the classes of the test data. The problem has two classes. The positive class\n",
    "is formed by vectors with 4 or more zeros. The negative class is formed by vectors of 3 or less zeros.\n",
    "\n",
    "Below, replace the XXX in LINE 1, LINE 2, and LINE 3 to get the predictions for the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 1415,
     "status": "aborted",
     "timestamp": 1602258890192,
     "user": {
      "displayName": "Julen Etxaniz Aragoneses",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64",
      "userId": "06956422670240182492"
     },
     "user_tz": -120
    },
    "id": "6RAfkaYmtO8D",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data\n",
      " [[1 1 1 0 1]\n",
      " [0 1 1 1 0]\n",
      " [1 1 0 1 0]\n",
      " [1 1 0 1 0]\n",
      " [0 0 1 0 1]\n",
      " [1 0 1 1 0]\n",
      " [0 0 1 1 0]\n",
      " [1 0 1 1 1]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]]\n",
      "predictions for test data [1 1 0 0 1 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Our training dataset has 10 instances\n",
    "# Each instance has 5 variables\n",
    "train_data = np.array([[1,1,1,1,1],\n",
    "                         [1,1,1,0,1],\n",
    "                         [1,1,0,1,1],\n",
    "                         [1,1,1,0,1],\n",
    "                         [1,0,1,1,1],\n",
    "                         [0,0,0,0,0],\n",
    "                         [0,0,0,1,0],\n",
    "                         [0,0,0,0,1],\n",
    "                         [1,0,0,0,0],\n",
    "                         [0,1,0,0,0]])\n",
    "\n",
    "# We have the classes for the ten instances\n",
    "# The first five instances belong to class 1\n",
    "# the rest to class 2\n",
    "train_classes = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "test_data = np.random.randint(low=0,high=2,size=(10,5)) \n",
    "print(\"test_data\\n\", test_data)              \n",
    "                        \n",
    "\n",
    "knn = KNeighborsClassifier()      # LINE 1                                      \n",
    "knn.fit(train_data,train_classes)                  # LINE 2\n",
    "\n",
    "prediction_test = knn.predict(test_data) # LINE 3\n",
    "\n",
    "print(\"predictions for test data\", prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab2_Classifiers-Students.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "072896e1172a455dbcf64195a75e7a8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Optimization Progress: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b61ea6ad5f6a4e598d28f86e3a8458fd",
      "max": 60,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_85a4459e63f445c99cc7c27051d35a10",
      "value": 60
     }
    },
    "0a91d819bbab447c983b7f2138b818f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85a4459e63f445c99cc7c27051d35a10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b61ea6ad5f6a4e598d28f86e3a8458fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dceee056709041f4b2c651cf70b2f705": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_072896e1172a455dbcf64195a75e7a8b",
       "IPY_MODEL_f674abb1f6f04712b4ffe79704334da4"
      ],
      "layout": "IPY_MODEL_eb71357051704335918278cd6ab98e32"
     }
    },
    "eb71357051704335918278cd6ab98e32": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2597eb9e6854cfeb205f368bece1760": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f674abb1f6f04712b4ffe79704334da4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a91d819bbab447c983b7f2138b818f8",
      "placeholder": "​",
      "style": "IPY_MODEL_f2597eb9e6854cfeb205f368bece1760",
      "value": " 60/60 [01:38&lt;00:00,  1.69s/pipeline]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

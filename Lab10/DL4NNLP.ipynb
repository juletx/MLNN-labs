{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Table of Content\n",
    "----\n",
    "\n",
    "<a href='#section1'> 1. Introduction</a>\n",
    "\n",
    "__PART 1: Sentiment Analysis__\n",
    "\n",
    "<a href='#section2'>2. Text Representation in DL</a>\n",
    "> <a href='#section2_load_data'>2.1 Load data</a>\n",
    "\n",
    "> <a href='#section2_examin_data'>2.2 Examine the dataset</a>\n",
    "\n",
    "> <a href='#section2_one_hot'>2.3 One-hot encoding of the data</a>\n",
    "\n",
    "> <a href='#section2_word_index'>2.4 Inspect created word-index</a>\n",
    "\n",
    "<a href='#section3'>3. What's going on inside a MLP</a>\n",
    "\n",
    "<a href='#section4'>4. MLP in Keras</a>\n",
    "\n",
    "> <a href='#section4_model_optimization'>4.1. Model Optimization</a>\n",
    "\n",
    "<a href='#section5'>5. Other models for text</a>\n",
    "\n",
    "> <a href='#section5_padding'>5.1 Padding sequences </a>\n",
    "\n",
    "> <a href='#section5_lstm'>5.2 Build a LSTM model in few lines</a>\n",
    "\n",
    "> <a href='#section5_cnn'>5.3 1D CNN for texts (optional)</a>\n",
    "\n",
    "\n",
    "__PART 2: Word Embeddings__\n",
    "\n",
    "<a href='#section6'>6. Set up</a>\n",
    "\n",
    "<a href='#section7'>7. Task 1: Semantically similar/related words</a>\n",
    "\n",
    "<a href='#section8'>8. Task 2: Semantic orientation</a>\n",
    "\n",
    "<a href='#section9'>9. Task 3: Analogy</a>\n",
    "\n",
    "<a href='#section10'>10. Visualization (optional)</a>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Introduction\n",
    "\n",
    "This lab session is organized in two main parts. The first part is devoted to build a classifier for textual input. In the second part we will work with word embeddings to understand their usefulness.  \n",
    "\n",
    "### Part 1: Sentiment Analysis (1h approx)\n",
    "\n",
    "In first part we will implement a series models for __sentence classification__ using Keras and TensorFlow. Given a sentence our model will predict if it is a positive or negative piece of texts. The dataset we are going to use ranges the polarity annotation from 0 to 5, where 0 denotes extremelt negative sentiment,  and 5  is the most  positive. \n",
    "\n",
    "Nevertheless, for this lab we will simplify the task, and we will translate the 5-way classification task (actually [0-5] regression task) into 2-way classification task (0 $\\rightarrow$ _negative,_ ;1 $\\rightarrow$ positive),\n",
    "\n",
    "All in all, the main __objectives__ of this lab session for this part are the following: \n",
    "- How to represent text as input of neural network models (the _one-hot econding_).\n",
    "- Understant the mechanics of a Multi-Layer Perceptron (a.k.a Feedforward layer) with TensorFlow.\n",
    "- Learn how to build, train and evaluate suitable for text model in Keras.\n",
    "\n",
    "-----\n",
    "\n",
    "Optionally, once you finished the whole lab, you could explore following things:\n",
    "- Explore hyperparameters like:\n",
    "  - Optimizers: SGD, ADAGRAD, etc.\n",
    "  - Learning Rates\n",
    "  - Regularization (more on this next lab)\n",
    "- Plot learning curves for model selection\n",
    "\n",
    "----\n",
    "\n",
    "### Part 2: Word Embeddings (1h approx)\n",
    "\n",
    "In the second part of the session  we will learn how to work with word embeddings. We show that applying simple techniques we can implement very cool stuff. \n",
    "\n",
    "This part is divided in many parts that in most of the cases do not need code implementation. Hopefully, you would complete the assignment in a short time and have great fun too. \n",
    "\n",
    "The main __objective__ of this part is to learn about some interesting task that can be acomplish with word embeddings:\n",
    "\n",
    "- __Task 1__: Word similarity and relatedness\n",
    "- __Task 2__: Score semantically the words\n",
    "- __Task 3__: Do analogies, like _Man is to King like Woman is to Queen_\n",
    "\n",
    "------\n",
    "\n",
    "## Exercise Index\n",
    "__Part 1 exercises__:\n",
    "\n",
    "<a href='#exercise1'>Exercise 1: Most vs leas frequent words</a>\n",
    "\n",
    "<a href='#exercise2'>Exercise 2: Coding MLP</a>\n",
    "\n",
    "<a href='#exercise3'>Exercise 3: Vocab size effect</a>\n",
    "\n",
    "<a href='#exercise4'>Exercise 4: Model optimization</a>\n",
    "\n",
    "<a href='#exercise5'>Exercise 5: One-hot encoding vs Embedding encoding</a>\n",
    "\n",
    "<a href='#exercise6'>Exercise 6: Building a LSTM</a>\n",
    "\n",
    "<a href='#exercise7'>Exercise 7 (optional): 1D CNN for text</a>\n",
    "\n",
    "-----\n",
    "__Part 2 exercises__:\n",
    "\n",
    "<a href='#exercise8'>Exercise 8: Word similarity/relatedness</a>\n",
    "\n",
    "<a href='#exercise9'>Exercise 9: Semantics orientation</a>\n",
    "\n",
    "<a href='#exercise10'>Exercise 10: Analogy</a>\n",
    "\n",
    "<a href='#exercise11'>Exercise 11: Visualization (optional)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Text Representation in DL\n",
    "\n",
    "Once data is loaded the next step is to preprocess it to obtain the vectorized form (i.e. the process of transforming text into numeric tensors), which basically consist of:\n",
    "\n",
    "- Tokenization, tipically segment the text into words. (Alternatively, we could segment text into characters, or extract n-grams of words or characters.)\n",
    "- Definition of the dictionary index and vocabulary size (in this case we set to 1000 most frequent words)\n",
    "- Transform each word into a vector. \n",
    "\n",
    "\n",
    "There are multiple ways to vectorize tokens. The main two are the following: ___One-hot encoding___ and ___word embedding___. In the first part of the lab, we'll make use of the basic tools provided by Keras to obtain __the one-hot encoding__. \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "<a id='section2_load_data'></a>\n",
    "### Load data\n",
    "\n",
    "We provide the __Movie Review from Rottern Tomatoes dataset (MRRT)__ dataset for sentiment analysis, which the original can be downloaded from [here](https://www.cs.cornell.edu/people/pabo/movie-review-data/). We are using version 1.0. Please, __download__ the dataset ready for the lab from here:\n",
    "\n",
    "- Download data from [this link](https://www.dropbox.com/s/kej8qc13fhgdd8c/data.tar.bz2?dl=0)\n",
    "\n",
    "\n",
    "Place the decompressed file in the same path of the notebook. You should have the dataset in the current folder: ```./data/rt-polaritydata/```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(pos_file, neg_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(pos_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(neg_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [0 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    data = pd.DataFrame(data={'text': x_text, 'label': y})\n",
    "    data = shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and make train and development partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: (10662, 2)\n",
      "\n",
      "Training set examples: 9000\n",
      "Development set examples: 1000\n"
     ]
    }
   ],
   "source": [
    "# data home\n",
    "sst_home = './data/rt-polaritydata/'\n",
    "dataset = load_data_and_labels(sst_home+'/rt-polarity.pos', sst_home+'rt-polarity.neg')\n",
    "\n",
    "print(\"dataset size:\", dataset.shape)\n",
    "\n",
    "training_set = dataset.head(9000)\n",
    "dev_set = dataset.tail(1000)\n",
    "\n",
    "# Obtain text and label vectors as lists \n",
    "train_texts = list(training_set.text)\n",
    "train_labels = list(training_set.label)\n",
    "\n",
    "dev_texts = list(dev_set.text)\n",
    "dev_labels = list(dev_set.label)\n",
    "\n",
    "print('')\n",
    "print('Training set examples: {}'.format(len(training_set)))\n",
    "\n",
    "print('Development set examples: {}'.format(len(dev_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2_examin_data'></a>\n",
    "### Examine the dataset\n",
    "We print some example of negative and positive cases, just to make an idea of what type of text we have.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7629</th>\n",
       "      <td>director clare kilner 's debut is never as daf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>it offers little beyond the momentary joys of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7807</th>\n",
       "      <td>sometimes smart but more often sophomoric</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10621</th>\n",
       "      <td>though there are many tense scenes in trapped ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>there are many things that solid acting can do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9414</th>\n",
       "      <td>after collateral damage , you might imagine th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10127</th>\n",
       "      <td>a dull , simple minded and stereotypical tale ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>an empty shell of an epic rather than the real...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071</th>\n",
       "      <td>befuddled in its characterizations as it begin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9179</th>\n",
       "      <td>an uninspired preachy and clich d war film</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "7629   director clare kilner 's debut is never as daf...      0\n",
       "7322   it offers little beyond the momentary joys of ...      0\n",
       "7807           sometimes smart but more often sophomoric      0\n",
       "10621  though there are many tense scenes in trapped ...      0\n",
       "9986   there are many things that solid acting can do...      0\n",
       "9414   after collateral damage , you might imagine th...      0\n",
       "10127  a dull , simple minded and stereotypical tale ...      0\n",
       "9915   an empty shell of an epic rather than the real...      0\n",
       "6071   befuddled in its characterizations as it begin...      0\n",
       "9179          an uninspired preachy and clich d war film      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a sample of negative text chunks\n",
    "training_set[training_set.label == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>elling really is about a couple of crazy guys ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>a rock solid gangster movie with a fair amount...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>a film that takes you inside the rhythms of it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>sweet and memorable film</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>one of those rare films that seems as though i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>this road movie gives you emotional whiplash ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>ultimately , mib ii succeeds due to its rapid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3617</th>\n",
       "      <td>there 's a spontaneity to the chateau , a sens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>with the gifted pearce on hand to keep things ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>you just know something terrible is going to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "484   elling really is about a couple of crazy guys ...      1\n",
       "3525  a rock solid gangster movie with a fair amount...      1\n",
       "893   a film that takes you inside the rhythms of it...      1\n",
       "2021                           sweet and memorable film      1\n",
       "4211  one of those rare films that seems as though i...      1\n",
       "511   this road movie gives you emotional whiplash ,...      1\n",
       "1903  ultimately , mib ii succeeds due to its rapid ...      1\n",
       "3617  there 's a spontaneity to the chateau , a sens...      1\n",
       "2381  with the gifted pearce on hand to keep things ...      1\n",
       "2258  you just know something terrible is going to h...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a sample of positive text chunks\n",
    "training_set[training_set.label == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2_one_hot'></a>\n",
    "### One-hot encoding of the data\n",
    "\n",
    "One-hot encoding is the most basic way to convert a token into a vectort. Here, we'll turn the input vectors into (0,1)-vectors. The process consist of associating a unique integer-index with every word in the vocabulary.\n",
    "\n",
    ">>>>![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/vectorize_small.png)\n",
    "\n",
    "\n",
    "For example, if the tokenized vector contains a word that its dictionary index is 14, then in the processed vector, the 14th entry of the vector will be 1 and the rest will set to 0.\n",
    "\n",
    "Note that when using keras built-in tools for indexing, ```0``` is a reserved index that won't be assigned to any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juletx/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/juletx/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text of the first examples: \n",
      "elling really is about a couple of crazy guys , and it 's therapeutic to laugh along with them\n",
      "\n",
      "Vector of the first example:\n",
      "[0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Binary representation of the output:\n",
      "1\n",
      "\n",
      "Shape of the training set (nb_examples, vector_size): (9000, 1000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# Create a tokenize that takes the nb_words most common words\n",
    "nb_words= 1000\n",
    "tokenizer = text.Tokenizer(num_words=nb_words)\n",
    "\n",
    "# Build the word index (dictionary)\n",
    "tokenizer.fit_on_texts(train_texts) # Create word index using only training part\n",
    "\n",
    "# Vectorize texts into one-hot encoding representations\n",
    "x_train = tokenizer.texts_to_matrix(train_texts, mode='binary')\n",
    "x_dev = tokenizer.texts_to_matrix(dev_texts, mode='binary')\n",
    "          \n",
    "# Change variable names (not needed, actually)\n",
    "y_train = train_labels\n",
    "y_dev = dev_labels\n",
    "\n",
    "print('Text of the first examples: \\n{}\\n'.format(train_texts[0]))\n",
    "print('Vector of the first example:\\n{}\\n'.format(x_train[0]))\n",
    "print('Binary representation of the output:\\n{}\\n'.format(y_train[0]))\n",
    "\n",
    "print('Shape of the training set (nb_examples, vector_size): {}\\n'.format(x_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to obtain the lists of integers indices instead of the one-hot binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elling really is about a couple of crazy guys , and it 's therapeutic to laugh along with them\n",
      "[112, 6, 30, 2, 979, 4, 934, 583, 3, 8, 7, 5, 507, 395, 14, 120]\n"
     ]
    }
   ],
   "source": [
    "# Turns strings into list of integer indices\n",
    "print(train_texts[0])\n",
    "one_hot_results = tokenizer.texts_to_sequences(train_texts)\n",
    "print(one_hot_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2_word_index'></a>\n",
    "### Inspect created word-index\n",
    "It is good idea to check the word index created for the task at hand, as this is going to be input representation for the model. \n",
    "\n",
    "The code below prints out the most common words and the least common words in the training-set. Note that the our vocabulary is defined by the 1000 most common words in the training part, but the tokniezer extracted 17359 word-types.\n",
    "\n",
    "<a id='exercise1'></a>\n",
    "#### Exercise 1:\n",
    "\n",
    "> Try to answer the following questions:\n",
    ">\n",
    "> - What can be the problem of not removing __the most frequent__ words?\n",
    "> - What can be the problem of not removing __the least frequent__ words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17405 unique tokens.\n",
      "\n",
      "Show the most frequent word index:\n",
      "   the (8670) --> 1\n",
      "   a (6191) --> 2\n",
      "   and (5282) --> 3\n",
      "   of (5197) --> 4\n",
      "   to (3603) --> 5\n",
      "   is (3001) --> 6\n",
      "   's (2999) --> 7\n",
      "   it (2926) --> 8\n",
      "   in (2261) --> 9\n",
      "   that (2260) --> 10\n",
      "\n",
      "Show the least frequent word index:\n",
      "   robberies (1) --> 9183\n",
      "   kilner (1) --> 9184\n",
      "   whiplash (1) --> 9185\n",
      "   distressing (1) --> 9186\n",
      "   heartedness (1) --> 9187\n",
      "   pearce (1) --> 9188\n",
      "   stable (1) --> 9189\n",
      "   unprepared (1) --> 9190\n",
      "   someday (1) --> 9191\n",
      "   aggrieved (1) --> 9192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recorver the word index that was created with the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.\\n'.format(len(word_index)))\n",
    "word_count = tokenizer.word_counts\n",
    "print(\"Show the most frequent word index:\")\n",
    "for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=True)):\n",
    "    print('   {} ({}) --> {}'.format(word, word_count[word], word_index[word]))\n",
    "    if i == 9: \n",
    "        print('')\n",
    "        break\n",
    "\n",
    "print(\"Show the least frequent word index:\")\n",
    "for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=False)):\n",
    "    print('   {} ({}) --> {}'.format(word, word_count[word], word_index[word]))\n",
    "    if i == 9: \n",
    "        print('')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. What’s going on inside a MLP? \n",
    "\n",
    "In this part, we provide you most of the code that in implement of a basic two layer (one hidden, and one output) multi-layer perceptron.  \n",
    "\n",
    "<a id='exercise2'></a>\n",
    "#### Exercise 2:\n",
    "> Modify the code to turn it into an MLP with one ReLU hidden layers of 50 dimensions. __Follow the instructions in the code__.\n",
    "\n",
    "> __Note__: Keep in mind that initializing weight matrices with zeros causes problems in deep neural networks trained by SGD. You should use tf.random_normal instead, with stddev=0.1.\n",
    "\n",
    "<a id='exercise3'></a>\n",
    "#### Exercise 3: \n",
    "> The model seems to learn something (loss values are slowly going down), but not much. Augment the size of vocabulary to 5000. Recall you need to preprocess the data again!\n",
    ">\n",
    "> > __I recommend to re-initialize kernel before you run again the model. I don't know why with re-initializing the model seems to get stuck in local minima when runing for the second time.__\n",
    ">\n",
    "> What is happening now? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, examples, labels):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(examples)\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "    for i, label in enumerate(labels):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == label:\n",
    "            correct += 1        \n",
    "    return correct / float(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-cb33bf7abbd0>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-cb33bf7abbd0>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    self.W1 =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class logistic_regression_classifier:\n",
    "    def __init__(self, dim):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 1.0  # Should be about right\n",
    "        self.training_epochs = 50  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = dim  # The number of features\n",
    "        self.batch_size = 128  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        \n",
    "        # TODO: Use this.\n",
    "        self.hidden_layer_size = 50\n",
    "        \n",
    "        # TODO: Overwrite this section\n",
    "        ### Start of model definition ###\n",
    "        \n",
    "        # Define the inputs\n",
    "        self.x = tf.placeholder(tf.float32, [None, dim])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 2])\n",
    "        \n",
    "        # Define (most of) the model's variable: input_size x hidden_layer_size \n",
    "        self.W0 = tf.Variable(tf.random_normal([self.dim, self.hidden_layer_size], stddev=0.1))\n",
    "        self.b0 = tf.Variable(tf.random_normal([self.hidden_layer_size], stddev=0.1))\n",
    "        \n",
    "        # TODO: Define variable, hidden_layer_size x number_of classes\n",
    "        self.W1 =  \n",
    "        self.b1 = \n",
    "\n",
    "        \n",
    "        self.logits0 = tf.matmul(self.x, self.W0) + self.b0\n",
    "        self.h0 = tf.nn.relu(self.logits0)\n",
    "        \n",
    "        # TODO: Calculate the logits and prediction probabilities for each class. \n",
    "        self.logits1 = \n",
    "        self.pred =  \n",
    "        \n",
    "        ### End of model definition ###\n",
    "        \n",
    "        # Define the cost function\n",
    "        self.cost = tf.reduce_mean(-tf.reduce_sum(self.y*tf.log(self.pred), reduction_indices=1))\n",
    "        \n",
    "        # Optionally you could add L2 regularization term\n",
    "        \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W0, W1, b0 and b1\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_set, dev_set):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = dataset[0][indices]\n",
    "            labels = dataset[1][indices]\n",
    "            return vectors, labels\n",
    "        \n",
    "        def shuffle_data(dataset):\n",
    "            combined = list(zip(dataset[0], dataset[1]))\n",
    "            random.shuffle(combined)\n",
    "            dataset[0][:], dataset[1][:] = zip(*combined)\n",
    "            \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)\n",
    "        print ('Training.')\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            shuffle_data(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set[0]) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
    "                                                                    self.batch_size * i, \n",
    "                                                                    self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print (\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0][0:500], dev_set[1][0:500]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0][0:500], training_set[1][0:500]))\n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        logits = self.sess.run(self.pred, feed_dict={self.x: examples})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Converts the labels to a one-hot representation\n",
    "y_train_onehot = np.array(to_categorical(y_train))\n",
    "y_dev_onehot = np.array(to_categorical(y_dev))\n",
    "\n",
    "classifier = logistic_regression_classifier(nb_words)\n",
    "classifier.train([x_train, y_train_onehot], [x_dev, y_dev_onehot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. MLP in Keras\n",
    "\n",
    "When we build a neural network we usually take into account the following points:\n",
    "- The __layers__, and how they are combined (that is, the structure and parameters of the model)\n",
    "- The __input__ and the __labeled output__ data that the model needs to map.\n",
    "- __Loss function__ that signals how well the model is doing.\n",
    "- The __optimizier__ which defines the learning procedure.\n",
    "\n",
    "Keras provide a simple framework for combining layers. There are available two types of classes for building the model: The _Sequential_ Class and the _functional_ API. The later is dedicated to DAGs structures, which let you to build arbitrary models. The former is for linear stacks of layers, which is the most common and simplest type of archicture. \n",
    "\n",
    "We built a simple MLP model in TensorFlow which one the most simple neural network model, but I took some lines of codes. On the contrary, the MLP model can be implemented in Keras stacking ```Dense``` layers with some kind of non-linearity. \n",
    "\n",
    "Regarding input data, we will use the __one-hot encoding__ (see above). We'll set ```(binary) cross-entropy``` as a __loss function__ and ```sgd``` (_Stochastic Gradient Descent_) as the __optimizer__ to build an equivalent model in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# make reproducible code\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='relu', input_shape=(input_size,)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  \n",
    "# Note that we do not need to indicate the input shape for the sucessives layes\n",
    "\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "sgd = SGD(lr=1.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=25, batch_size=128, validation_data=(x_dev, y_dev), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "                    \n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc.')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "                    \n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax(history.history['val_acc'])\n",
    "print(\"Best accuracy on dev: {} (epoch {})\".format(history.history['val_acc'][best_idx], best_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reproducible models**\n",
    "Run the code below if you want to train your model from scratch. \n",
    "\n",
    "`Clear_session` will destroys the current TF graph and creates a new one. Useful to avoid clutter from old models / layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4_model_optimization'></a>\n",
    "### 4.1 Model Optimization\n",
    "\n",
    "How much can we improve the current classifier? We can follow multiple strategies to improve the performance of our model: \n",
    "\n",
    "- **Increase training size**: Usually this is the most effective way to improve the resuls, but it is the most costly at the same time. In our case we are using the whole dataset, so we can't exploit this estrategy (you can simulate by using smaller set of training as baseline).\n",
    "\n",
    "- **Increase vocabulary**: We are not using the whole vocabulary of the training set. Vocabulary pruning can be useful to avoid overfiting, but can affect generalization properties as well.\n",
    "\n",
    "- **The use of correct neural archictecture**: We can increase the expresivity of the model by adding more parameters in different forms: 1) increasing number of hidden layers, 2) increasing the size of layers, 3) combining both. \n",
    "\n",
    "- **The use of correct optimization strategy**: We can try more effective optimizers than SGD. For example, Adam optimizer is one of the most popular in NLP. In many cases correct learning rate value makes a difference. So exploring different learning rate values can be interesting choice to improve results.\n",
    "\n",
    "- **Number of epochs**: How long we traing is something we need to control as training to long can derive to overfit the model. On the contrary, if we train for a short time the model will hardly learn anything. There are different ways to select numbe of training epochs, but a common one is to inspect learning curves and select the point that has lower loss in development. \n",
    "\n",
    "- **Regularization weight, dropout, batch size, etc.**: Deep learning is full of hyperparameters and small *tricks* that are important to know. Each model in a particular task has its own optimum values which we don't know in advance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise4'></a>\n",
    "#### Exercise 4: \n",
    " * Explore what happens when we use the whole vocabulary instead to the most 1000 frequent words.\n",
    " * It looks like learning is quite unstable as the accuracy on development fluctuates quite significantly. Select a smaller learning rate (e.g. 0.1 or even smaller). You'll probably need to train longer.   \n",
    " * Now, explore what happens when we increase the model (e.g. 300 > 50 > 1). You might need to adjust the learning rate and the number of epochs. \n",
    " * Now, explore with a different optimizer (e.g Adam).\n",
    " \n",
    " What's your best accuracy and model's detail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# In order to use the whole vocabulary you need to preprocess the data again.\n",
    "# You can use the code in section 2.3\n",
    "\n",
    "# TODO: \n",
    "# Create a tokenizer object with 'num_words=None' as argument.\n",
    "\n",
    "# TODO:\n",
    "# Build the word index (dictionary)\n",
    "\n",
    "# TODO:\n",
    "# Vectorize texts into one-hot encoding representations\n",
    "\n",
    "# Change variable names (not needed, actually)\n",
    "y_train = train_labels\n",
    "y_dev = dev_labels\n",
    "\n",
    "nb_words = x_train.shape[1]\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n",
    "print(\"New input size: {}\".format(input_size))\n",
    "\n",
    "# TODO:\n",
    "# Define the model\n",
    "model_av = Sequential()\n",
    "    \n",
    "# TODO: \n",
    "# Define the optimizer (SGD, Adam, etc.)\n",
    "\n",
    "# TODO:\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "\n",
    "# TODO: \n",
    "# Fit the model with the appropiate number of epochs and batch size\n",
    "history_av = None\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_av.history['acc'])\n",
    "plt.plot(history_av.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc.')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax(history_av.history['val_acc'])\n",
    "print(\"Best accuracy on dev: {} (epoch {})\".format(history_av.history['val_acc'][best_idx], best_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Other models for text: LSTM\n",
    "\n",
    "When working with text it is typical to model the input as it would be time dependent series. Due to compositional nature of language, we could encode the semantics of the context conditioning on previous seen words, and **Deep Recurrent Neural Networks** are suitable to model these ideas. \n",
    "\n",
    "In this section, we will use Keras to implement a more advanced __Long Short Term Memory__ (LSTM) based sentiment classifier. \n",
    "\n",
    "----\n",
    "\n",
    "Remenber from the theory that Recurrent Neural Networks apply over and over the same function (the recursive cell) to every token in the sequence. In a simplified version the next token is combined with the output of the previous _state_ (contains the information of what has been seen so far) into the recursive function, so that the whole sequence is represented in a single vector. \n",
    "\n",
    "Figure below shows an unrolled LSTM archicture, in which input text sequence, once tokenized and obtained the word index, is represented by word embeddings. Emebdding lookup layer takes a list of word indexes and returns a list word embeddings (low-dimensional dense vectors that represent words). These word embeddings are what actually fed to sentence encoder. Finally, the last output of the LSTM is fed to a fully connected Dense layer. As we'll learn, this is fairly easy to code with Keras.\n",
    "\n",
    "\n",
    "![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/LSTM_sentiment.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5_padding'></a>\n",
    "### Padding sequences\n",
    "When process data to feed a recurrent archicture, we need to do it differently compared to what we have seen so far. Unrolling each sequence one by one would take for ever, as we would lost the capability for parallelization. That's why we need to pad the input sentences. That is, as learning is typically done by mini-batching the training data, it requires having same sequence length for all the input examples in the mini-batch so we can run the operation in parallel over the whole batch.\n",
    "\n",
    "In order to do mini-batch we need to do the following:\n",
    "- Choose a single unrolling constant N (e.g. max sequence length)\n",
    "- Pad first words with zeros (shifting right)\n",
    "\n",
    "\n",
    "There are more sophisticated alternivative like shuffling examples by sentence length (set N to max. length in mini-batch), but for our purposes that would be enough. \n",
    "\n",
    "In the following chunk of code, we will use Keras built-in functions for padding sequence. Note that tokenization and word-indexing is already done in <a href='#section2_one_hot'>Section 2.3 (One-hot encoding of the data)</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall from previous code sections:\n",
    "# - training text examples: train_texts\n",
    "# - training labels: train_labels\n",
    "#\n",
    "# - development text examples: dev_texts\n",
    "# - development labels: dev_labels\n",
    "\n",
    "from keras import preprocessing\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_seq = 40 \n",
    "\n",
    "# Get data as a lists of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_texts)\n",
    "nb_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding data: Turn the lists of integers into a 2D integer tensor of shape `(samples, max_seq)`\n",
    "x_train = preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_seq)\n",
    "x_dev = preprocessing.sequence.pad_sequences(dev_sequences, maxlen=max_seq)\n",
    "\n",
    "y_train = train_labels\n",
    "y_dev = dev_labels\n",
    "\n",
    "print('Shape of the training set (nb_examples, vector_size): {}'.format(x_train.shape))\n",
    "print('Shape of the validation set (nb_examples, vector_size): {}'.format(x_dev.shape))\n",
    "\n",
    "# Print some examples\n",
    "print()\n",
    "print('TEXT: {}\\nPADDED: {}'.format(train_texts[0], x_train[0]))\n",
    "print()\n",
    "print('TEXT: {}\\nPADDED: {}'.format(train_texts[1], x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise5'></a>\n",
    "#### Exercise 5\n",
    ">Differences of the embedding representation with the one-hot encoding.\n",
    ">- Do we keep now the temporal order of words?\n",
    ">- What is one of the goal of the embedding table? Would be possile to apply LSTMs to text with the one-hot encoding representation?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section5_lstm></a>\n",
    "### Build a LSTM model in few lines\n",
    "In this section we will build our first LSTM-based classifier in Keras. Note that the LSTM layer, like the rest of layers in Keras, takes inputs of shape ```[batch_size, sequence_length, input_features]```. This is the reason of why we perform padding when preprocessed the data in the previous section.\n",
    "\n",
    "Note that after the padding our data still is 2D tenfor of shape ```[batch_size, sequence_length]```. Transformation from 2D to 3D is done with```Embedding``` layer in Keras, which takes the 2D tensor (```[batch_size, sequence_length]```). ```sequence_length``` is an entry of a list of word indexes, where all the entries in the bacth have the same length. (That's why we padded with zeros the shorter sequences).\n",
    "\n",
    "```Embedding``` layer return a tensor of shape ```[batch_size, sequence_length, embedding_size]```. This can be understood like adding the corresponding embedded vector to each word in the sequence. The layer can be initialized at radom and learn with backpropagation, or use precomputed embedding vector like _Word2vec_ or _Glove_.\n",
    "\n",
    "Once our data is represented with 3D tensors we can use directly the ```LSTM``` layer. For this task we will combine three Keras layers in this specific order: \n",
    "\n",
    "- ```Embedding``` layer: it will transfor the data from 2D to 3D by addig associated embeddings to words in the sequences. In the constructor we need to specify two arguments: \n",
    "   - input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "   - output_dim: int >= 0. Dimension of the dense embedding.\n",
    "   \n",
    "- ```LSTM``` layer: It will encode the input sequnces and return output tensor. For the LSTM we need to specify the number of units of the LSTM:\n",
    "   - units: Positive integer, dimensionality of the output space.\n",
    "   \n",
    "- ```Dense```: It will take the output of the LSTM as input and perform the classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise6'></a>\n",
    "#### Exercise 6: \n",
    "> Complete the code to turn it into a LSTM. You just need to add the LSTM layer in Keras, and another layer for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# nb_words defined above \n",
    "print('Vocabulary size: {}'.format(nb_words))\n",
    "embedding_size = 128\n",
    "\n",
    "# TODO: Use this\n",
    "lstm_hidden_size = 128 \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1. Define and add Embedding layer to the model. \n",
    "#    Note we are using mask_zero=True as we want to ignore the '0' words in the padding\n",
    "model.add(Embedding(nb_words, embedding_size, mask_zero=True))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(batch_size, max_seq, embedding_size)`.\n",
    "\n",
    "# TODO: \n",
    "# 2. Define and add LSTM layer to the model.\n",
    "\n",
    "# TODO:\n",
    "# 3. Define and add Dense layer to the model\n",
    "\n",
    "\n",
    "# Define optimizer:\n",
    "opt=Adam(lr=0.001)\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_dev, y_dev), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax(history.history['val_acc'])\n",
    "print(\"Best accuracy on dev: {} (epoch {})\".format(history.history['val_acc'][best_idx], best_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5_cnn'></a>\n",
    "### 1D CNNs for texts (optional)\n",
    "\n",
    "The model archicture fot texts are much like a 2D CNN that are tipically used in computer vision. Instead of having two dimension for applying convolutional filters, in this case, due to the sequential nature of the data, convolutions are applied along one axis (See image below). \n",
    "\n",
    "CNN based archictures for sentence classification have tipically the following layers:\n",
    "\n",
    "- __Convolutional layer__: It uses convolutions that extracts local 1D patches (subsequences) from the imput sentences. Recall that word indeces are first transformed to embedding space. This type of convolutions can recognize local patterns much bigrams and trigrams.  As explained in class, convolution layers change the shape of the input sequence. Depending on the type of padding and stride the length of the output would be longer, same size, or shorter. The depending on the number of filters used the output depth would change as well.\n",
    "\n",
    "  If ```padding = 'same'```, then the spatial dimensions of the convolutional layer are the following:\n",
    "\n",
    "  ```output_length = ceil(input_length / stride)```\n",
    "\n",
    "  If ```padding = 'valid'```, then the spatial dimensions of the convolutional layer are the following:\n",
    "\n",
    "  ```output_length = ceil(input_length - kernel_size + 1) / stride)```\n",
    "\n",
    "\n",
    "- __Max-pooling layer__: The main objective is to reduce drastically the output size of the convolution layers. 1D pooling operation is the same as the 2D. It takes 1D subsequences from convolutional output as input and return the maximum value. Similarly to convolutional layer, depending on the ```pool_size```, ```stride``` and ```padding``` we'd obtain different reduction of the input size.\n",
    "\n",
    "- __Fully connected layer__: We can flatten the output of the max-pooling layer and feed to fully-connected layer to perform _positive_ vs _negative_ classification.\n",
    "\n",
    "\n",
    ">>>![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/CNN_for_texts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise7'></a>\n",
    "#### Exercise 7 (optional)\n",
    "Following the idea explained in the introduction of the notebook, you will stack two layers of ```convolution+max-pooling```. Note that the last layer of max-pooling is ```GlobalMaxPooling1D```, which returns a flat vector that can be directly feed in the ```Dense``` layer for doing classification.\n",
    "\n",
    "__Hint__: You need to combine the following keras layers:\n",
    "- ```Embedding```\n",
    "- ```Conv1D```\n",
    "- ```MaxPoolling```\n",
    "- ```GlobalMaxPooling1D```\n",
    "- ```Dense```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "max_words = nb_words ## defined above\n",
    "max_seq = 40 ## defined above\n",
    "embedding_size = 128 \n",
    "output_filters = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# TODO: Define the model. conv1d+max_pooling + convd1+globalmaxpooling + dense\n",
    "#       Activations. Typically RELU works OK in this type of tasks.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Word Embeddings\n",
    "\n",
    "In this part of the lab session we will learn what kind of thing can be don with the word embeddings. \n",
    "\n",
    "We will focus in three task:\n",
    "- __Task 1__: Word similarity and relatedness\n",
    "- __Task 2__: Score semantically the words\n",
    "- __Task 3__: Do analogies, like _Man is to King like Woman is to Queen_\n",
    "\n",
    "But as always, We will start setting up the environment and loading the data . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## Set up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firtst define some helper functions for printing results and reading embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_header(title):\n",
    "    print('┌───────────────────────────────────────────────────────────────┐')\n",
    "    print('│{0:^63}│'.format(title))\n",
    "    print('├──────────┬─────────────────────────────┬──────────┬───────────┤')\n",
    "\n",
    "def print_footer():\n",
    "    print('└──────────┴─────────────────────────────┴──────────┴───────────┘')\n",
    "\n",
    "def print_oov(oov):\n",
    "    if len(oov) > 0:\n",
    "        print('OOV: {0}'.format(', '.join(oov)))\n",
    "\n",
    "def print_row(index, last, trg_words, knn, sim):\n",
    "    if last >= index or index < 0:\n",
    "        return last\n",
    "    if last < index - 1:\n",
    "        print('│  {0:>5}  │  {0:^25}  │  {0:>5}   │  {0:^7} │'.format('⋮'))\n",
    "    word = trg_words[knn[index]]\n",
    "    word = ('{0:^25}').format(word)\n",
    "    print('│  {0:>6}  │  {1}  │  {2:>6}  │  {3:7.4f}  │'.format(index + 1, word, knn[index], sim[knn[index]]))\n",
    "    return index\n",
    "\n",
    "def read(file, threshold=0, dim=50, vocabulary=None):\n",
    "    count = 400000 if threshold <= 0 else min(threshold, 400000)\n",
    "    words = []\n",
    "    matrix = np.empty((count, dim)) if vocabulary is None else []\n",
    "    for i in range(count):\n",
    "        word, vec = file.readline().split(' ', 1)\n",
    "        if vocabulary is None:\n",
    "            words.append(word)\n",
    "            matrix[i] = np.fromstring(vec, sep=' ')\n",
    "        elif word in vocabulary:\n",
    "            words.append(word)\n",
    "            matrix.append(np.fromstring(vec, sep=' '))\n",
    "    return (words, matrix) if vocabulary is None else (words, np.array(matrix))\n",
    "\n",
    "def length_normalize(matrix):\n",
    "    norms = np.sqrt(np.sum(matrix**2, axis=1))\n",
    "    norms[norms == 0] = 1\n",
    "    return matrix / norms[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6_load'></a>\n",
    "### Load data\n",
    "\n",
    "First let's load a set of 50D word vectors from GloVe. Original files can be downloaded at this [url](http://nlp.stanford.edu/data/glove.6B.zip). The zip file includes embeddings of different dimensionality (50d, 100d, 200d, 300d) for a vocabulary of 400000 words.\n",
    "\n",
    "We already provide you the required file, which is specifified in \n",
    "`glove_home`. \n",
    "\n",
    "Variables like `matrix` and `word2ind` are used below in the notebook so you need first load data in order to make everything work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input embeddings\n",
    "glove_home = 'data/embeddings/glove.6B.50d.txt'\n",
    "embsfile = open(glove_home, encoding='utf-8', errors='surrogateescape')\n",
    "words, matrix = read(embsfile)\n",
    "\n",
    "# Length normalize embeddings so their dot product effectively computes the cosine similarity\n",
    "matrix = length_normalize(matrix)\n",
    "\n",
    "# Build word to index map\n",
    "word2ind = {word: i for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## Task 1: Semantically similar/related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(word, k=10):\n",
    "    try:\n",
    "        i = word2ind[word]\n",
    "        sim = matrix[i].dot(matrix.T)\n",
    "        knn = np.argsort(-sim)\n",
    "    except KeyError:\n",
    "        print_header('{0} (OOV)'.format(word))\n",
    "        print_footer()\n",
    "        print()\n",
    "        return\n",
    "    print_header('{0} ({1})'.format(word, i + 1))\n",
    "    last = -1\n",
    "    for j in range(len(knn)):\n",
    "        word = words[knn[j]]\n",
    "        if j < k:\n",
    "            last = print_row(j, last=last, trg_words=words, knn=knn, sim=sim)\n",
    "    last = print_row(len(knn)-1, last=last, trg_words=words, knn=knn, sim=sim)\n",
    "    print_footer()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`knn` function retrieve the _k_ most similar/related words of the target word according to the given embedding space. The output looks like the following:\n",
    "\n",
    "   - fist column for nearest neighbour index\n",
    "   - second column for nearest neighbour word\n",
    "   - third column for index of word in frequency ranking (1 is most frequent)\n",
    "   - last column for cosine (1 for perfect similarity)\n",
    "\n",
    "<a id='exercise8'></a>\n",
    "#### Exercise 8\n",
    ">Check the results for the words below. List which words you think are working well, and which ones it is failing. You can use the example in the slide  as reference.\n",
    ">\n",
    ">- france, jesus, xbox, reddish, scratched, megabits \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn('jesus',k=30)\n",
    "knn('france', k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "## Task2: Semantic Orientation\n",
    "The __semantic orientation__ method of [Turney and Littman 2003](http://doi.acm.org/10.1145/944012.944013) is a method for automatically scoring words along some single semantic dimension like sentiment. It works from a pair of small seed sets of words that represent two opposing points on that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_pos = ['good', 'great', 'awesome', 'like', 'love']\n",
    "seed_neg = ['bad', 'awful', 'terrible', 'hate', 'dislike']\n",
    "\n",
    "def determine_coefficient(candidate_word, seed_pos, seed_neg):\n",
    "    pos_ind = np.array([word2ind[word] for word in seed_pos])\n",
    "    pos_mat = matrix[pos_ind]\n",
    "\n",
    "    neg_ind = np.array([word2ind[word] for word in seed_neg])\n",
    "    neg_mat = matrix[neg_ind]\n",
    "\n",
    "    i = word2ind[candidate_word]\n",
    "\n",
    "    pos_sim = np.sum(matrix[i].dot(pos_mat.T))\n",
    "    neg_sim = np.sum(matrix[i].dot(neg_mat.T))\n",
    "\n",
    "    return pos_sim - neg_sim\n",
    "\n",
    "print(determine_coefficient('abhorrent', seed_pos, seed_neg))\n",
    "print(determine_coefficient('vacations', seed_pos, seed_neg))\n",
    "print(determine_coefficient('hunger', seed_pos, seed_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sort our vocabulary by its score along the axis. For now, we're only scoring frequent words, since this process can be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "scored_words = [(word, determine_coefficient(word, seed_pos, seed_neg)) for word in words[1:5000]]\n",
    "sorted_words = sorted(scored_words, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "pp.pprint(sorted_words[:10])\n",
    "print()\n",
    "pp.pprint(sorted_words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise9'></a>\n",
    "#### Exercise 9\n",
    "\n",
    "> Spend a few minutes exploring possible seed sets for semantic dimensions other than sentiment (e.g. \"animals\" vs \"tools\"). \n",
    ">- Define your semantic orientation with the two sets of seeds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "## Task 3: Analogy\n",
    "Next, let's try to build a similar function for determining which words are likely to be good completions for an analogy. Our inputs will be a pair and a singleton word that together represent an analogy problem.\n",
    "\n",
    "- Analogy pair:  good $\\rightarrow$ best,  man $\\rightarrow$ king\n",
    "- Analogy problem: bad $\\rightarrow$ ??,  woman $\\rightarrow$ ??\n",
    "\n",
    "\n",
    "Remenber from slides:\n",
    "\n",
    "- Task: _a is to b as c is to?_\n",
    "    + $a-b \\approx c-d$\n",
    "    + $c-a+b \\approx d$\n",
    "    + $argmax_{d\\in V} (cos(d , c−a+b))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(pront_pair, pront_seed, k=10):\n",
    "    # The function make use of embedding matrix and word indices.\n",
    "    # Recall to load data and inialize matrix and word2ind variables.\n",
    "    try:\n",
    "        i = word2ind[pront_pair[0]]\n",
    "        w1v = matrix[i]\n",
    "    except KeyError:\n",
    "        print_header('{0} (OOV)'.format(pront_pair[0]))\n",
    "        print_footer()\n",
    "        print()\n",
    "        return\n",
    "    try:\n",
    "        i = word2ind[pront_pair[1]]\n",
    "        w2v = matrix[i]\n",
    "    except KeyError:\n",
    "        print_header('{0} (OOV)'.format(pront_pair[1]))\n",
    "        print_footer()\n",
    "        print()\n",
    "        return\n",
    "    try:\n",
    "        i = word2ind[pront_seed]\n",
    "        w3v = matrix[i]\n",
    "    except KeyError:\n",
    "        print_header('{0} (OOV)'.format(pront_seed))\n",
    "        print_footer()\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    ########### YOUR SOLUTION HERE\n",
    "    \n",
    "\n",
    "    ###########\n",
    "\n",
    "    print_header('{0} - {1} + {2}'.format(pront_pair[0], pront_pair[1], pront_seed))\n",
    "    last = -1\n",
    "    for j in range(len(knn)):\n",
    "        word = words[knn[j]]\n",
    "        if j < k:\n",
    "            last = print_row(j, last=last, trg_words=words, knn=knn, sim=sim)\n",
    "    last = print_row(len(knn)-1, last=last, trg_words=words, knn=knn, sim=sim)\n",
    "    print_footer()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise10'></a>\n",
    "#### Exercise 10\n",
    ">-  The embeddings space can be used to do analogies. Please examine the formula in the of slides, and apply it to the three embeddings in the function below `analogy`. If you programmed it correctly, the following analogy should work:\n",
    ">    + `man:king; woman:?` (Note that, in the result list, words in the query need to be ignored)\n",
    ">    \n",
    ">\n",
    ">- Check 10 of the analogie below and see the position of the correct answer (discounting the words in the query).\n",
    "\n",
    ">>> ![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/analogy_exercise.png)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(['france', 'paris'], 'japan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id='section10'></a>\n",
    "## Visualization (optional)\n",
    "Below we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project these vectors into two dimensions while preserving local stucture. Check out [this post from Christopher Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) to learn more about T-SNE and other ways to visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# reduce size of the matrix to speed up the operations\n",
    "viz_words = 500\n",
    "start_ind = 1000\n",
    "end_ind = start_ind+viz_words\n",
    "small_ind = np.array([word2ind[word] for word in words[start_ind:end_ind]])\n",
    "small_word2ind = {word : i for i, word in enumerate(words[start_ind:end_ind])}\n",
    "small_matrix =  matrix[small_ind]\n",
    "\n",
    "# Project word embeddings to two-dimensions\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(small_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "plt.scatter(x=embed_tsne[:,0], y=embed_tsne[:,1], c='steelblue')\n",
    "for i, word in enumerate(words[start_ind:end_ind]):\n",
    "    plt.annotate(word, (embed_tsne[small_word2ind[word], 0], embed_tsne[small_word2ind[word], 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise10'></a>\n",
    "#### Exercise 10 (optional)\n",
    "> It would be interesting to color each point according to their semantic oreintation (e.g positive vs negative). For that, you could use the result obtained in Task2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12.  Autoencoders and Generative Models\n",
    "\n",
    "In this Lab we will focus on three different questions related to RNNs.\n",
    "\n",
    "1) Autoencoders\n",
    "\n",
    "2) Applications of generative methods and their combination\n",
    "\n",
    "\n",
    "We will use material from the books:\n",
    "\n",
    "\"Hands-On Machine Learning with Scikit-Learn and TensorFlow. Concepts, Tools, and Techniques to Build Intelligent Systems\" by Aurélien Géron. http://shop.oreilly.com/product/0636920052289.do \n",
    "\n",
    "- \"Deep Learning with Python\" by F. Chollet. https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438\n",
    "\n",
    "which are recommended as Bibliography of the course.\n",
    "\n",
    "\n",
    "We also use code that have been taken from the following souces:\n",
    "\n",
    "\n",
    "- Keras blog (F. Chollet)  https://blog.keras.io/\n",
    "\n",
    "- \"Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.\" available from https://github.com/wiseodd/generative-models\n",
    "The code there, and the comments available from https://wiseodd.github.io/  are an excellent source of knowledge and references about work on generative models. \n",
    "- Building Autoencoders in Keras. https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the python libraries required to solve the problems\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import itertools \n",
    "import os\n",
    "import sys\n",
    "\n",
    "# We used some utilities from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Tensorflow library used for implementation of the DNNs\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from keras.layers import Input, Dense , merge, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "\n",
    "\n",
    "# Used for plotting and display of figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples,theshape):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    gs = gridspec.GridSpec(5, 5)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in  enumerate(samples[:25,:]):    #enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(theshape), cmap='Greys_r')\n",
    "        #plt.imshow(sample.reshape(theshape))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function set the seeds of the tensorflow function\n",
    "# to make this notebook's output stable across runs\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Autoencoder \n",
    "\n",
    "\n",
    "In the simplest Autoencoder (AE) model the goal is to learn to reconstruct the input. There is one hidden layer and the model is trained to produce an output identical to the input. If the number of neurons in the hidden layer is higher than the number of neurons in the input layer the AE is overcomplete. If it is smaller, the AE is undercomplete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating some data in three dimensions. From these data points we will select 100 points for training and  100 for test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnd.seed(4)\n",
    "\n",
    "# Number of points\n",
    "m = 200\n",
    "\n",
    "# The third variable will be depending on the first two \n",
    "# according to w1 and w2\n",
    "w1, w2 = 0.1, 0.3\n",
    "\n",
    "# There is some noise in the data\n",
    "noise = 0.1\n",
    "\n",
    "angles = rnd.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "data = np.empty((m, 3))\n",
    "data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * rnd.randn(m) / 2\n",
    "data[:, 1] = np.sin(angles) * 0.7 + noise * rnd.randn(m) / 2\n",
    "data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * rnd.randn(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the train and test data and scale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(data[:100])\n",
    "X_test = scaler.transform(data[100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is visualized. Observed the semiring-like shape in the three-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Training data is shown as red points. \n",
    "ax.scatter(X_train[:,0], X_train[:,1], X_train[:,2], c='r', marker='o', label=\"Train\")\n",
    "\n",
    "# Test data is shown as blue points. \n",
    "ax.scatter(X_test[:,0], X_test[:,1], X_test[:,2], c='b', marker='o', label=\"Test\")\n",
    "\n",
    "ax.set_xlabel('X Variable')\n",
    "ax.set_ylabel('Y Variable')\n",
    "ax.set_zlabel('Z Variable')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the AE is defined. It has a structure very similar to a MLP. The main difference is in the Loss function. \n",
    "\n",
    "Read carefully the different steps to define the AE below. In particular, notice the way the reconstruction loss is defined. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Since there are only three dimensions, the number of inputs is 3\n",
    "n_inputs = 3\n",
    "\n",
    "# Two neurons in the single hidden layers\n",
    "n_hidden = 2  \n",
    "\n",
    "# The number of inputs and outputs in the autoencoder is always the same\n",
    "n_outputs = n_inputs\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "learning_rate = 0.01\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "# Instead of defining the weights and biases, \n",
    "# and performing the matrix operation, we use the dense()\n",
    "# function, that basically does the same.\n",
    "\n",
    "hidden = tf.layers.dense(X, n_hidden, name='hidden_layer')\n",
    "outputs = tf.layers.dense(hidden, n_outputs)\n",
    "\n",
    "# The loss function is the mean-square error of the difference \n",
    "# between output and input\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "# Adam optimizer is used\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Minimization of the reconstruction loss \n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "# The computation graph is initialized\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the AE is run. We will store the output of the hidden layer  (only 2 dimensions) to inspect these values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations for the network\n",
    "n_iterations = 1000\n",
    "# Hidden layer \n",
    "codings = hidden\n",
    "\n",
    "# We will keep the weights of the \"hidden\" layer in the variable \"weights\"\n",
    "weights = tf.get_default_graph().get_tensor_by_name(os.path.split(hidden.name)[0] + '/kernel:0')\n",
    "\n",
    "# The AE is run\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        training_op.run(feed_dict={X: X_train})\n",
    "        \n",
    "    # We use the hidden layer as a function and it outputs\n",
    "    # 2-dimensional representation of the input\n",
    "    codings_val = codings.eval(feed_dict={X: X_test})\n",
    "    reconstruction = outputs.eval(feed_dict={X: X_test})\n",
    "\n",
    "    # We print the weights learned by the network\n",
    "    print(weights.eval())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the output of the hidden layer (codings_val) which is a vector of two dimensions. This is the type of compress representation that AEs produce as a side-product and that it is very useful for dimensionality reduction. \n",
    "\n",
    "Finally, we print the two dimensional output of the hidden layer.  Notice the similarity to the original data in three  dimensions. In this example AE has shown its use as a dimensionality reduction method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot(codings_val[:,0], codings_val[:, 1], \"bo\")\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# We visualize the reconstruction as red points. \n",
    "ax.scatter(reconstruction[:,0], reconstruction[:,1], reconstruction[:,2], c='r', marker='o', label=\"Train\")\n",
    "\n",
    "# Test data is shown as blue points. \n",
    "ax.scatter(X_test[:,0], X_test[:,1], X_test[:,2], c='b', marker='o', label=\"Test\")\n",
    "\n",
    "ax.set_xlabel('X Variable')\n",
    "ax.set_ylabel('Y Variable')\n",
    "ax.set_zlabel('Z Variable')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoding MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following auxiliary function is used to read the mnist and fashion datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_And_Normalize_Image_Dataset(db_name='mnist', max_train_samples = 10000,max_test_samples = 10000):\n",
    "\n",
    "    if db_name=='mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()               \n",
    "    elif db_name=='fashion':\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()        \n",
    "        \n",
    "    #print(x_train.shape,y_train.shape)\n",
    "    #print(x_test.shape,y_test.shape)\n",
    "\n",
    "        \n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    \n",
    "    x_train = np.expand_dims(x_train, axis=3)\n",
    "    x_test = np.expand_dims(x_test, axis=3)\n",
    "    #x_train = x_train.reshape((len(x_train), 28*28))\n",
    "    #x_test = x_test.reshape((len(x_test), 28*28))\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "- Modify the implementation in the previous example to learn an autoencoder for the MNIST dataset.\n",
    "\n",
    "Note that the previous implementation was designed for a simple problem. You should modify the structure so that the reconstruction phase has sufficient information. For example, you could use 3 hidden layers of 400, 200, and 400 neurons each.\n",
    "\n",
    "Suggestions:\n",
    "- Use the function provided to read the mnist dataset.\n",
    "- Check the quality of the implementation running the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = \n",
    "\n",
    "#Change these if you want to\n",
    "n_hidden1 = \n",
    "n_hidden2 = \n",
    "n_hidden3 = \n",
    "\n",
    "n_outputs = n_inputs\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "# Instead of defining the weights and biases, \n",
    "# and performing the matrix operation, we use the dense()\n",
    "# function, that basically does the same.\n",
    "\n",
    "hidden = tf.layers.dense()\n",
    "hidden = tf.layers.dense()\n",
    "hidden = tf.layers.dense()\n",
    "outputs = tf.layers.dense()\n",
    "\n",
    "# The loss function is the mean-square error of the difference \n",
    "# between output and input\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "# Adam optimizer is used\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Minimization of the reconstruction loss \n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "# The computation graph is initialized\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train, mnist_train_y), (mnist_test, mnist_test_y) = Read_And_Normalize_Image_Dataset(___)\n",
    "mnist_train = np.reshape(mnist_train, (-1, 784))\n",
    "mnist_test = np.reshape(mnist_test, (-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations for the network\n",
    "n_epochs = 80\n",
    "save = np.arange(1, n_epochs+1, n_epochs//4)\n",
    "saver = tf.train.Saver()\n",
    "batch_size = int(mnist_train.shape[0]/50)\n",
    "\n",
    "# The AE is run\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        batch = 0\n",
    "        while batch < mnist_train.shape[0]:\n",
    "            _, ls = sess.run([training_op, reconstruction_loss], feed_dict={X: mnist_train[batch:batch+batch_size]})\n",
    "            batch += batch_size\n",
    "        if epoch in save:\n",
    "            print(epoch, ls)\n",
    "            saver.save(sess, \"./MNIST_autoencoder\" + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_MNIST = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in save:\n",
    "        saver.restore(sess, \"./MNIST_autoencoder\" + str(i))\n",
    "        pred_MNIST.append(sess.run(outputs, feed_dict={X: mnist_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to see how the quality of the autoencoder has increased during the training phase\n",
    "\n",
    "\n",
    "def plot_evolution(original, preds, c=False):\n",
    "    plt.figure(figsize=(16,16))\n",
    "    for i in np.arange(0,25,5):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.gca().axis('off')\n",
    "        plt.imshow(original[i].reshape((28, 28)), cmap=plt.cm.gray_r)\n",
    "        plt.gca().set_title('Original')\n",
    "        plt.subplot(5, 5, i + 2)\n",
    "        plt.gca().axis('off')\n",
    "        plt.imshow(preds[0][i].reshape((28, 28)), cmap=plt.cm.gray_r)\n",
    "        if c:\n",
    "            plt.gca().set_title(\"Noisy\")\n",
    "        else:\n",
    "            plt.gca().set_title(\"Rec., 1 iteration\")\n",
    "        plt.subplot(5, 5, i + 3)\n",
    "        plt.gca().axis('off')\n",
    "        plt.imshow(preds[1][i].reshape((28, 28)), cmap=plt.cm.gray_r)\n",
    "        if c:\n",
    "            plt.gca().set_title(\"Rec., 1 iteration\")\n",
    "        else:\n",
    "            plt.gca().set_title(\"Rec., 2 iteration\")\n",
    "        plt.subplot(5, 5, i + 4)\n",
    "        plt.gca().axis('off')\n",
    "        plt.imshow(preds[2][i].reshape((28, 28)), cmap=plt.cm.gray_r)\n",
    "        if c:\n",
    "            plt.gca().set_title(\"Rec., 2 iteration\")\n",
    "        else:\n",
    "            plt.gca().set_title(\"Rec., 3 iteration\")\n",
    "        plt.subplot(5, 5, i + 5)\n",
    "        plt.gca().axis('off')\n",
    "        plt.imshow(preds[3][i].reshape((28, 28)), cmap=plt.cm.gray_r)\n",
    "        if c:\n",
    "            plt.gca().set_title(\"Rec., 3 iteration\")\n",
    "        else:\n",
    "            plt.gca().set_title(\"Rec., 4 iteration\")\n",
    "        plt.gca().axis('off')\n",
    "    plt.suptitle('Evolution of the Autoencoder', fontsize=16)\n",
    "    plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution(mnist_test, pred_MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to see how good the autoencoder trained with MNIST can reconstruct the fashion database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fashion_train, fashion_train_y), (fashion_test, fashion_test_y) = Read_And_Normalize_Image_Dataset(db_name='fashion', max_train_samples = 10000, max_test_samples = 10000)\n",
    "fashion_train = np.reshape(fashion_train, (-1, 784))\n",
    "fashion_test = np.reshape(fashion_test, (-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fashion = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in save:\n",
    "        saver.restore(sess, \"./MNIST_autoencoder\" + str(i))\n",
    "        pred_fashion.append(sess.run(outputs, feed_dict={X: fashion_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution(fashion_test, pred_fashion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "As you can see, the reconstruction of the MNIST autoencoder for the fashion dataset is not very strong. Try to improve these results by training another autoencoder for the fashion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This should be done in the exercise\n",
    "\n",
    "n_inputs = 784\n",
    "\n",
    "\n",
    "# These can be changed too. Note that this problem is al little more difficult. Maybe more layers/neurons than\n",
    "# in the previous exercise are necessary\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations for the network\n",
    "n_epochs = 100\n",
    "save = np.arange(1, n_epochs+1, n_epochs//4)\n",
    "saver = tf.train.Saver()\n",
    "batch_size = int(mnist_train.shape[0]/50)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        batch = 0\n",
    "        while batch < fashion_train.shape[0]:\n",
    "            _, ls = sess.run([training_op, reconstruction_loss], feed_dict={X: fashion_train[batch:batch+batch_size]})\n",
    "            batch += batch_size\n",
    "        if epoch in save:\n",
    "            print(epoch, ls)\n",
    "            saver.save(sess, \"./fashion_autoencoder\" + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fashion = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in save:\n",
    "        saver.restore(sess, \"./fashion_autoencoder\" + str(i))\n",
    "        pred_fashion.append(sess.run(outputs, feed_dict={X: fashion_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution(fashion_test, pred_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising autoencoders (DAE) are very similar to AEs but they are trained to recover a \"corrupted\" input. This means that some noise will added to the input for training, but the corresponding output will be kept without any noise.\n",
    "\n",
    "The key point in this DAE implementation is that the noise is added to the input of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset of chinese characters. It contains images of around 9000 chinese characters. The original dataset from which these images have been produced is available from: https://github.com/skishore/makemeahanzi\n",
    "\n",
    "Check that you have the folder \"chinese\" that contains this dataset and run the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_char = np.load('chinese/chinese_characters.npy')\n",
    "chinese_char = chinese_char.reshape((-1, 784))\n",
    "chinese_test = chinese_char[:chinese_char.shape[0]//4]\n",
    "chinese_train = chinese_char[chinese_char.shape[0]//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(chinese_char[10].reshape((28, 28)), cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(chinese_char[20].reshape((28, 28)), cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(chinese_char[30].reshape((28, 28)), cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 784\n",
    "\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 400\n",
    "\n",
    "n_outputs = n_inputs\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "learning_rate = 0.001\n",
    "noise_level = 10\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "X_noisy = X + noise_level * tf.random_normal(tf.shape(X))\n",
    "\n",
    "\n",
    "hidden1 = tf.layers.dense(X_noisy, n_hidden1, name='hidden_layer1')\n",
    "z = tf.layers.dense(hidden1, n_hidden2, name='z')\n",
    "hidden2 = tf.layers.dense(z, n_hidden3, name='hidden_layer2')\n",
    "outputs = tf.layers.dense(hidden2, n_outputs)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "saver = tf.train.Saver()\n",
    "# Adam optimizer is used\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Minimization of the reconstruction loss \n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "# The computation graph is initialized\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations for the network\n",
    "n_epochs = 500\n",
    "save = np.arange(1, n_epochs+2, n_epochs//2)\n",
    "print(save)\n",
    "saver = tf.train.Saver()\n",
    "batch_size = int(chinese_train.shape[0]/50)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(1, n_epochs+2):\n",
    "        batch = 0\n",
    "        while batch < chinese_train.shape[0]:\n",
    "            _, ls = sess.run([training_op, reconstruction_loss], feed_dict={X: chinese_train[batch:batch+batch_size]})\n",
    "            batch += batch_size\n",
    "            \n",
    "        if epoch in save:\n",
    "            print(epoch, ls)\n",
    "            saver.save(sess, \"./chinese_DAE\" + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_chinese = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in save:\n",
    "        saver.restore(sess, \"./chinese_DAE\" + str(i))\n",
    "        pred_chinese.append(sess.run(outputs, feed_dict={X: chinese_test}))\n",
    "    pred_chinese = [sess.run(X_noisy, feed_dict={X: chinese_test})] + pred_chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the predictions of the network for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution(chinese_test, pred_chinese, c=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "In the previous example, we have assumed that the noise the DAE is going to face is known, and we have therefore trained it to deal with this specific kind of noise - both the training and testing data were *generated* with the same noise function.\n",
    "\n",
    "In this exercise, we are going to test a more realistic problem, in which we are applying two kinds of noise to the training and testing data:\n",
    "\n",
    "- With the same training and testing separation that we have made, modify the example so that you *manually* introduce noises - with numpy, for example.\n",
    "    - The two choosen noises should be different, for example, random uniform, and random normal\n",
    "- Train a DAE with the training data with one noise\n",
    "- Test the DAE with the test data with the other noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 784\n",
    "\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 400\n",
    "\n",
    "n_outputs = n_inputs\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "learning_rate = 0.001\n",
    "\n",
    "noise_level = 20\n",
    "\n",
    "chinese_train_noisy = chinese_train + \n",
    "chinese_test_noisy = chinese_test + \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "hidden1 = \n",
    "z =\n",
    "hidden2 = \n",
    "outputs =\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "saver = tf.train.Saver()\n",
    "# Adam optimizer is used\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Minimization of the reconstruction loss \n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "# The computation graph is initialized\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations for the network\n",
    "n_epochs = 500\n",
    "save = np.arange(1, n_epochs+2, n_epochs//2)\n",
    "print(save)\n",
    "saver = tf.train.Saver()\n",
    "batch_size = int(chinese_train_noisy.shape[0]/50)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(1, n_epochs+2):\n",
    "        batch = 0\n",
    "        while batch < chinese_train.shape[0]:\n",
    "            _, ls = sess.run([training_op, reconstruction_loss], feed_dict={X: chinese_train_noisy[batch:batch+batch_size]})\n",
    "            batch += batch_size\n",
    "            \n",
    "        if epoch in save:\n",
    "            print(epoch, ls)\n",
    "            saver.save(sess, \"./chinese_DAE\" + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_chinese = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in save:\n",
    "        saver.restore(sess, \"./chinese_DAE\" + str(i))\n",
    "        pred_chinese.append(sess.run(outputs, feed_dict={X: chinese_test}))\n",
    "    pred_chinese = [chinese_test_noisy] + pred_chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution(chinese_test, pred_chinese, c=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Modeling\n",
    "\n",
    "Now we will focus on Generative Models. In particular we analyze:\n",
    "1. GAN  \n",
    "2. Variational Autoencoders (VAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a GAN model for the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we define a GAN model to generate images from the MNIST dataset. We will define the Discriminator and the Generator components and the loss-function that we want to optimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28                   # The number of inputs to the network n_inputs=28*28=784\n",
    "n_G_noise = 100                    # The size of the noise input for the Generator\n",
    "n_D_hidden_ouputs = 128            # The size of the hidden layer in the Discriminator\n",
    "n_G_hidden_ouputs = 128            # The size of the hidden layer in the Generator\n",
    "\n",
    "\n",
    "# xavier_init is a function to randomly initialize the weights of neurons.\n",
    "def xavier_init(size):  # Normal version\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Discriminator. It will be a multilayer perceptron of only one hidden layer and one output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The input variable\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "# The hidden layer of the discriminator. It receives n_inputs=784 and \n",
    "# outputs n_D_hidden_ouputs=128 values. \n",
    "# Here the weights and bias are defined\n",
    "D_W1 = tf.Variable(xavier_init([n_inputs, n_D_hidden_ouputs]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[n_D_hidden_ouputs]))\n",
    "\n",
    "# The discriminator output layer. It receives n_inputs=128 and \n",
    "# outputs only one value [PROB. OF WHETHER THE INPUT IMAGE IS FAKE OR NOT]\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([n_D_hidden_ouputs, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "# These theta_D values are all the parameters of the model.\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "# Finally the discriminator is defined. Notice that it computes\n",
    "# the output probability using a sigmoid function\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we define the Generator. It will be a multilayer perceptron of only one hidden layer and one output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z is the noise vector, needed to generate samples from the generator\n",
    "Z = tf.placeholder(tf.float32, shape=[None, n_G_noise])\n",
    "\n",
    "# The hidden layer of the Generator. It receives n_G_noise=100 and \n",
    "# outputs n_G_hidden_ouputs=128 values. \n",
    "# Here the weights and bias are defined\n",
    "G_W1 = tf.Variable(xavier_init([n_G_noise, n_G_hidden_ouputs]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[n_G_hidden_ouputs]))\n",
    "\n",
    "# The generator output layer. It receives n_G_hidden_ouputs=100 noise values\n",
    "# outputs a FAKE input vector (an invented MNIST image)\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([n_G_hidden_ouputs, n_inputs]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[n_inputs]))\n",
    "\n",
    "# These theta_G values are all the parameters of the model.\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "# Z is randomly sampled between -1 and 1\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "# This is the generator that receive a noise vector\n",
    "# and using the generator component outputs a sample\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the loss functions and optimization algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get samples from the generator\n",
    "G_sample = generator(Z)\n",
    "\n",
    "# We get the discriminator to get the probabilities (and logits)\n",
    "# of the real input and of the fake G_sample\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# Alternative loss functions \n",
    "# D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "# G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# Loss functions \n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, \n",
    "                                                                     labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake,\n",
    "                                                                     labels=tf.zeros_like(D_logit_fake)))\n",
    "# For the discriminator the total loss is the sum of the two\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "# For the generator, we want to change G_sample in such a way that it gets\n",
    "# a higher probability from the Discriminator\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, \n",
    "                                                                labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "# The problems are solved \n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the tensorflow session and run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "mb_size = 250\n",
    "Z_dim = 100\n",
    "number_iterations = 41\n",
    "batch_size = 150\n",
    "n_batches = mnist_train.shape[0]//batch_size\n",
    "print_cycle = 8\n",
    "\n",
    "\n",
    "sess = tf.Session() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('gan_out/'):\n",
    "    os.makedirs('gan_out/')\n",
    "\n",
    "for it in range(number_iterations):\n",
    "\n",
    "        \n",
    "    # 25 random samples are taken from the generator for visualization purposes\n",
    "    samples = sess.run(G_sample, feed_dict={Z: sample_Z(25, Z_dim)})\n",
    "\n",
    "    # They are saved in the directory gan_out\n",
    "    fig = plot(samples,[28,28])\n",
    "    plt.savefig('gan_out/{}.png'.format(str(it).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "        \n",
    "    for i in range(n_batches):\n",
    "\n",
    "        # A batch is picked from MNIST    \n",
    "        X_mb = mnist_train[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        # The training data is used to train the GAN\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "\n",
    "    if it % print_cycle == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. What is the meaning of using the loss function tf.nn.sigmoid_cross_entropy_with_logits for the discriminator? Explain the way it is used. \n",
    "\n",
    "2. What is the meaning of using the loss function tf.nn.sigmoid_cross_entropy_with_logits for the generator? Explain the way it is used. \n",
    "\n",
    "3. Go to the directory gan_out. What do the figures show? Can you identify any pattern as the generation advances?\n",
    "\n",
    "\n",
    "Suggestion:\n",
    "\n",
    "You can see tensorflow help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataset for the Second Exercise\n",
    "\n",
    "For this exercise, we are going to change the dataset to a more *realistic* one, using images of coins to train a GAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coins DataSet  (See https://github.com/despoisj/CoinsDataset for details on dataset)\n",
    "x_train = np.load('coin_images.npy')\n",
    "x_train = x_train/255\n",
    "print(x_train.shape)\n",
    "fig = plot(x_train,[50,50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Modify the following cells to run a GAN on the selected dataset. \n",
    "\n",
    "1.  Take into account the number of samples in the dataset and the number of variables for deciding on the batch size, the number of input variables, and the number of iterations. Add at least one layer compared to the previous example, as this problem is significantly more difficult.\n",
    "2. Modify the name of the folder where the samples from the generator are going to be saved\n",
    "3. Inspect the images generated by the GAN\n",
    "4. Evaluate the effect of adding a hidden layer to the generator and discriminator modules\n",
    "5. After completing the runs for the assigned dataset, select another dataset of your choice and repeat the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs =                    # The number of inputs to the network \n",
    "n_G_noise =                     # The size of the noise input for the Generator\n",
    "n_D_hidden_ouputs =             # The size of the hidden layer in the Discriminator\n",
    "n_G_hidden_ouputs =             # The size of the hidden layer in the Generator\n",
    "\n",
    "\n",
    "# xavier_init is a function to randomly initialize the weights of neurons.\n",
    "def xavier_init(size):  # Normal version\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The input variable\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "# The hidden layer of the discriminator. \n",
    "# Here the weights and bias are defined\n",
    "\n",
    "\n",
    "# The discriminator output layer.\n",
    "\n",
    "\n",
    "# These theta_D values are all the parameters of the model.\n",
    "theta_D = \n",
    "\n",
    "# Finally the discriminator is defined. Notice that it computes\n",
    "# the output probability using a sigmoid function\n",
    "def discriminator(x):\n",
    "    \n",
    "\n",
    "    return D_prob, D_logit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z is the noise vector, needed to generate samples from the generator\n",
    "Z = tf.placeholder(tf.float32, shape=[None, n_G_noise])\n",
    "\n",
    "# The hidden layer of the Generator.\n",
    "\n",
    "\n",
    "# The generator output layer. \n",
    "\n",
    "\n",
    "# These theta_G values are all the parameters of the model.\n",
    "theta_G = \n",
    "\n",
    "# Z is randomly sampled between -1 and 1\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "# This is the generator that receive a noise vector\n",
    "# and using the generator component outputs a sample\n",
    "def generator(z):\n",
    "\n",
    "    return G_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get samples from the generator\n",
    "\n",
    "\n",
    "# We get the discriminator to get the probabilities (and logits)\n",
    "# of the real input and of the fake G_sample\n",
    "\n",
    "\n",
    "# Alternative loss functions \n",
    "# D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "# G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# Loss functions \n",
    "D_loss_real = \n",
    "D_loss_fake = \n",
    "# For the discriminator the total loss is the sum of the two\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "# For the generator, we want to change G_sample in such a way that it gets\n",
    "# a higher probability from the Discriminator\n",
    "G_loss = \n",
    "\n",
    "# The problems are solved \n",
    "D_solver = tf.train.AdamOptimizer(0.001).minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer(0.001).minimize(G_loss, var_list=theta_G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "mb_size = 250\n",
    "number_iterations = 41\n",
    "batch_size = 150\n",
    "n_batches = x_train.shape[0]//batch_size\n",
    "print_cycle = 8\n",
    "\n",
    "\n",
    "sess = tf.Session() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('gan_out/'):\n",
    "    os.makedirs('gan_out/')\n",
    "\n",
    "for it in range(number_iterations):\n",
    "\n",
    "        \n",
    "    # 25 random samples are taken from the generator for visualization purposes\n",
    "    samples = sess.run(G_sample, feed_dict={Z: sample_Z(25, n_G_noise)})\n",
    "\n",
    "    # They are saved in the directory gan_out\n",
    "    fig = plot(samples,[50,50])\n",
    "    plt.savefig('gan_out/{}.png'.format(str(it).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "        \n",
    "    for i in range(n_batches):\n",
    "        \n",
    "\n",
    "        # A batch is picked from MNIST    \n",
    "        X_mb = x_train[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        # The training data is used to train the GAN\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, n_G_noise)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, n_G_noise)})\n",
    "\n",
    "    if it % print_cycle == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        #print(sess.run(D_loss_fake, feed_dict={Z: sample_Z(mb_size, n_G_noise)}))\n",
    "        #print(sess.run(D_loss_real, feed_dict={X: X_mb}))\n",
    "        #print(sess.run(D_real, feed_dict={X: X_mb, Z: sample_Z(mb_size, n_G_noise)}))\n",
    "        #print(sess.run(D_fake, feed_dict={X: X_mb, Z: sample_Z(mb_size, n_G_noise)}))\n",
    "        print('D_loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last part of the exercise, try to train a GAN with one of the following datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese Characters Dataset\n",
    "#x_train = np.load('chinese/chinese_characters.npy')\n",
    "#x_train = x_train.reshape((-1,784))\n",
    "#print(x_train.shape)\n",
    "#fig = plot(x_train,[28,28])\n",
    "#plt.show()\n",
    "\n",
    "#Short Texture DataSet  (see https://www.robots.ox.ac.uk/~vgg/data/dtd/ for details on original data)\n",
    "#There are 47 textures represented \n",
    "#x_train = np.load('texture_images.npy')\n",
    "#x_train = x_train/255.\n",
    "#print(x_train.shape)\n",
    "#fig = plot(x_train,[50,50])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The variational autoencoder contains two components. The encoder learns the conditional distribution of a latent variable z given the observed variable x (the inputs). A neural network acts as the encoder and it is assumed that the prior probability of the latent variant z follows a Normal distribution. The decoder, also a deep neural network, takes samples generated by a distribution parametrized by the results of the encoder and produces samples similar to those in our data.\n",
    "\n",
    "The encoder and decoder will be represented as neural network of one hidden layer. Before advancing to see the VAE implementation remind of the VAE structure looking at the image in the following cell and/or slides from the class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image is taken from http://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in.html, a blog which is a good introduction to VAEs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/vae.4.png\", width=600, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the networks parameters are defined\n",
    "\n",
    "# Number of variables (784)\n",
    "X_dim = mnist_train.shape[1]\n",
    "\n",
    "# Dimension of the latent variable z\n",
    "z_dim = 100\n",
    "\n",
    "# Size of the hidden layer\n",
    "h_dim = 128\n",
    "\n",
    "\n",
    "c = 0\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell the encoder is defined. It is a network that receives an input vector and outputs two vectors of latent variables. \n",
    "\n",
    "1) A vector of mu values. \n",
    "\n",
    "2) A vector of $log(\\sigma^2)$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================== Q(z|X) ======================================\n",
    "\n",
    "# Input variables\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "\n",
    "# Output variables\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "# Input layer (weights and biases)\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "# Mean layer (weights and biases)\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "# Variance layer (weights and biases)\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "\n",
    "# The network computation is implemented \n",
    "def Q(X):\n",
    "    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)\n",
    "    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu\n",
    "    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "# Creates a sample from the Normal distribution defined by parameters mu and log_var\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell the decoder is implemented. It is a network of only one hidden layer. It receives samples from the Normal distribution and ouputs a sample from the input space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== P(X|z) ======================================\n",
    "\n",
    "# Hidden layer\n",
    "P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "# Output layer\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "# The network computations are made. Notice that both,\n",
    "# the logits and the probability computed from it are output\n",
    "def P(z):\n",
    "    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)\n",
    "    logits = tf.matmul(h, P_W2) + P_b2\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "    return prob, logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells the loss function and optimizers are defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== TRAINING ====================================\n",
    "\n",
    "# The parameters of the latent distribution that \n",
    "# are learned from the encoder\n",
    "z_mu, z_logvar = Q(X)\n",
    "\n",
    "# The sample that is obtained sampling from \n",
    "# the latent distribution\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "_, logits = P(z_sample)\n",
    "\n",
    "# Sampling from random z\n",
    "X_samples, _ = P(z)\n",
    "\n",
    "######## Loss Functions ##################\n",
    "\n",
    "# E[log P(X|z)]\n",
    "recon_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X))\n",
    "\n",
    "# D_KL(Q(z|X) || P(z)); compute in closed form\n",
    "kl_loss = 0.5 * tf.reduce_mean(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar)\n",
    "\n",
    "# VAE loss\n",
    "vae_loss = recon_loss + kl_loss\n",
    "\n",
    "# The Adam optimizer is used\n",
    "solver = tf.train.AdamOptimizer().minimize(vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorflow session is initialized and the simulation is run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Minibatch size\n",
    "mb_size = 150\n",
    "number_iterations = 101\n",
    "n_batches = mnist_train.shape[0]//mb_size\n",
    "print_cycle = 10\n",
    "\n",
    "\n",
    "sess = tf.Session() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Some samples from the latent space are saved in the directory vae_out\n",
    "\n",
    "if not os.path.exists('single/'):\n",
    "    os.makedirs('single/')\n",
    "\n",
    "\n",
    "for it in range(number_iterations):\n",
    "\n",
    "    for batch in range(n_batches):\n",
    "        X_mb = mnist_train[batch*mb_size:(batch+1)*mb_size]\n",
    "        _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb})\n",
    "\n",
    "    if it % print_cycle == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print()\n",
    "\n",
    "        samples = sess.run(X_samples, feed_dict={z: np.random.randn(25, z_dim)})\n",
    "\n",
    "        fig = plot(samples,[28,28])\n",
    "        plt.savefig('single/{}.png'.format(str(it).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coins DataSet  (See https://github.com/despoisj/CoinsDataset for details on dataset)\n",
    "#x_train = np.load('coin_images.npy')\n",
    "#x_train = x_train/255.\n",
    "#print(x_train.shape)\n",
    "#fig = plot(x_train,[50,50])\n",
    "\n",
    "\n",
    "# Chinese Characters Dataset\n",
    "x_train = np.load('chinese/chinese_characters.npy')\n",
    "x_train = x_train/255.\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "print(x_train.shape)\n",
    "fig = plot(x_train,[28,28])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Short Texture DataSet  (see https://www.robots.ox.ac.uk/~vgg/data/dtd/ for details on original data)\n",
    "#There are 47 textures represented \n",
    "#x_train1 = np.load('texture_images.npy')\n",
    "#x_train1 = x_train1/255.\n",
    "#print(x_train1.shape)\n",
    "#fig = plot(x_train1,[50,50])\n",
    "\n",
    "# Fashion Dataset (Remember update the directory where your fashion data is)\n",
    "x_train1 = fashion_train\n",
    "fig = plot(x_train1,[28,28])\n",
    "plt.show()\n",
    "\n",
    "combined_data = np.vstack((x_train,x_train1))\n",
    "\n",
    "np.random.shuffle(combined_data)\n",
    "\n",
    "fig = plot(combined_data,[28,28])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Modify the following cells to run a VAE on another dataset.\n",
    "\n",
    "\n",
    "1.  Take into account the number of samples in the dataset and the number of variables for deciding on the batch size, the number of input variables, and the number of iterations\n",
    "2. Modify the name of the folder where the samples from the generator are going to be saved \n",
    "3. Inspect the images generated by the VAE\n",
    "4. Evaluate the effect of adding a hidden layer to the encoder and decoder modules.\n",
    "5. After completing the runs for the assigned dataset, select another dataset of your choice and repeat the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the networks parameters are defined\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# Number of variables (784)\n",
    "X_dim = combined_data.shape[1]\n",
    "#y_dim = mnist.train.labels.shape[1]\n",
    "\n",
    "# Dimension of the latent variable z\n",
    "z_dim = 100\n",
    "\n",
    "# Size of the hidden layer\n",
    "h_dim = 128\n",
    "\n",
    "\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "# =============================== Q(z|X) ======================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================== P(X|z) ======================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================== TRAINING ====================================\n",
    "\n",
    "\n",
    "######## Loss Functions ##################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Minibatch size\n",
    "mb_size = 150\n",
    "number_iterations = 300\n",
    "n_batches = combined_data.shape[0]//mb_size\n",
    "print_cycle = 20\n",
    "\n",
    "sess = tf.Session() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Some samples from the latent space are saved in the directory vae_out\n",
    "\n",
    "if not os.path.exists('combined/'):\n",
    "    os.makedirs('combined/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(number_iterations):\n",
    "    for batch in range(n_batches):\n",
    "        X_mb = combined_data[batch*mb_size:(batch+1)*mb_size]\n",
    "\n",
    "        _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb})\n",
    "\n",
    "    if it % print_cycle == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print()\n",
    "\n",
    "        samples = sess.run(X_samples, feed_dict={z: np.random.randn(25, z_dim)})\n",
    "\n",
    "        fig = plot(samples,[28,28])\n",
    "        plt.savefig('combined/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Learn VAEs from combinations of datasets\n",
    "\n",
    "1. Inspect the images generated by the VAE\n",
    "2. Compare them with those obtained with single datasets\n",
    "\n",
    "\n",
    "IMPORTANT: For the large_texture dataset, you should use only a subset of the images, preferably between 1200 and 3600 images, for example large_texture_data[2400:3600,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE in Keras\n",
    "\n",
    "Once again, we see how the Keras abstraction can help us at the time of designing *simple* models.\n",
    "In this example, we are defining a VAE for the MNIST problem with **z** of size only two, so that we can do some visualization of the latent representation.\n",
    "\n",
    "Additionally, we will visualize a 2D manifold of the data. In our case, the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# Dimension of the original data 28x28\n",
    "original_dim = 784\n",
    "\n",
    "# Number of neurons in the hidden layer\n",
    "intermediate_dim = 50\n",
    "\n",
    "# Latent dimension (only 2 variables for easier visualization)\n",
    "latent_dim = 2\n",
    "\n",
    "# Stardard deviation for the normal distribution\n",
    "epsilon_std = 0.01\n",
    "\n",
    "\n",
    "\n",
    "## ENCODER\n",
    "# Input to the encoder\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "# hidden layer of the encoder\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "# Output of the encoder (the mean and std)\n",
    "z_mean = Dense(latent_dim//2)(h)\n",
    "z_log_sigma = Dense(latent_dim//2)(h)\n",
    "\n",
    "# Using the mean and the variance new samples\n",
    "# of the hidden variables are generated\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# It transforms the sampling function into a layer of the network\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# Defines the hidden layer of the decoder \n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "# Defines the ouput layer of the decoder\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "# The hidde layer receives outputs from the gaussian distribution\n",
    "h_decoded = decoder_h(z)\n",
    "# The output layer decodes the transformed z values into x \n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# end-to-end autoencoder\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model(x, [z_mean, z_log_sigma])\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "vae.fit(mnist_train, mnist_train,\n",
    "        shuffle=True,\n",
    "        epochs=n_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(mnist_test, mnist_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the hidden representation to visualize th points in the 10 different classes. It can be seen that examples for the same class (the same color) are, at least to some extent, grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_encoded = encoder.predict(mnist_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(mnist_test_encoded[0][:, 0], mnist_test_encoded[1][:, 0], c=mnist_test_y)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we explore the space of latent representation, we create a grid of values in [-15,15] for the two latent variables and using all possible combinations we decode the variable X from the decoder. We observe how the classes of the problems are represented in the latent representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]]) * epsilon_std\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "Learn a VAE for the fashion data\n",
    "\n",
    "1) Plot how points are grouped in the latent representation.\n",
    "\n",
    "2) Display the how the classes of the problem are organized in the latent representation (as in the example above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# Dimension of the original data 28x28\n",
    "original_dim = 784\n",
    "\n",
    "# Number of neurons in the hidden layer\n",
    "intermediate_dim = 50\n",
    "\n",
    "# Latent dimension (only 2 variables for easier visualization)\n",
    "latent_dim = 2\n",
    "\n",
    "# Stardard deviation for the normal distribution\n",
    "epsilon_std = 0.01\n",
    "\n",
    "\n",
    "\n",
    "## ENCODER\n",
    "# Input to the encoder\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "# hidden layer of the encoder\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "# Output of the encoder (the mean and std)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)\n",
    "\n",
    "# Using the mean and the variance new samples\n",
    "# of the hidden variables are generated\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# It transforms the sampling function into a layer of the network\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# Defines the hidden layer of the decoder \n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "# Defines the ouput layer of the decoder\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "# The hidde layer receives outputs from the gaussian distribution\n",
    "h_decoded = decoder_h(z)\n",
    "# The output layer decodes the transformed z values into x \n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# end-to-end autoencoder\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model(x, [z_mean, z_log_sigma])\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "vae.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_test_encoded = encoder\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(, , c=)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]]) * epsilon_std\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 9 \n",
    "\n",
    "Below there is an implementation of the VAE when the number of latent variables is 20. \n",
    "\n",
    "1) Use the chinese dataset to learn the VAE\n",
    "\n",
    "2) Generate data from the VAE and evaluate its quality (you  can use the function predict() and the function plot() to visualize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "\n",
    "# Dimension of the original data 28x28\n",
    "original_dim = \n",
    "\n",
    "# Number of neurons in the hidden layer\n",
    "intermediate_dim = 50\n",
    "\n",
    "# Latent dimension (only 2 variables for easier visualization)\n",
    "latent_dim = \n",
    "\n",
    "# Stardard deviation for the normal distribution\n",
    "epsilon_std = 0.01\n",
    "\n",
    "\n",
    "## ENCODER\n",
    "# Input to the encoder\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "# hidden layer of the encoder\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "# Output of the encoder (the mean and std)\n",
    "z_mean = Dense()()\n",
    "z_log_sigma = Dense()()\n",
    "\n",
    "# Using the mean and the variance new samples\n",
    "# of the hidden variables are generated\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# It transforms the sampling function into a layer of the network\n",
    "z = \n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The hidde layer receives outputs from the gaussian distribution\n",
    "h_decoded = Dense(intermediate_dim, activation='relu')(z)\n",
    "# The output layer decodes the transformed z values into x \n",
    "x_decoded = Dense(original_dim, activation='sigmoid')(h_decoded)\n",
    "\n",
    "\n",
    "# end-to-end autoencoder\n",
    "vae = Model()\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model()\n",
    "\n",
    "# decoder, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chinese Characters Dataset\n",
    "chinese = np.load('chinese/chinese_characters.npy')\n",
    "chinese = chinese.reshape((-1,784))\n",
    "chinese_train = chinese[:9000,:]\n",
    "chinese_test = chinese[9000:,:]\n",
    "\n",
    "chinese_train = chinese_train.astype('float32') / 255.  #+ 1/255.\n",
    "chinese_test = chinese_test.astype('float32') / 255.  # + 1/255.\n",
    "    \n",
    "print(chinese_train.shape,chinese_test.shape)\n",
    "fig = plot(chinese_train,[28,28])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "\n",
    "vae.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(samples, [28, 28])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
